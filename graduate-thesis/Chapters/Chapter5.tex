\chapter{Discussion and Suggestions for Future Research}

\section{Time Series Analysis}

\subsection{Random Walk}
The random walk theory suggests that stock price changes have the same distribution and are independent of each other, so the past movement or trend of a stock price or market cannot be used to predict its future movement. In short, this is the idea that stocks take a random and unpredictable path.

As can be seen in figures 4.4, 4.6, 4.8, and 4.10, the correlation plots show this theory to be false and that past movement is related to future movement.

The histogram of returns show that the stocks follow a normal distribution, where some stocks showed higher returns than others as can be seen from a wider x-axis.

\subsection{Ordinary Least Squares (OLS)}
This algorithm achieved a good fit in the in-sample tests, having a prediction rate above 96\% for all the stocks tested. It is evident that OLS may be used for stock market forecasting.

\subsection{Auto Regressive (AR)}
This algorithm failed to achieve a good fit in the in-sample tests, having a huge difference in sharpe ratios based on the original price returns and in-sample predicted price returns. This is also evident in the QQ and probability plots, both of which show heavy tails indicating a bad fit to the data. The algorithm failed completely in forecasting price returns in out-of-sample testing. 

The histogram of returns further showed that the algorithm did not achieve a good fit, as the histogram itself was not completely symmetrical.

\subsection{Moving Average (MA)}
This algorithm faired better than AR having a lower difference in sharpe ratios based on the original price returns and in-sample predicted price returns, however still failed to achieve a good fit in the in-sample tests. The QQ and probability plots, although containing lighter tails than the previous algorithm, still show that the algorithm in question failed to achieve a good fit.

Although achieving a better fit, the histogram of returns still showed flaws as they were not perfectly symmetrical.

\subsection{Auto Regressive Moving Average (ARMA)}
This algorithm once again faired better than the previous algorithm, MA, having an even lower difference in sharpe ratios based on the original price returns and in-sample predicted price returns, however still failed to achieve a good fit in the in-sample tests. As can be seen in the time series analysis plots, ARMA showed to have very heavy tails in the QQ and probability plots. The algorithm also failed to forecast future price returns for certain stocks, as is evident in figure 4.81.

As can be seen from the histogram of returns, a better fit was achieved as the histogram was more symmetrical than those of the previous algorithms.

\subsection{Auto Regressive Integrated Moving Average (ARIMA)}
ARIMA faired worse than the previous algorithm, ARMA, having a higher difference in sharpe ratios based on the original price returns and in-sample predicted price returns. This was also evident in the QQ and probability plots, both of which showed to have heavy tails.

This can also be seen from the histogram of returns, in which its distribution was less normal than that of the previous algorithm.

\section{Machine Learning}

\subsection{Classification}

\subsubsection{Decision Tree}
This algorithm faired well in predicting stock price movements in the in-sample tests, having an accuracy score above 75\% for all the stocks prices that were forecast.

\subsubsection{Boosted Decision Tree}
This algorithm yielded slightly better results when compared to the previous algorithm in predicting stock price movements in the in-sample tests.

\subsubsection{Support Vector Machine (SVM)}
This algorithm once again yielded slightly better results when compared to the previous algorithm in predicting stock price movements in the in-sample tests.

\subsubsection{Random Forest}
This algorithm scored the same accuracy as the previous algorithm.

\subsubsection{K-Nearest Neighbour}
This algorithm's accuracy was not consistent as it scored mixed results when tested against all the stocks, where the accuracy is some was much lower than others. 

\subsubsection{Logistic Regression}
This algorithm faired worse than the others having not achieved an accuracy score of above 80\% when tested against all the stocks.

\subsubsection{Bernoulli Naive Bayes}
This algorithm faired the worst from the alogrithms tested so far, scoring accuracy scores in the lower 70th percentile.

\subsubsection{Gaussian Naive Bayes}
This algorithm also scored a lower accuracy score, showing that Naive Bayes isn't the best form of forecasting stock prices.

\subsubsection{Neural Network}
This algorithm, although did not score the best accuracy score from the other algorithms, still faired well in correctly predicting the price movments of the stocks.

\subsubsection{Stochastic Gradient Descent}
This algorithm faired the worst from the lot, varying heavily in accuracy scores, in which one stock had an accuracy score of 59\%. Even though it faired miuch better in correctly predicting the price movements of other stocks, this algorithm proved to be unreliable.

\subsection{Regression}

\subsubsection{Decision Tree}
As can be seen in figure 4.102, the algorithm not only failed to achieve a good fit in the in-sample test, but also failed completely in predicting any values at all. Figure 4.103 also shoes that the algorithm predicted a sharp fall at the end of the out-of-sample test, which could be related to the incorrect fit achieved in the in-sample test. This was reflected in the metric score, having a high number of data losses, and low accuracy scores.

\subsubsection{Boosted Decision Tree}
This algorithm also did not do very well, as can be seen in figures 4.112, 4.114, 4.116, and 4.118, the algorithm not only failed to achieve a good fit in the in-sample test, but also failed completely in predicting any values at all. The algorithm also predicted some sharp inclines at the end of the test, as can be seen in figure 4.115. The bad fit achieved is also reflected in the metric scores, in which there was a high number of data losses, and low accuracy scores.

\subsubsection{K-Nearest Neighbour}
This algorithm also didn't fair too well, showing bad fits in figures 4.122, 4.124, 4.126, 4.128, and 4.130. Similarly to the previous algorithm, the alforithm failed completely in predicting any values at all in some parts of the in-sample test.

\subsubsection{Random Forest}
Similarly to previous algorithms, this algorithm failed to achieve a good fit as can be seen in figures 4.132, 4.134, 4.136, 4,138, and 4.140. The algorithm also failed to predict any values at all in the of the in-sample tests, and also predicted sharp inclines and declines at the end some of the out-of-sample tests.

\subsubsection{Linear Regression} 
This algorithm performed very well in the in-sample tests, achieving a good fit with high accuracy scores above 95\% with a low number of data losses.

\subsubsection{Neural Network}
This algorithm, although scored very well in some of the in-sample tests, was not very reliable as accuracy scores varied between each test, performing extremely well in some, however not too well in others. 

\subsubsection{Stochastic Gradient Descent}
This algorithm performed fairly well in most in-sample tests, having high accuracy scores and low data losses escept for CDE as can be seen in figure 4.164.

\section{Bayesian Statistics}

\subsubsection{Metropolis-Hastings}
This algorithm failed to achieve a good fit in the in-sample tests, having a huge difference in sharpe ratios based on the original price returns and in-sample predicted price returns. 

\subsubsection{No-U-Turn-Sampler (NUTS)}
The algorithm achieved a good fit in the in-sample tests, having very similar sharpe ratios  based on the original price returns and in-sample predicted price returns.

\section{Strategy}

\subsection{Classification}
When tasked with predicting rises in stock prices, the algorithm did fairly well, marking a total return of 49.1\%, this however did not beat the benchmark's return of 100.8\%. The agorithm underperformed immensely when tasked at also predicting stock price falls, marking a negative total return of -80\%. To compensate for this, a stop loss was added to sell all positions if the stock in question falls below -20\%, this resulted in a profit of 67.5\%.

\subsection{Regression}
The performance of this algorithm was extremely similar to that of the previous one, this goes to show that both classifiers and regressors can be used for stock price prediction. When tasked with predicting rises in stock prices, the algorithm did fairly well, marking a slightly lower total return of 43\%. The agorithm also underperformed immensely when tasked at also predicting stock price falls, marking a negative total return of -80.4\%. To compensate for this, a stop loss was added to sell all positions if the stock in question falls below -20\%, this resulted in a profit of 83.7\%.

\section{Future Research}
Although this study provides important insights into the relationship between machine learning models and stock market forecasting, the models still failed to achieve a higher return than the market, thus discrediting them as a viable option for investors to incorporate in their strategy. It is however the firm belief of the author that machine learning models can indeed be used to exploit the market, using methods which were not applied to this study due to a lack of data and resources. 

Since only technical analysis was applied due to restrictions in data, a link may indeed be found between fundamental analysis and machine learning models, a relationship that may be well worth looking into. Many claim fundamental analysis to be extremely critical to financial analysis, providing a vast amount of strategies to implement. Since this type of analysis involves financial statements, such an increase in data could easily boost the predictive power of machine learning models.

Deep learning is also currently a growing field, making strides in many forms of applications. Being a subset of machine learning, it works similarly to other models except that its inner workings are designed to mimic the decision making of the human brain. However due to its requirements of a vast amount of data and powerful hardware, it was not possible to include it in this study. Just as deep learning has proven to perform extremely well in other applications, it may very well prove to do so in this field too.

Sentiment analysis is also another field which is currently proving to be a powerful tool in predicting outcomes. This form of analysis involves computationally identifying and categorising opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, is positive, negative, or neutral. This could be easily used to identify consumer sentiment which is a powerful economic indicator of the overall health of the economy as determined by consumer opinion. This could make is very easy to determine which stocks are being overvalued or undervalued, and quickly act on that information.