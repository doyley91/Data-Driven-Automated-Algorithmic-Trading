\chapter{Background Theory}

The field of computational finance consists of one of the many branches of applied computer science, where the problems dealt with are in the interest of finance. Some slightly different definitions are the study of data and algorithms currently used in finance and the mathematics of computer programs that realize financial models or systems. Using computational finance in order to allocate assets in a portfolio is not at all unheard of and was first documented  1952.\cite{Markowitz:1952aa} Markowitz first introduced the concept of portfolio selection as an exercise in mean-variance optimisation. This required more computer power than was available at the time, so he worked on useful algorithms for approximate solutions. He theorised that investors looking for low risk investments are able to form portfolios that are able to optimise or maximise expected return based on a given level of market risk, emphasising that a higher reward cannot be achieved without taking on more risk. Following his theory, it is possible to construct portfolios which are optimised to achieve the maximum possible expected return for a given level of risk.

\section{Time Series Analysis}

A time series is a series of data points which may be indexed, listed, or graphed, in a time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are daily temperature fluctuations, the number of cars on our roads, and the daily closing price of the S\&P500. Brockwell et al. provide a formal description of time series as having a set of observations xt, each one being recorded at a specific time t. A discrete-time time series is one in which the set T0 of times at which observations are made is a discrete set, as is the case, for example, when observations are made at fixed time intervals. Continuous time series are obtained when observations are recorded continuously over some time interval, e.g., when T0 = [0, 1].\cite{Peter-J.-Brockwell:2016aa}

Noteworthy "time series momentum" has likewise been achieved in equity index, currency, commodity, and bond futures for each of the 58 liquid instruments which were considered.\cite{Moskowitz:2011aa} Moskowitz et al. discover persistence in returns ranging from 1 to 12 months that partially reverses over longer horizons, consistent with sentiment theories of initial under-reaction and delayed over-reaction. Their study brought forward evidence that time series momentum strategies were found to deliver substantial abnormal returns using a diversified portfolio consisting of all asset classes having little exposure to standard asset pricing factors. Moskowitz et al. also back their claims by testing their strategies during extreme markets, which they describe to perform best in. Moskowitz et al. also studied the trading patterns of speculators and hedgers, in which they concluded that speculators tended to make profit mostly when making use of time series momentum strategies, all at the expense of hedgers.

\subsection{Auto Regressive Moving Average (ARMA)}

In the statistical analysis of time series, ARMA models provide a parsimonious description of a weakly stationary stochastic process in terms of two polynomials, one for the autoregression and the second for the moving average. The notation ARMA(p, q) refers to the model with p autoregressive terms and q moving-average terms. This model contains the AR(p) and MA(q) models,

\(x_t = c+\varepsilon_t+\sum^{q}_{i=1} \varphi_iX_{t-i}\theta_i\varepsilon_{t-i}\)

There is a growing demand for forecasting interest rates, as financial researchers, economists, and players in the fixed income markets seek to find the best method to get ahead of the market. A study was carried out to develop an appropriate model for forecasting the short-term interest rates, implicit yield on 91 day treasury bill, overnight MIBOR rate, and call money rate.\cite{Radha:2015aa} The short-term interest rates are forecasted using univariate models such as the Random Walk, ARIMA, ARMA-GARCH, and ARMA-EGARCH. The appropriate model for forecasting is determined considering a six-year period from 1999. 
Radha et al. showed evidence that GARCH models are best suited for forecastint when applied towards time series having volatility clustering effects. It was their firm belief that ARIMA-EGARCH is the most appropriate forecasting model for these circumstances.

\subsection{Auto Regressive Integrated Moving Average (ARIMA)}

In time series analysis, an autoregressive integrated moving average (ARIMA) model is quite similar to an ARMA model as it is simply a generalisation of it. Given a time series of data $X_t$ where t is an integer index and the $X_t$ are real numbers, an ARMA(p, q) model is given by

\(x_t - \alpha_1 X_{t-1} - \dots \alpha_{p'} X_{t-p'} = \varepsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}\)

The aforementioned model is fitted to time series data either to better understand the data or to predict future points in the series, also referred to as forecasting. ARIMA models are best suited for data showing non-stationarity, and require data to be normalised in most cases. These models provide an initial differencing step in order to reduce the stationarity found in data.

The existence of weak-form efficiency in the Russian stock market is examined for the period 1st September 1995 to 1st May 2001 using daily, weekly and monthly Russian Trading System index time series.\cite{Abrosimova:2002aa} Several different approaches are used to assess the predictability of the RTS index time series. Unit root, autocorrelation and variance ratio tests are conducted for the null hypothesis of a random walk model. The results support the null hypothesis for the monthly data only. Further analysis is performed for the daily and weekly data. Linear and non-linear modelling of the serial dependence is conducted using ARIMA and GARCH models estimated on the in-sample period 1st September 1995 to 1st January 2001. Forecasts based on the best fitting models are performed for the out-of-sample period 2nd January 2001 to 1st May 2001. Comparisons of the forecasts reveal that none of the models outperforms the others, and the most accurate forecasts are obtained for just the first out-of-sample observation. Whilst our research results provide some limited evidence of short-term market predictability on the RTS, there is insufficient evidence to suggest that it would lead to a profitable trading rule, once transaction costs and risk are taken into account.

Darrat et al. set out to investigate with the use of new daily data, whether prices in the two Chinese stock exchanges (Shanghai and Shenzhen) follow a random walk process as required by market efficiency.\cite{Darrat:2001aa} Two different approaches were applied, the standard variance-ratio test, and a model-comparison test that compares the ex post forecasts from a naive model with those obtained from several alternative models such as ARIMA, GARCH, and ANNs. To evaluate ex post forecasts, Darrat et al. made use of several procedures including root-mean-square error (RMSE), mean absolute error (MAE), uncertainty coefficient, and encompassing tests. It was concluded that the model-comparison approach yielded results which were quite strongly rejected the RWH in both Chinese stock markets when compared with the variance-ratio test. Darret et al. recommended the use of ANNs, as their results showed strong support for the model as a potentially useful factor for forecasting stock prices in emerging markets.

\section{Statistical Machine Learning}

A machine learning algorithm is an algorithm that is able to learn through examples from data. To understand what an algorithm is, Cormen et al. informally describe algorithms as "any well-defined computational procedures which takes some value, or set of values, as input and produce some value, or set of values, as output. An algorithm is thus a sequence of computational steps that transform the input into the output."\cite{Cormen:2009aa} In simple terms, it is possible to say that an algorithm is a sequence of steps which allow to solve a certain task. Similarly to a normal algorithm, a machine learning algorithm as defined formally by Tom M. Mitchell, states that "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."\cite{Mitchell:1997aa}

\subsection{Classification}

Classifiers are a class of machine learning algorithms which identify in which category a new observation belongs to, on the basis of a training set of data containing observations whose category membership is known. A model based on discriminant analysis was sought out to categorise a stock as manipulated or non-manipulated based on certain key variables that capture the characteristics of the stock.\cite{Murugesan:2012aa} The model in which Murugesan et al. chose, helps them identify stocks witnessing activities that are indicative of potential manipulation irrespective of the type of manipulation, such as action-based, information-based, or trade-based. The model which they proposed, helps investigators to arrive at a shortlist of securities that are potentially manipulated and which could be subject to further detailed investigation to detect the type and nature of the manipulation, if any. In a market like India, where there are about 5000 plus securities listed on its major exchanges, it becomes extremely difficult to monitor all securities for potential market abuse. Academics who have earlier used discriminant analysis have used the Linear Classification Function without validating the assumption that governs the model. Through their research, they have tested the assumption on data from the Indian capital market and found that the data does not comply with the assumptions that govern the use of the linear classification function. This therefore resulted in them using the Quadratic Classification Function, which is the appropriate technique for instances where the data does not meet the sated assumptions, to categorise stocks into two categories, namely manipulated and non-manipulated.

\subsection{Support Vector Machines}

Support vector machines (SVM), are a class of machine learning algorithms that have become incredibly popular in the past few years. SVMs are very similar to classifiers in the sense that they also classify data by drawing a line, called a decision boundary, to separate them. However, SVMs go a step further by calculating a vector from the data point with the smallest margin to the decision boundary. This is called a support vector. A vast majority of academics tend to predict the price of stocks in financial markets, however most models used are flawed and only focus on the accurate forecasting of the levels of the underlying stock index. There is a lack of studies examining the predictability of the direction of stock index movement. Given the notion that a prediction with little forecast error does not necessarily translate into capital gain, the authors of this research attempt to predict the direction of the S\&P CNX NIFTY Market Index of the National Stock Exchange, one of the fastest growing financial exchanges in developing Asian countries.\cite{Kumar:2016aa} Machine learning models such as random forest and SVMs, differ widely from other models, and are making strides in predicting the financial markets. Kumar et al. tested classification models to predict the direction of the markets, by applying models such as linear discriminant analysis, logistic regression, ANNs, random forest, and SVM. Their evidence shows that SVMs outperform the other classification methods in terms of predicting the direction of the stock market movement, and that the random forest model outperforms other models such as ANNs, discriminant analysis, and logistic regression.

\subsection{Regression}

Regression analysis is widely used for predicting and forecasting as it proves to be a powerful tool in identifying which of the independent variables contain a relationship between the dependent variable, while also allowing the forms of such relationships to be explored. In restricted circumstances, regression analysis can be used to infer causal relationships between the independent and dependent variables. Regression analysis helps one understand how the typical value of the dependent variable changes when any one of the independent variables is varied, while the other independent variables are held fixed. Kakushadze et al. provide a systematic quantitative framework in what is intended to be a “pedagogical” fashion for discussing mean-reversion and optimisation.\cite{Kakushadze:2015aa} In their paper, they start off their research with pair trading and add complexity by following the sequence “mean-reversion via demeaning, regression, weighted regression, (constrained) optimisation, factor models”. Using this apprach, Kakushadze et al. studied mean reversion models in further detail, going into the common pitfalls found in practical applications, including the difference between maximising the Sharpe ratio and minimising an objective function when trading costs are included. Kakushadze et al. also discuss explicit algorithms for optimization with linear costs, constraints and bounds, and also illustrate their discussion on an explicit intraday mean-reversion alpha.

\subsection{Decision Trees}

Decision tree learning uses a decision tree as a predictive model which maps observations about an item, represented in the branches, to conclusions about the item's target value represented in the leaves. Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values, typically real numbers, are called regression trees. In their research, Creamer et al. developed a automated trading algorithm making use of multiple stocks relying on a layered structure consisting of a machine learning algorithm, an online learning utility, and a risk management overlay.\cite{Creamer:2010aa} The machine learning algorithm which they made use of was an Alternating Decision Tree (ADT) implemented with Logitboost. Their algorithm was able to select the best combination of rules derived from well known technical analysis indicators, and the best peramaters of the indicators in question. Additionally, their online learning layer was also able to combine the output of several ADTs, suggesting a short or long position. Finally, the risk management layer in which they implemented, was able to validate the trading signal once it exceeds a specified non-zero threshold and limit the application of their trading strategy when it is not profitable. They tested the expert weighting algorithm with data of 100 randomly selected companies of the S\&P 500 index during the period 2003–2005. They found that their algorithm generated abnormal returns during the test period. Their experiments show that the boosting approach was able to improve the predictive capacity when indicators were combined and aggregated as a single predictor. Furthermode, their results indicated that the combination of indicators of different stocks were adequate in order to reduce the use of computational resources, while still maintaining an adequate predictive capacity.

\section{Bayesian Statistics}

Bayesian Statistics, a form of probabilistic programming, describes probabilistic models and then performs inference in those models. Probabilistic reasoning is a foundational technology of machine learning and has been used by companies such as Google, Amazon, and Microsoft. Probabilistic reasoning has been used for predicting stock prices, recommending movies, diagnosing computers, detecting cyber intrusions, and image detection. Gelman et al. defines bayesian inference as the process of fitting a probability model to a set of data and summarising the result by a probability distribution on the parameters of the model and on unobserved quantities such as predictions for new observations.\cite{Gelman:2014aa}

\subsection{Markov Chain Monte Carlo (MCMC)}

In statistics, (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution of its equilibrium distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.

Bond yields today are well below and stock market valuations are well above their historical average.\cite{Blanchett:2013aa} There are no historical periods in the United States where comparable low bond yields and high equity valuations have occurred simultaneously. It has been proven in the past that both current bond yields and stock values hold predictive power for near-term returns. It is also known for portfolio returns to have an outsize impact on retirement income strategies in the first decade. Traditional Monte Carlo simulation approaches generally do not incorporate market valuations into their analysis. In order to simulate how retirees will fare in a low return environment for both stocks and bonds, Blanchett et al. incorporate the predictive ability of current valuations to simulate its impact on retirement portfolios. Blanchett et al. estimate bond returns through an autoregressive model that uses an initial bond yield value where yields drift in the future. Blanchett et al. use the cyclically adjusted price-to-earnings (CAPE) ratio as an estimate of market valuation to predict short-run stock performance. In their tests, they ran a number of simulations to show this, in which it was evident that risk factor of widthdrawal strategies are significantly affected by the initial bond yield and CAPE value at retirement, and that the relative impact varies based on the portfolio equity allocation. Using valuation measures from April 15, 2013, having a bond yield of 2.0\% and a CAPE of 22, Blanchett et al. the probability of success to be approximately 48\% when having a 40\% equity allocation with a 4\% initial withdrawal rate over a 30 year period. They concluded that the success rate in question is far lower than previous studies, having serious implications on the likelihood of success for retirees today, as well as how much those near retirement may need to save to ensure a successful retirement.

Hoffman et al. introduce the 'No-U-Turn-Sampler', an extension to the Hamiltonian Monte Carlo (HMC), which is an MCMC algorithm that avoids the random walk behavioir and sensitivity to correlated paramaters that plague many MCMC methods by taking a series of steps informed by first-order gradient information.\cite{Matthew-D.-Hoffman:2014aa} In their paper, they claim HMC’s performance to be highly sensitive to two user-specified parameters: a step size, and a desired number of steps. NUTS is an improvement from HMC as it eliminates the need to set a number of steps, and works by build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. They achieved all of this by making use of a recursive algorithm.