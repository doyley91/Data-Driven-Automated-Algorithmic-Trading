\chapter{Background Theory}

The field of computational finance consists of one of the many branches of applied computer science, where the problems dealt with are in the interest of finance. Some slightly different definitions are the study of data and algorithms currently used in finance and the mathematics of computer programs that realize financial models or systems. Using computational finance in order to allocate assets in a portfolio is not at all unheard of and was first documented  1952.\cite{Markowitz:1952aa} Markowitz first introduced the concept of portfolio selection as an exercise in mean-variance optimisation. This required more computer power than was available at the time, so he worked on useful algorithms for approximate solutions. He theorised that investors looking for low risk investments are able to form portfolios that are able to optimise or maximise expected return based on a given level of market risk, emphasising that a higher reward cannot be achieved without taking on more risk. Following his theory, it is possible to construct portfolios which are optimised to achieve the maximum possible expected return for a given level of risk.

\section{Time Series Analysis}

A time series is a series of data points which may be indexed, listed, or graphed, in a time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are daily temperature fluctuations, the number of cars on our roads, and the daily closing price of the S\&P500. Brockwell et al. provide a formal description of time series as having a set of observations xt, each one being recorded at a specific time t. A discrete-time time series is one in which the set T0 of times at which observations are made is a discrete set, as is the case, for example, when observations are made at fixed time intervals. Continuous time series are obtained when observations are recorded continuously over some time interval, e.g., when T0 = [0, 1].\cite{Peter-J.-Brockwell:2016aa}

Noteworthy "time series momentum" has likewise been achieved in equity index, currency, commodity, and bond futures for each of the 58 liquid instruments which were considered.\cite{Moskowitz:2011aa} Moskowitz et al. discover persistence in returns ranging from 1 to 12 months that partially reverses over longer horizons, consistent with sentiment theories of initial under-reaction and delayed over-reaction. Their study brought forward evidence that time series momentum strategies were found to deliver substantial abnormal returns using a diversified portfolio consisting of all asset classes having little exposure to standard asset pricing factors. Moskowitz et al. also back their claims by testing their strategies during extreme markets, which they describe to perform best in. Moskowitz et al. also studied the trading patterns of speculators and hedgers, in which they concluded that speculators tended to make profit mostly when making use of time series momentum strategies, all at the expense of hedgers.

\subsection{Auto Regressive Moving Average (ARMA)}

In the statistical analysis of time series, ARMA models provide a parsimonious description of a weakly stationary stochastic process in terms of two polynomials, one for the autoregression and the second for the moving average. The notation ARMA(p, q) refers to the model with p autoregressive terms and q moving-average terms. This model contains the AR(p) and MA(q) models,

\(x_t = c+\varepsilon_t+\sum^{q}_{i=1} \varphi_iX_{t-i}\theta_i\varepsilon_{t-i}\)

There is a growing demand for forecasting interest rates, as financial researchers, economists, and players in the fixed income markets seek to find the best method to get ahead of the market. A financial model was developed to forecast short-term interest rates, implicit yield on 91 day treasury bill, overnight MIBOR rate, and call money rate.\cite{Radha:2015aa} Radha et al. made use of univariate models to forecast the short-term interest rates. These models include Random Walk, ARIMA, ARMA-GARCH, and ARMA-EGARCH. They selected the best peforming model by considering a six year period starting from 1999. Radha et al. showed evidence that GARCH models are best suited for forecastint when applied towards time series having volatility clustering effects. It was their firm belief that ARIMA-EGARCH is the most appropriate forecasting model for these circumstances.

\subsection{Auto Regressive Integrated Moving Average (ARIMA)}

In time series analysis, an autoregressive integrated moving average (ARIMA) model is quite similar to an ARMA model as it is simply a generalisation of it. Given a time series of data $X_t$ where t is an integer index and the $X_t$ are real numbers, an ARMA(p, q) model is given by

\(x_t - \alpha_1 X_{t-1} - \dots \alpha_{p'} X_{t-p'} = \varepsilon_t + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}\)

The aforementioned model is fitted to time series data either to better understand the data or to predict future points in the series, also referred to as forecasting. ARIMA models are best suited for data showing non-stationarity, and require data to be normalised in most cases. These models provide an initial differencing step in order to reduce the stationarity found in data.

Abrosimova et al. examined the existence of weak-form efficiency in the Russian stock market for the period 1st September 1995 to 1st May 2001, whule making use of daily, weekly, and monthly Russian Trading System index data.\cite{Abrosimova:2002aa} They made us of a number of approaches in order to determine the predictability of the RTS index time series. Abrosimova et al. tested the null hypothesis of a random walk model using unit root, autocorrelation, and variance ratio tests. It was evident in their results that the null hypothesis was only true for the monthly data, making them focus more on the daily and weekly data. Using linear and non-linear modelling, they fit ARIMA and GARCH models to the data in the in-sample tests over the period 1st September 1995 to 1st January 2001. Abrosimova et al. carried out forecast be selecting the models which achieved the best fit in the out-of-same tests over the period 2nd January 2001 to 1st May 2001. Their results clearly indicated that there was no clear winner from the models, and the most accurate forecasts were only obtained in the out-of-sample tests. Although their research shed some light on the predictability of the RTS, there was not enough evidence to suggest that it would lead to a profitable trading rule, once transaction costs and risk were taken into account.

Darrat et al. set out to investigate with the use of new daily data, whether prices in the two Chinese stock exchanges (Shanghai and Shenzhen) follow a random walk process as required by market efficiency.\cite{Darrat:2001aa} Two different approaches were applied, the standard variance-ratio test, and a model-comparison test that compares the ex post forecasts from a naive model with those obtained from several alternative models such as ARIMA, GARCH, and ANNs. To evaluate ex post forecasts, Darrat et al. made use of several procedures including root-mean-square error (RMSE), mean absolute error (MAE), uncertainty coefficient, and encompassing tests. It was concluded that the model-comparison approach yielded results which were quite strongly rejected the RWH in both Chinese stock markets when compared with the variance-ratio test. Darret et al. recommended the use of ANNs, as their results showed strong support for the model as a potentially useful factor for forecasting stock prices in emerging markets.

\section{Statistical Machine Learning}

A machine learning algorithm is an algorithm that is able to learn through examples from data. To understand what an algorithm is, Cormen et al. informally describe algorithms as "any well-defined computational procedures which takes some value, or set of values, as input and produce some value, or set of values, as output. An algorithm is thus a sequence of computational steps that transform the input into the output."\cite{Cormen:2009aa} In simple terms, it is possible to say that an algorithm is a sequence of steps which allow to solve a certain task. Similarly to a normal algorithm, a machine learning algorithm as defined formally by Tom M. Mitchell, states that "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."\cite{Mitchell:1997aa}

\subsection{Classification}

Classification is a form of machine learning which is used to categorise data, such as predicting whether a tumor is benign or malignant. Murugesan et al. used this form of machine learning to categorise a stock as manipulated or non-manipulated depending on certain features in the data that characterises the particular stock.\cite{Murugesan:2012aa} The model they selected was able to identify particular stocks which were prone to any kind of potential manipulation, including action-based, information-based, and trade-based. Murugesan et al. aided investigators to narrow down a list of stocks being potentially manipulated requiring further investigation down the line to determine the root of the manipulation. This model was of great use for markets like India, where over 5000 stocks are traded every day on the major exchanges, making it cumbersome to track all stocks for potential manipulation. Murugesan et al. carried out their research by testing their model on data from the Indian capital market. It was made evident to them from the start of their research that the data does not comply with the assumptions that govern the use of the linear classification function, which goes against academics who have earlier used discriminant analysis have used the Linear classification function. Murugesan et al. claimed that this was because academics failed to validate the assumption that governs the model in their studies. It was for these reasons that Murugesan et al. decided to use the Quadratic classification function, being the appropriate methoid for instances where the data does not meet the assumptions, to categorise stocks into two categories, being manipulated and non-manipulated.

\subsection{Support Vector Machines}

Support vector machines (SVM), are a class of machine learning algorithms that have become incredibly popular in the past few years. SVMs are very similar to classifiers in the sense that they also classify data by drawing a line, called a decision boundary, to separate them. However, SVMs go a step further by calculating a vector from the data point with the smallest margin to the decision boundary. This is called a support vector. A vast majority of academics tend to predict the price of stocks in financial markets, however most models used are flawed and only focus on the accurate forecasting of the levels of the underlying stock index. There is a lack of studies examining the predictability of the direction of stock index movement. Given the notion that a prediction with little forecast error does not necessarily translate into capital gain, the authors of this research attempt to predict the direction of the S\&P CNX NIFTY Market Index of the National Stock Exchange, one of the fastest growing financial exchanges in developing Asian countries.\cite{Kumar:2016aa} Machine learning models such as random forest and SVMs, differ widely from other models, and are making strides in predicting the financial markets. Kumar et al. tested classification models to predict the direction of the markets, by applying models such as linear discriminant analysis, logistic regression, ANNs, random forest, and SVM. Their evidence shows that SVMs outperform the other classification methods in terms of predicting the direction of the stock market movement, and that the random forest model outperforms other models such as ANNs, discriminant analysis, and logistic regression.

\subsection{Regression}

Regression is another form of machine learning which is used to predict data, allowing researcgers to correctly identify the relationship that lies between the independent dependent variables. These models make it easier to understand how the typical value of the dependent variable changes when any one of the independent variables is varied, while the other independent variables are held fixed. Kakushadze et al. provide a systematic quantitative framework in what is intended to be a “pedagogical” fashion for discussing mean-reversion and optimisation.\cite{Kakushadze:2015aa} In their paper, they start off their research with pair trading and add complexity by following the sequence “mean-reversion via demeaning, regression, weighted regression, (constrained) optimisation, factor models”. Using this approach, Kakushadze et al. studied mean reversion models in further detail, going into the common pitfalls found in practical applications, including the difference between maximising the Sharpe ratio and minimising an objective function when trading costs are included. Kakushadze et al. also discuss explicit algorithms for optimization with linear costs, constraints and bounds, and also illustrate their discussion on an explicit intraday mean-reversion alpha.

\subsection{Decision Trees}

Decision trees are a class of machine learning models which make use of a decision tree as a predictive model which creates a map of observations about an item, represents them in the form branches, to conclude about the item's target value represented in the form of leaves. There are two types of decision trees, classification trees, which its target variable can take a finite set of values, and regression trees, in which the target variable can take continuous values. In classification trees, each leaf represents a class label, and each branche represents conjunctions of features that lead to those class labels. In their research, Creamer et al. developed an automated trading algorithm making use of multiple stocks relying on a layered structure consisting of a machine learning algorithm, an online learning utility, and a risk management overlay.\cite{Creamer:2010aa} The machine learning algorithm which they made use of was an Alternating Decision Tree (ADT) implemented with Logitboost. Their algorithm was able to select the best combination of rules derived from well known technical analysis indicators, and the best peramaters of the indicators in question. Additionally, their online learning layer was also able to combine the output of several ADTs, suggesting a short or long position. Finally, the risk management layer in which they implemented, was able to validate the trading signal once it exceeds a specified non-zero threshold and limit the application of their trading strategy when it is not profitable. They tested the expert weighting algorithm with data of 100 randomly selected companies of the S\&P 500 index during the period 2003–2005. They found that their algorithm generated abnormal returns during the test period. Their experiments show that the boosting approach was able to improve the predictive capacity when indicators were combined and aggregated as a single predictor. Furthermode, their results indicated that the combination of indicators of different stocks were adequate in order to reduce the use of computational resources, while still maintaining an adequate predictive capacity.

\section{Bayesian Statistics}

Bayesian Statistics, a form of probabilistic programming, describes probabilistic models and then performs inference in those models. Probabilistic reasoning is a foundational technology of machine learning and has been used by companies such as Google, Amazon, and Microsoft. Probabilistic reasoning has been used for predicting stock prices, recommending movies, diagnosing computers, detecting cyber intrusions, and image detection. Gelman et al. defines bayesian inference as the process of fitting a probability model to a set of data and summarising the result by a probability distribution on the parameters of the model and on unobserved quantities such as predictions for new observations.\cite{Gelman:2014aa}

\subsection{Markov Chain Monte Carlo (MCMC)}

MCMC models are a family of algorithms for sampling from a probability distribution based on constructing a Markov Chain that has the desired distribution of its equilibrium distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps. At the time of conducting their research, bond yields were well below and stock market valuations were well above their historical average.\cite{Blanchett:2013aa} Blanchett et al. claim this to be a unique opportunity as this has never happened before in the United States. It is also known for portfolio returns to have an outsize impact on retirement income strategies in the first decade. When conducting their research, academics do not normally incorporate market valuations in their study. This is of course a flawed approach as a big piece of the puzzle is being left out. Blanchett et al. set out to simulate how a low return environment for both stocks and bonds would affect retirees. In their study, they used current valuations to predict the impact on retirement portfolios. Blanchett et al. used an autoregressive model to estimate bond returns, making use of an initial bond yield value where yields drift in the future. CAPE ratios were also used in order to estimate the market valuation and predict short-run stock peroformance. It was evident from the number of sumulations that they ran, that the risk factor of widthdrawal strategies was directly affected by the initial bond yield and the CAPE value at retirement. The relative impact was also found to vary based on the portfolio stock allocation. Blanchett et al. found the probability of success to sit at approximately 48\% when having a 40\% stock allocation with a 4\% initial withdrawal rate over a 30 year period by using the valuation measures from April 15, 2013. They concluded that the success rate in question is far lower than previous studies, which points to having serious implications on the likelihood of success for retirees today, as well as how much those near retirement may need to save to ensure a successful retirement.

In their paper, Hoffman et al. introduced the NUTS model, an improvement to the HMC model. The NUTS model is a model of MCMC family of algorithms, in which its main focus is to avoid the random walk behavioir and sensitivity to correlated paramaters affecting many other MCMC models. Hoffman et al. achieved this by taking a series of steps informed by first-order gradient information.\cite{Matthew-D.-Hoffman:2014aa} In their study, they claim HMC to be flawed as its performance is highly sensitive to two user-specified parameters, the step size, and the desired number of steps. NUTS is an improvement from HMC as it eliminates the need for the researcher to set the number of steps, and works by building a set of likely candidate points which span a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. They achieved all of this by making use of a recursive algorithm.