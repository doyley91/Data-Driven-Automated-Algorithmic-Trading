<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Data Driven Automated Algorithmic Trading</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html,htex4ht --> 
<meta name="src" content="Thesis.tex"> 
<meta name="date" content="2017-06-04 22:27:00"> 
<link rel="stylesheet" type="text/css" href="Thesis.css"> 
</head><body 
>
                                                                                
                                                                                
<div class="maketitle">
                                                                                
                                                                                
                                                                                
                                                                                
<a 
href="University" Web Site URL Here (include http://mcast.edu.mt/) ><span 
class="cmbx-12">MCAST</span></a>

<h2 class="titleHead">Data Driven Automated Algorithmic
Trading</h2>
<span 
class="cmr-17">by</span>
<a 
href="gabriel@gaucimaistre.com" ><span 
class="cmr-17">Gabriel Gauci Maistre</span></a>
<span 
class="cmr-12">A thesis submitted in partial fulfillment for the</span>
<span 
class="cmr-12">degree of Bachelor of Science</span>
<span 
class="cmr-12">in the</span>
<a 
href="Faculty" Web Site URL Here (include http://ict.mcast.edu.mt/) ><span 
class="cmr-12">Information and Communications Technology</span></a>
<a 
href="Department" or School Web Site URL Here (include http://mcast.edu.mt/) ><span 
class="cmr-12">Malta College of Art, Science, and Technology</span></a>
<div class="date" ><span 
class="cmr-12x-x-120">June 2017</span></div>
                                                                                
                                                                                
                                                                                
                                                                                
</div>
<a 
 id="x1-2r1"></a>
<a 
 id="Q1-1-1"></a>
<div class="center" 
>
<!--l. 71--><p class="noindent" >
<!--l. 71--><p class="noindent" ><span 
class="cmbx-12x-x-172">Declaration of Authorship</span>
</div>
<!--l. 71--><p class="noindent" >I, Gabriel Gauci Maistre, declare that this thesis titled, &#8216;Data Driven Automated Algorithmic
Trading&#8217; and the work presented in it are my own. I confirm that:
<ul class="itemize1">
<li class="itemize">This work was done wholly or mainly while in candidature for a research degree
at this University.
</li>
<li class="itemize">Where any part of this thesis has previously been submitted for a degree or any
other qualification at this University or any other institution, this has been clearly
stated.
</li>
<li class="itemize">Where I  have  consulted  the  published  work  of  others,  this  is  always  clearly
attributed.
</li>
<li class="itemize">Where I have quoted from the work of others, the source is always given. With the
exception of such quotations, this thesis is entirely my own work.
</li>
<li class="itemize">I have acknowledged all main sources of help.
</li>
<li class="itemize">Where the thesis is based on work done by myself jointly with others, I have made
clear exactly what was done by others and what I have contributed myself. <br 
class="newline" /></li></ul>
                                                                                
                                                                                
<!--l. 71--><p class="noindent" >Signed:<br 
class="newline" />____________________________________________________
<!--l. 71--><p class="noindent" >Date:<br 
class="newline" />____________________________________________________
                                                                                
                                                                                
<!--l. 80--><p class="noindent" ><span 
class="cmti-10x-x-109">&#8220;If past history was all there was to the game, the richest people would be librarians.&#8221;</span>
                                                                  <div class="flushright" 
>
<!--l. 82--><p class="noindent" >
Warren Buffett</div>
                                                                                
                                                                                
<a 
 id="x1-3r2"></a>
<a 
 id="Q1-1-2"></a>
<div class="center" 
>
<!--l. 92--><p class="noindent" >
<!--l. 92--><p class="noindent" ><a 
href="University" Web Site URL Here (include http://mcast.edu.mt/) >MCAST</a>
<!--l. 92--><p class="noindent" ><span 
class="cmti-12x-x-172">Abstract</span>
<!--l. 92--><p class="noindent" ><a 
href="Faculty" Web Site URL Here (include http://ict.mcast.edu.mt/) >Information and Communications Technology</a>
<!--l. 92--><p class="noindent" ><a 
href="Department" or School Web Site URL Here (include http://mcast.edu.mt/) >Malta College of Art, Science, and Technology</a>
<!--l. 92--><p class="noindent" >Bachelor of Science
<!--l. 92--><p class="noindent" >by <a 
href="gabriel@gaucimaistre.com" >Gabriel Gauci Maistre</a>
</div>
<!--l. 95--><p class="noindent" >Various existing stock market price forecasting methods were analysed in this report. Three
methods were applied towards the problem making use of Technical Analysis, these were Time
Series Analysis, Machine Learning, and Bayesian Statistics. Through the results of this report,
it was found that the Efficient Market Hypothesis remains true, that past data does not
contain enough useful information to forecast future prices and gain an advantage over the
market. However, the results proved that Technical Analysis and Machine Learning could still
be used to guide an investors decision. It was also found that the Random Walk Hypothesis
was not necessarily true, as some stocks showed signs of auto and partial correlation. A
common application of technical analysis was demonstrated and shown to produce limited
useful information in beating the market. Based on the findings, a number of automated
trading algorithms were developed using machine learning and backtested to determine their
effectiveness.
                                                                                
                                                                                
<a 
 id="x1-4r3"></a>
<a 
 id="Q1-1-3"></a>
<div class="center" 
>
<!--l. 118--><p class="noindent" >
<!--l. 118--><p class="noindent" ><span 
class="cmti-12x-x-172">Acknowledgements</span>
</div>
<!--l. 118--><p class="noindent" >I would like to express my special thanks of gratitude to my supervisor, Alan Gatt, for the
patient guidance, encouragement, and advice he has provided throughout my time as his
student.
<!--l. 118--><p class="noindent" >I would also like to thank Luke Vella Critien, for guiding me towards the right parth in the
early stages of my research and for recommending Alan Gatt as my tutor.
<!--l. 118--><p class="noindent" >My gratitude is also extended to Emma Galea and Miguel attard for their valuable input
while carrying out my research.
<!--l. 118--><p class="noindent" >Completing this work would have been all the more difficult were it not for the support and
friendship provided by the other members of the Malta College of Arts, Sciences, and
Technology, and the institute of Information and Technology. I am indebted to them for their
help.
<!--l. 118--><p class="noindent" >Finally, I would like to thank my family who have supported me all throughout the final year
of my bachelor&#8217;s.
                                                                                
                                                                                
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-10003"></a>Contents</h2> <div class="tableofcontents">
<span class="chapterToc" ><a 
href="#Q1-1-1">Declaration of Authorship</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-2">Abstract</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-3">Acknowledgements</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-5">List of Figures</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-7">List of Tables</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-9">Abbreviations</a></span>
<br /><span class="chapterToc" >1 <a 
href="#x1-50001" id="QQ2-1-11">Introduction</a></span>
<br />&#x00A0;<span class="sectionToc" >1.1 <a 
href="#x1-60001.1" id="QQ2-1-12">Project goals and scope</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >1.1.1 <a 
href="#x1-70001.1.1" id="QQ2-1-13">Efficient Market Hypothesis</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >1.1.2 <a 
href="#x1-80001.1.2" id="QQ2-1-14">Random Walk Hypothesis</a></span>
<br /><span class="chapterToc" >2 <a 
href="#x1-90002" id="QQ2-1-15">Background Theory</a></span>
<br />&#x00A0;<span class="sectionToc" >2.1 <a 
href="#x1-100002.1" id="QQ2-1-16">Time Series Analysis</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.1.1 <a 
href="#x1-110002.1.1" id="QQ2-1-17">Auto Regressive Moving Average (ARMA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.1.2 <a 
href="#x1-120002.1.2" id="QQ2-1-18">Auto Regressive Integrated Moving Average (ARIMA)</a></span>
<br />&#x00A0;<span class="sectionToc" >2.2 <a 
href="#x1-130002.2" id="QQ2-1-19">Statistical Machine Learning</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.2.1 <a 
href="#x1-140002.2.1" id="QQ2-1-20">Classification</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.2.2 <a 
href="#x1-150002.2.2" id="QQ2-1-21">Support Vector Machines</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.2.3 <a 
href="#x1-160002.2.3" id="QQ2-1-22">Regression</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.2.4 <a 
href="#x1-170002.2.4" id="QQ2-1-23">Decision Trees</a></span>
<br />&#x00A0;<span class="sectionToc" >2.3 <a 
href="#x1-180002.3" id="QQ2-1-24">Bayesian Statistics</a></span>
                                                                                
                                                                                
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.3.1 <a 
href="#x1-190002.3.1" id="QQ2-1-25">Markov Chain Monte Carlo (MCMC)</a></span>
<br /><span class="chapterToc" >3 <a 
href="#x1-200003" id="QQ2-1-26">Experimental Setup</a></span>
<br />&#x00A0;<span class="sectionToc" >3.1 <a 
href="#x1-210003.1" id="QQ2-1-27">Data Tidying</a></span>
<br />&#x00A0;<span class="sectionToc" >3.2 <a 
href="#x1-220003.2" id="QQ2-1-28">Stock Selection</a></span>
<br />&#x00A0;<span class="sectionToc" >3.3 <a 
href="#x1-230003.3" id="QQ2-1-29">Time Series Analysis</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.1 <a 
href="#x1-240003.3.1" id="QQ2-1-30">Random Walk</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.2 <a 
href="#x1-250003.3.2" id="QQ2-1-31">Ordinary Least Squares (OLS)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.3 <a 
href="#x1-260003.3.3" id="QQ2-1-32">Auto Regressive (AR)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.4 <a 
href="#x1-270003.3.4" id="QQ2-1-33">Moving Average (MA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.5 <a 
href="#x1-280003.3.5" id="QQ2-1-34">Auto Regressive Moving Average (ARMA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.6 <a 
href="#x1-290003.3.6" id="QQ2-1-35">Auto Regressive Integrated Moving Average (ARIMA)</a></span>
<br />&#x00A0;<span class="sectionToc" >3.4 <a 
href="#x1-300003.4" id="QQ2-1-36">Machine Learning</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.4.1 <a 
href="#x1-310003.4.1" id="QQ2-1-37">Classification</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.1 <a 
href="#x1-320003.4.1.1" id="QQ2-1-38">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.2 <a 
href="#x1-330003.4.1.2" id="QQ2-1-39">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.3 <a 
href="#x1-340003.4.1.3" id="QQ2-1-40">Support Vector Machine (SVM)</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.4 <a 
href="#x1-350003.4.1.4" id="QQ2-1-41">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.5 <a 
href="#x1-360003.4.1.5" id="QQ2-1-42">K-Nearest Neighbour</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.6 <a 
href="#x1-370003.4.1.6" id="QQ2-1-43">Logistic Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.7 <a 
href="#x1-380003.4.1.7" id="QQ2-1-44">Bernoulli Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.8 <a 
href="#x1-390003.4.1.8" id="QQ2-1-45">Gaussian Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.9 <a 
href="#x1-400003.4.1.9" id="QQ2-1-46">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.10 <a 
href="#x1-410003.4.1.10" id="QQ2-1-47">Stochastic Gradient Descent</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.4.2 <a 
href="#x1-420003.4.2" id="QQ2-1-48">Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.1 <a 
href="#x1-430003.4.2.1" id="QQ2-1-49">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.2 <a 
href="#x1-440003.4.2.2" id="QQ2-1-50">Boosted Decision Tree</a></span>
                                                                                
                                                                                
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.3 <a 
href="#x1-450003.4.2.3" id="QQ2-1-51">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.4 <a 
href="#x1-460003.4.2.4" id="QQ2-1-52">Linear Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.5 <a 
href="#x1-470003.4.2.5" id="QQ2-1-53">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.6 <a 
href="#x1-480003.4.2.6" id="QQ2-1-54">Stochastic Gradient Descent</a></span>
<br />&#x00A0;<span class="sectionToc" >3.5 <a 
href="#x1-490003.5" id="QQ2-1-55">Bayesian Statistics</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.5.1 <a 
href="#x1-500003.5.1" id="QQ2-1-56">No-U-Turn Sampler (NUTS)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.5.2 <a 
href="#x1-510003.5.2" id="QQ2-1-57">Metropolis-Hastings</a></span>
<br />&#x00A0;<span class="sectionToc" >3.6 <a 
href="#x1-520003.6" id="QQ2-1-58">Strategy</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.6.1 <a 
href="#x1-530003.6.1" id="QQ2-1-59">Classification</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.6.2 <a 
href="#x1-540003.6.2" id="QQ2-1-60">Regression</a></span>
<br /><span class="chapterToc" >4 <a 
href="#x1-550004" id="QQ2-1-61">Research Findings</a></span>
<br />&#x00A0;<span class="sectionToc" >4.1 <a 
href="#x1-560004.1" id="QQ2-1-64">Time Series Analysis</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.1 <a 
href="#x1-570004.1.1" id="QQ2-1-65">Random Walk</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.2 <a 
href="#x1-580004.1.2" id="QQ2-1-76">Ordinary Least Squares (OLS)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.3 <a 
href="#x1-590004.1.3" id="QQ2-1-87">Auto Regressive (AR)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.4 <a 
href="#x1-600004.1.4" id="QQ2-1-108">Moving Average (MA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.5 <a 
href="#x1-610004.1.5" id="QQ2-1-129">Auto Regressive Moving Average (ARMA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.6 <a 
href="#x1-620004.1.6" id="QQ2-1-150">Auto Regressive Integrated Moving Average (ARIMA)</a></span>
<br />&#x00A0;<span class="sectionToc" >4.2 <a 
href="#x1-630004.2" id="QQ2-1-171">Machine Learning</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.2.1 <a 
href="#x1-640004.2.1" id="QQ2-1-172">Classification</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.1 <a 
href="#x1-650004.2.1.1" id="QQ2-1-173">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.2 <a 
href="#x1-660004.2.1.2" id="QQ2-1-175">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.3 <a 
href="#x1-670004.2.1.3" id="QQ2-1-177">Support Vector Machine (SVM)</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.4 <a 
href="#x1-680004.2.1.4" id="QQ2-1-179">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.5 <a 
href="#x1-690004.2.1.5" id="QQ2-1-181">K-Nearest Neighbour</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.6 <a 
href="#x1-700004.2.1.6" id="QQ2-1-183">Logistic Regression</a></span>
                                                                                
                                                                                
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.7 <a 
href="#x1-710004.2.1.7" id="QQ2-1-185">Gaussian Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.8 <a 
href="#x1-720004.2.1.8" id="QQ2-1-187">Bernoulli Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.9 <a 
href="#x1-730004.2.1.9" id="QQ2-1-189">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.10 <a 
href="#x1-740004.2.1.10" id="QQ2-1-191">Stochastic Gradient Descent</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.2.2 <a 
href="#x1-750004.2.2" id="QQ2-1-193">Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.1 <a 
href="#x1-760004.2.2.1" id="QQ2-1-194">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.2 <a 
href="#x1-770004.2.2.2" id="QQ2-1-205">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.3 <a 
href="#x1-780004.2.2.3" id="QQ2-1-216">K-Nearest Neighbour</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.4 <a 
href="#x1-790004.2.2.4" id="QQ2-1-227">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.5 <a 
href="#x1-800004.2.2.5" id="QQ2-1-238">Linear Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.6 <a 
href="#x1-810004.2.2.6" id="QQ2-1-249">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.7 <a 
href="#x1-820004.2.2.7" id="QQ2-1-260">Stochastic Gradient Descent</a></span>
<br />&#x00A0;<span class="sectionToc" >4.3 <a 
href="#x1-830004.3" id="QQ2-1-271">Bayesian Statistics</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.3.1 <a 
href="#x1-840004.3.1" id="QQ2-1-272">Metropolis-Hastings</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.3.2 <a 
href="#x1-850004.3.2" id="QQ2-1-283">No-U-Turn Sampler (NUTS)</a></span>
<br />&#x00A0;<span class="sectionToc" >4.4 <a 
href="#x1-860004.4" id="QQ2-1-294">Strategy</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.4.1 <a 
href="#x1-870004.4.1" id="QQ2-1-295">Classification</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.4.2 <a 
href="#x1-880004.4.2" id="QQ2-1-302">Regression</a></span>
<br /><span class="chapterToc" >5 <a 
href="#x1-890005" id="QQ2-1-309">Discussion and Suggestions for Future Research</a></span>
<br />&#x00A0;<span class="sectionToc" >5.1 <a 
href="#x1-900005.1" id="QQ2-1-310">Time Series Analysis</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.1 <a 
href="#x1-910005.1.1" id="QQ2-1-311">Random Walk</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.2 <a 
href="#x1-920005.1.2" id="QQ2-1-312">Ordinary Least Squares (OLS)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.3 <a 
href="#x1-930005.1.3" id="QQ2-1-313">Auto Regressive (AR)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.4 <a 
href="#x1-940005.1.4" id="QQ2-1-314">Moving Average (MA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.5 <a 
href="#x1-950005.1.5" id="QQ2-1-315">Auto Regressive Moving Average (ARMA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.6 <a 
href="#x1-960005.1.6" id="QQ2-1-316">Auto Regressive Integrated Moving Average (ARIMA)</a></span>
                                                                                
                                                                                
<br />&#x00A0;<span class="sectionToc" >5.2 <a 
href="#x1-970005.2" id="QQ2-1-317">Machine Learning</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.2.1 <a 
href="#x1-980005.2.1" id="QQ2-1-318">Classification</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.1 <a 
href="#x1-990005.2.1.1" id="QQ2-1-319">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.2 <a 
href="#x1-1000005.2.1.2" id="QQ2-1-320">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.3 <a 
href="#x1-1010005.2.1.3" id="QQ2-1-321">Support Vector Machine (SVM)</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.4 <a 
href="#x1-1020005.2.1.4" id="QQ2-1-322">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.5 <a 
href="#x1-1030005.2.1.5" id="QQ2-1-323">K-Nearest Neighbour</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.6 <a 
href="#x1-1040005.2.1.6" id="QQ2-1-324">Logistic Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.7 <a 
href="#x1-1050005.2.1.7" id="QQ2-1-325">Bernoulli Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.8 <a 
href="#x1-1060005.2.1.8" id="QQ2-1-326">Gaussian Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.9 <a 
href="#x1-1070005.2.1.9" id="QQ2-1-327">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.10 <a 
href="#x1-1080005.2.1.10" id="QQ2-1-328">Stochastic Gradient Descent</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.2.2 <a 
href="#x1-1090005.2.2" id="QQ2-1-329">Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.1 <a 
href="#x1-1100005.2.2.1" id="QQ2-1-330">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.2 <a 
href="#x1-1110005.2.2.2" id="QQ2-1-331">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.3 <a 
href="#x1-1120005.2.2.3" id="QQ2-1-332">K-Nearest Neighbour</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.4 <a 
href="#x1-1130005.2.2.4" id="QQ2-1-333">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.5 <a 
href="#x1-1140005.2.2.5" id="QQ2-1-334">Linear Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.6 <a 
href="#x1-1150005.2.2.6" id="QQ2-1-335">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.7 <a 
href="#x1-1160005.2.2.7" id="QQ2-1-336">Stochastic Gradient Descent</a></span>
<br />&#x00A0;<span class="sectionToc" >5.3 <a 
href="#x1-1170005.3" id="QQ2-1-337">Bayesian Statistics</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.3.0.1 <a 
href="#x1-1180005.3.0.1" id="QQ2-1-338">Metropolis-Hastings</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.3.0.2 <a 
href="#x1-1190005.3.0.2" id="QQ2-1-339">No-U-Turn-Sampler (NUTS)</a></span>
<br />&#x00A0;<span class="sectionToc" >5.4 <a 
href="#x1-1200005.4" id="QQ2-1-340">Strategy</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.4.1 <a 
href="#x1-1210005.4.1" id="QQ2-1-341">Classification</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.4.2 <a 
href="#x1-1220005.4.2" id="QQ2-1-342">Regression</a></span>
                                                                                
                                                                                
<br /><span class="chapterToc" >6 <a 
href="#x1-1230006" id="QQ2-1-343">Conclusion</a></span>
<br /><span class="appendixToc" >A <a 
href="#x1-124000A" id="QQ2-1-344">An Appendix</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-345">Bibliography</a></span>
</div>
<a 
 id="x1-1001r4"></a>
<a 
 id="Q1-1-5"></a>
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-20004"></a>List of Figures</h2> <div class="tableofcontents"><span class="lofToc" >4.1&#x00A0;<a 
href="#x1-550011">Basket of stocks</a></span><br /><span class="lofToc" >4.2&#x00A0;<a 
href="#x1-570012">MSFT time series analysis</a></span><br /><span class="lofToc" >4.3&#x00A0;<a 
href="#x1-570023">MSFT
histogram of returns</a></span><br /><span class="lofToc" >4.4&#x00A0;<a 
href="#x1-570034">CDE time series analysis</a></span><br /><span class="lofToc" >4.5&#x00A0;<a 
href="#x1-570045">CDE histogram of
returns</a></span><br /><span class="lofToc" >4.6&#x00A0;<a 
href="#x1-570056">NAVB time series analysis</a></span><br /><span class="lofToc" >4.7&#x00A0;<a 
href="#x1-570067">NAVB histogram of returns</a></span><br /><span class="lofToc" >4.8&#x00A0;<a 
href="#x1-570078">HRG time
series analysis</a></span><br /><span class="lofToc" >4.9&#x00A0;<a 
href="#x1-570089">HRG histogram of returns</a></span><br /><span class="lofToc" >4.10&#x00A0;<a 
href="#x1-5700910">HL time series analysis</a></span><br /><span class="lofToc" >4.11&#x00A0;<a 
href="#x1-5701011">HL
histogram of returns</a></span><br /><span class="lofToc" >4.12&#x00A0;<a 
href="#x1-5800112">MSFT OLS in-sample prediction</a></span><br /><span class="lofToc" >4.13&#x00A0;<a 
href="#x1-5800213">100 day MSFT
OLS in-sample forecast</a></span><br /><span class="lofToc" >4.14&#x00A0;<a 
href="#x1-5800314">CDE OLS in-sample prediction</a></span><br /><span class="lofToc" >4.15&#x00A0;<a 
href="#x1-5800415">100 Day CDE
OLS out of sample forecast</a></span><br /><span class="lofToc" >4.16&#x00A0;<a 
href="#x1-5800516">NAVB OLS in-sample prediction</a></span><br /><span class="lofToc" >4.17&#x00A0;<a 
href="#x1-5800617">100 day
NAVB OLS in-sample forecast</a></span><br /><span class="lofToc" >4.18&#x00A0;<a 
href="#x1-5800718">HRG OLS in-sample prediction</a></span><br /><span class="lofToc" >4.19&#x00A0;<a 
href="#x1-5800819">100 day
HRG OLS in-sample forecast</a></span><br /><span class="lofToc" >4.20&#x00A0;<a 
href="#x1-5800920">HL OLS in-sample prediction</a></span><br /><span class="lofToc" >4.21&#x00A0;<a 
href="#x1-5801021">100 day
HL OLS in-sample forecast</a></span><br /><span class="lofToc" >4.22&#x00A0;<a 
href="#x1-5900122">MSFT AR time series analysis</a></span><br /><span class="lofToc" >4.23&#x00A0;<a 
href="#x1-5900223">MSFT AR
histogram of returns</a></span><br /><span class="lofToc" >4.24&#x00A0;<a 
href="#x1-5900324">MSFT AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.25&#x00A0;<a 
href="#x1-5900425">100 day
MSFT AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.26&#x00A0;<a 
href="#x1-5900526">CDE AR time series analysis</a></span><br /><span class="lofToc" >4.27&#x00A0;<a 
href="#x1-5900627">CDE
AR histogram of returns</a></span><br /><span class="lofToc" >4.28&#x00A0;<a 
href="#x1-5900728">CDE AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.29&#x00A0;<a 
href="#x1-5900829">100 day
CDE AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.30&#x00A0;<a 
href="#x1-5900930">NAVB AR time series analysis</a></span><br /><span class="lofToc" >4.31&#x00A0;<a 
href="#x1-5901031">NAVB
AR histogram of returns</a></span><br /><span class="lofToc" >4.32&#x00A0;<a 
href="#x1-5901132">NAVB AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.33&#x00A0;<a 
href="#x1-5901233">100 day
NAVB AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.34&#x00A0;<a 
href="#x1-5901334">HRG AR time series analysis</a></span><br /><span class="lofToc" >4.35&#x00A0;<a 
href="#x1-5901435">HRG
AR histogram of returns</a></span><br /><span class="lofToc" >4.36&#x00A0;<a 
href="#x1-5901536">HL AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.37&#x00A0;<a 
href="#x1-5901637">100 day
HRG AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.38&#x00A0;<a 
href="#x1-5901738">HL AR time series analysis</a></span><br /><span class="lofToc" >4.39&#x00A0;<a 
href="#x1-5901839">HL AR
histogram of returns</a></span><br /><span class="lofToc" >4.40&#x00A0;<a 
href="#x1-5901940">HL AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.41&#x00A0;<a 
href="#x1-5902041">100 day HL
AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.42&#x00A0;<a 
href="#x1-6000142">MSFT MA time series analysis</a></span><br /><span class="lofToc" >4.43&#x00A0;<a 
href="#x1-6000243">MSFT MA
histogram of returns</a></span><br /><span class="lofToc" >4.44&#x00A0;<a 
href="#x1-6000344">MSFT MA in-sample returns prediction</a></span><br /><span class="lofToc" >4.45&#x00A0;<a 
href="#x1-6000445">100 day
MSFT MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.46&#x00A0;<a 
href="#x1-6000546">CDE MA time series analysis</a></span><br /><span class="lofToc" >4.47&#x00A0;<a 
href="#x1-6000647">CDE
MA histogram of returns</a></span><br /><span class="lofToc" >4.48&#x00A0;<a 
href="#x1-6000748">CDE MA in-sample returns prediction</a></span><br /><span class="lofToc" >4.49&#x00A0;<a 
href="#x1-6000849">100
day CDE MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.50&#x00A0;<a 
href="#x1-6000950">NAVB MA time series
analysis</a></span><br /><span class="lofToc" >4.51&#x00A0;<a 
href="#x1-6001051">NAVB MA histogram of returns</a></span><br /><span class="lofToc" >4.52&#x00A0;<a 
href="#x1-6001152">NAVB MA in-sample returns
prediction</a></span><br /><span class="lofToc" >4.53&#x00A0;<a 
href="#x1-6001253">100 day NAVB MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.54&#x00A0;<a 
href="#x1-6001354">HRG MA
time series analysis</a></span><br /><span class="lofToc" >4.55&#x00A0;<a 
href="#x1-6001455">HRG MA histogram of returns</a></span><br /><span class="lofToc" >4.56&#x00A0;<a 
href="#x1-6001556">HRG MA in-sample
returns prediction</a></span><br /><span class="lofToc" >4.57&#x00A0;<a 
href="#x1-6001657">100 day HRG MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.58&#x00A0;<a 
href="#x1-6001758">HL MA
time series analysis</a></span><br /><span class="lofToc" >4.59&#x00A0;<a 
href="#x1-6001859">HL MA histogram of returns</a></span><br /><span class="lofToc" >4.60&#x00A0;<a 
href="#x1-6001960">HL MA in-sample
returns prediction</a></span><br /><span class="lofToc" >4.61&#x00A0;<a 
href="#x1-6002061">100 day HL MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.62&#x00A0;<a 
href="#x1-6100162">MSFT
ARMA time series analysis</a></span><br /><span class="lofToc" >4.63&#x00A0;<a 
href="#x1-6100263">MSFT ARMA histogram of returns</a></span><br /><span class="lofToc" >4.64&#x00A0;<a 
href="#x1-6100364">MSFT
ARMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.65&#x00A0;<a 
href="#x1-6100465">100 day MSFT ARMA in-sample
returns forecast</a></span><br /><span class="lofToc" >4.66&#x00A0;<a 
href="#x1-6100566">CDE ARMA time series analysis</a></span><br /><span class="lofToc" >4.67&#x00A0;<a 
href="#x1-6100667">CDE ARMA
histogram of returns</a></span><br /><span class="lofToc" >4.68&#x00A0;<a 
href="#x1-6100768">CDE ARMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.69&#x00A0;<a 
href="#x1-6100869">100
day CDE ARMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.70&#x00A0;<a 
href="#x1-6100970">NAVB ARMA time series
analysis</a></span><br /><span class="lofToc" >4.71&#x00A0;<a 
href="#x1-6101071">NAVB ARMA histogram of returns</a></span><br /><span class="lofToc" >4.72&#x00A0;<a 
href="#x1-6101172">NAVB ARMA in-sample
returns prediction</a></span><br /><span class="lofToc" >4.73&#x00A0;<a 
href="#x1-6101273">100 day NAVB ARMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.74&#x00A0;<a 
href="#x1-6101374">HRG
ARMA time series analysis</a></span><br /><span class="lofToc" >4.75&#x00A0;<a 
href="#x1-6101475">HRG ARMA histogram of returns</a></span><br /><span class="lofToc" >4.76&#x00A0;<a 
href="#x1-6101576">HRG
ARMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.77&#x00A0;<a 
href="#x1-6101677">100 day HRG ARMA in-sample
returns forecast</a></span><br /><span class="lofToc" >4.78&#x00A0;<a 
href="#x1-6101778">HL ARMA time series analysis</a></span><br /><span class="lofToc" >4.79&#x00A0;<a 
href="#x1-6101879">HL ARMA histogram
of returns</a></span><br /><span class="lofToc" >4.80&#x00A0;<a 
href="#x1-6101980">HL ARMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.81&#x00A0;<a 
href="#x1-6102081">100 day HL ARMA
in-sample returns forecast</a></span><br /><span class="lofToc" >4.82&#x00A0;<a 
href="#x1-6200182">MSFT ARIMA time series analysis</a></span><br /><span class="lofToc" >4.83&#x00A0;<a 
href="#x1-6200283">MSFT
                                                                                
                                                                                
ARIMA histogram of returns</a></span><br /><span class="lofToc" >4.84&#x00A0;<a 
href="#x1-6200384">MSFT ARIMA in-sample returns
prediction</a></span><br /><span class="lofToc" >4.85&#x00A0;<a 
href="#x1-6200485">100 day MSFT ARIMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.86&#x00A0;<a 
href="#x1-6200586">CDE
ARIMA time series analysis</a></span><br /><span class="lofToc" >4.87&#x00A0;<a 
href="#x1-6200687">CDE ARIMA histogram of returns</a></span><br /><span class="lofToc" >4.88&#x00A0;<a 
href="#x1-6200788">CDE
ARIMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.89&#x00A0;<a 
href="#x1-6200889">100 day CDE ARIMA in-sample
returns forecast</a></span><br /><span class="lofToc" >4.90&#x00A0;<a 
href="#x1-6200990">NAVB ARIMA time series analysis</a></span><br /><span class="lofToc" >4.91&#x00A0;<a 
href="#x1-6201091">NAVB ARIMA
histogram of returns</a></span><br /><span class="lofToc" >4.92&#x00A0;<a 
href="#x1-6201192">NAVB ARIMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.93&#x00A0;<a 
href="#x1-6201293">100
day NAVB ARIMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.94&#x00A0;<a 
href="#x1-6201394">HRG ARIMA time series
analysis</a></span><br /><span class="lofToc" >4.95&#x00A0;<a 
href="#x1-6201495">HRG ARIMA histogram of returns</a></span><br /><span class="lofToc" >4.96&#x00A0;<a 
href="#x1-6201596">HRG ARIMA in-sample
returns prediction</a></span><br /><span class="lofToc" >4.97&#x00A0;<a 
href="#x1-6201697">100 day HRG ARIMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.98&#x00A0;<a 
href="#x1-6201798">HL
ARIMA time series analysis</a></span><br /><span class="lofToc" >4.99&#x00A0;<a 
href="#x1-6201899">HL ARIMA histogram of returns</a></span><br /><span class="lofToc" >4.100&#x00A0;<a 
href="#x1-62019100">HL
ARIMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.101&#x00A0;<a 
href="#x1-62020101">100 day HL ARIMA in-sample
returns forecast</a></span><br /><span class="lofToc" >4.102&#x00A0;<a 
href="#x1-76001102">MSFT Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.103&#x00A0;<a 
href="#x1-76002103">100 day
MSFT Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.104&#x00A0;<a 
href="#x1-76003104">CDE Decision Trees in-sample
prediction</a></span><br /><span class="lofToc" >4.105&#x00A0;<a 
href="#x1-76004105">100 day CDE Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.106&#x00A0;<a 
href="#x1-76005106">NAVB
Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.107&#x00A0;<a 
href="#x1-76006107">100 day NAVB Decision Trees
out-of-sample forecast</a></span><br /><span class="lofToc" >4.108&#x00A0;<a 
href="#x1-76007108">HRG Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.109&#x00A0;<a 
href="#x1-76008109">100
day HRG Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.110&#x00A0;<a 
href="#x1-76009110">HL Decision Trees in-sample
prediction</a></span><br /><span class="lofToc" >4.111&#x00A0;<a 
href="#x1-76010111">100 day HL Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.112&#x00A0;<a 
href="#x1-77001112">MSFT
Boosted Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.113&#x00A0;<a 
href="#x1-77002113">100 day MSFT Boosted
Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.114&#x00A0;<a 
href="#x1-77003114">CDE Boosted Decision Trees
in-sample prediction</a></span><br /><span class="lofToc" >4.115&#x00A0;<a 
href="#x1-77004115">100 day CDE Boosted Decision Trees out-of-sample
forecast</a></span><br /><span class="lofToc" >4.116&#x00A0;<a 
href="#x1-77005116">NAVB Boosted Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.117&#x00A0;<a 
href="#x1-77006117">100 day
NAVB Boosted Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.118&#x00A0;<a 
href="#x1-77007118">HRG Boosted Decision
Trees in-sample prediction</a></span><br /><span class="lofToc" >4.119&#x00A0;<a 
href="#x1-77008119">100 day HRG Boosted Decision Trees out-of-sample
forecast</a></span><br /><span class="lofToc" >4.120&#x00A0;<a 
href="#x1-77009120">HL Boosted Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.121&#x00A0;<a 
href="#x1-77010121">100 day HL
Boosted Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.122&#x00A0;<a 
href="#x1-78001122">MSFT K-Nearest Neighbour
in-sample prediction</a></span><br /><span class="lofToc" >4.123&#x00A0;<a 
href="#x1-78002123">100 day MSFT K-Nearest Neighbour out-of-sample
forecast</a></span><br /><span class="lofToc" >4.124&#x00A0;<a 
href="#x1-78003124">CDE K-Nearest Neighbour in-sample prediction</a></span><br /><span class="lofToc" >4.125&#x00A0;<a 
href="#x1-78004125">100 day CDE
K-Nearest Neighbour out-of-sample forecast</a></span><br /><span class="lofToc" >4.126&#x00A0;<a 
href="#x1-78005126">NAVB K-Nearest Neighbour
in-sample prediction</a></span><br /><span class="lofToc" >4.127&#x00A0;<a 
href="#x1-78006127">100 day NAVB K-Nearest Neighbour out-of-sample
forecast</a></span><br /><span class="lofToc" >4.128&#x00A0;<a 
href="#x1-78007128">HRG K-Nearest Neighbour in-sample prediction</a></span><br /><span class="lofToc" >4.129&#x00A0;<a 
href="#x1-78008129">100 day
HRG K-Nearest Neighbour out-of-sample forecast</a></span><br /><span class="lofToc" >4.130&#x00A0;<a 
href="#x1-78009130">HL K-Nearest Neighbour
in-sample prediction</a></span><br /><span class="lofToc" >4.131&#x00A0;<a 
href="#x1-78010131">100 day HL K-Nearest Neighbour out-of-sample
forecast</a></span><br /><span class="lofToc" >4.132&#x00A0;<a 
href="#x1-79001132">MSFT Random Forest in-sample prediction</a></span><br /><span class="lofToc" >4.133&#x00A0;<a 
href="#x1-79002133">100 day MSFT
Random Forest out-of-sample forecast</a></span><br /><span class="lofToc" >4.134&#x00A0;<a 
href="#x1-79003134">CDE Random Forest in-sample
prediction</a></span><br /><span class="lofToc" >4.135&#x00A0;<a 
href="#x1-79004135">100 day CDE Random Forest out-of-sample forecast</a></span><br /><span class="lofToc" >4.136&#x00A0;<a 
href="#x1-79005136">NAVB
Random Forest in-sample prediction</a></span><br /><span class="lofToc" >4.137&#x00A0;<a 
href="#x1-79006137">100 day NAVB Random Forest
out-of-sample forecast</a></span><br /><span class="lofToc" >4.138&#x00A0;<a 
href="#x1-79007138">HRG Random Forest in-sample prediction</a></span><br /><span class="lofToc" >4.139&#x00A0;<a 
href="#x1-79008139">100
day HRG Random Forest out-of-sample forecast</a></span><br /><span class="lofToc" >4.140&#x00A0;<a 
href="#x1-79009140">HL Random Forest in-sample
prediction</a></span><br /><span class="lofToc" >4.141&#x00A0;<a 
href="#x1-79010141">100 day HL Random Forest out-of-sample forecast</a></span><br /><span class="lofToc" >4.142&#x00A0;<a 
href="#x1-80001142">MSFT
Linear Regression in-sample prediction</a></span><br /><span class="lofToc" >4.143&#x00A0;<a 
href="#x1-80002143">100 day MSFT Linear Regression
out-of-sample forecast</a></span><br /><span class="lofToc" >4.144&#x00A0;<a 
href="#x1-80003144">CDE Linear Regression in-sample prediction</a></span><br /><span class="lofToc" >4.145&#x00A0;<a 
href="#x1-80004145">100
day CDE Linear Regression out-of-sample forecast</a></span><br /><span class="lofToc" >4.146&#x00A0;<a 
href="#x1-80005146">CDE Linear Regression
in-sample prediction</a></span><br /><span class="lofToc" >4.147&#x00A0;<a 
href="#x1-80006147">100 day CDE Linear Regression out-of-sample
forecast</a></span><br /><span class="lofToc" >4.148&#x00A0;<a 
href="#x1-80007148">HRG Linear Regression in-sample prediction</a></span><br /><span class="lofToc" >4.149&#x00A0;<a 
href="#x1-80008149">100 day HRG
Linear Regression out-of-sample forecast</a></span><br /><span class="lofToc" >4.150&#x00A0;<a 
href="#x1-80009150">HL Linear Regression in-sample
prediction</a></span><br /><span class="lofToc" >4.151&#x00A0;<a 
href="#x1-80010151">100 day HL Linear Regression out-of-sample forecast</a></span><br /><span class="lofToc" >4.152&#x00A0;<a 
href="#x1-81001152">MSFT
Neural Network in-sample prediction</a></span><br /><span class="lofToc" >4.153&#x00A0;<a 
href="#x1-81002153">100 day MSFT Neural Network
out-of-sample forecast</a></span><br /><span class="lofToc" >4.154&#x00A0;<a 
href="#x1-81003154">CDE Neural Network in-sample prediction</a></span><br /><span class="lofToc" >4.155&#x00A0;<a 
href="#x1-81004155">100
day CDE Neural Network out-of-sample forecast</a></span><br /><span class="lofToc" >4.156&#x00A0;<a 
href="#x1-81005156">NAVB Neural Network
                                                                                
                                                                                
in-sample prediction</a></span><br /><span class="lofToc" >4.157&#x00A0;<a 
href="#x1-81006157">100 day NAVB Neural Network out-of-sample
forecast</a></span><br /><span class="lofToc" >4.158&#x00A0;<a 
href="#x1-81007158">HRG Neural Network in-sample prediction</a></span><br /><span class="lofToc" >4.159&#x00A0;<a 
href="#x1-81008159">100 day HRG
Neural Network out-of-sample forecast</a></span><br /><span class="lofToc" >4.160&#x00A0;<a 
href="#x1-81009160">HL Neural Network in-sample
prediction</a></span><br /><span class="lofToc" >4.161&#x00A0;<a 
href="#x1-81010161">100 day HL Neural Network out-of-sample forecast</a></span><br /><span class="lofToc" >4.162&#x00A0;<a 
href="#x1-82001162">MSFT
SGD in-sample prediction</a></span><br /><span class="lofToc" >4.163&#x00A0;<a 
href="#x1-82002163">100 day MSFT SGD out-of-sample
forecast</a></span><br /><span class="lofToc" >4.164&#x00A0;<a 
href="#x1-82003164">CDE SGD in-sample prediction</a></span><br /><span class="lofToc" >4.165&#x00A0;<a 
href="#x1-82004165">100 day CDE SGD
out-of-sample forecast</a></span><br /><span class="lofToc" >4.166&#x00A0;<a 
href="#x1-82005166">NAVB SGD in-sample prediction</a></span><br /><span class="lofToc" >4.167&#x00A0;<a 
href="#x1-82006167">100 day NAVB
SGD out-of-sample forecast</a></span><br /><span class="lofToc" >4.168&#x00A0;<a 
href="#x1-82007168">HRG SGD in-sample prediction</a></span><br /><span class="lofToc" >4.169&#x00A0;<a 
href="#x1-82008169">100 day
HRG SGD out-of-sample forecast</a></span><br /><span class="lofToc" >4.170&#x00A0;<a 
href="#x1-82009170">HL SGD in-sample prediction</a></span><br /><span class="lofToc" >4.171&#x00A0;<a 
href="#x1-82010171">100
day HL SGD out-of-sample forecast</a></span><br /><span class="lofToc" >4.172&#x00A0;<a 
href="#x1-84001172">MSFT Metropolis-Hastimgs
in-sample prediction</a></span><br /><span class="lofToc" >4.173&#x00A0;<a 
href="#x1-84002173">100 day MSFT Metropolis-Hastings out-of-sample
forecast</a></span><br /><span class="lofToc" >4.174&#x00A0;<a 
href="#x1-84003174">CDE Metropolis-Hastimgs in-sample prediction</a></span><br /><span class="lofToc" >4.175&#x00A0;<a 
href="#x1-84004175">100 day CDE
Metropolis-Hastings out-of-sample forecast</a></span><br /><span class="lofToc" >4.176&#x00A0;<a 
href="#x1-84005176">NAVB Metropolis-Hastimgs
in-sample prediction</a></span><br /><span class="lofToc" >4.177&#x00A0;<a 
href="#x1-84006177">100 day NAVB Metropolis-Hastings out-of-sample
forecast</a></span><br /><span class="lofToc" >4.178&#x00A0;<a 
href="#x1-84007178">HRG Metropolis-Hastimgs in-sample prediction</a></span><br /><span class="lofToc" >4.179&#x00A0;<a 
href="#x1-84008179">100 day
HRG Metropolis-Hastings out-of-sample forecast</a></span><br /><span class="lofToc" >4.180&#x00A0;<a 
href="#x1-84009180">HL Metropolis-Hastimgs
in-sample prediction</a></span><br /><span class="lofToc" >4.181&#x00A0;<a 
href="#x1-84010181">100 day HL Metropolis-Hastings out-of-sample
forecast</a></span><br /><span class="lofToc" >4.182&#x00A0;<a 
href="#x1-85001182">MSFT NUTS in-sample prediction</a></span><br /><span class="lofToc" >4.183&#x00A0;<a 
href="#x1-85002183">100 day MSFT NUTS
out-of-sample forecast</a></span><br /><span class="lofToc" >4.184&#x00A0;<a 
href="#x1-85003184">CDE NUTS in-sample prediction</a></span><br /><span class="lofToc" >4.185&#x00A0;<a 
href="#x1-85004185">100 day CDE
NUTS out-of-sample forecast</a></span><br /><span class="lofToc" >4.186&#x00A0;<a 
href="#x1-85005186">NAVB NUTS in-sample prediction</a></span><br /><span class="lofToc" >4.187&#x00A0;<a 
href="#x1-85006187">100
day NAVB NUTS out-of-sample forecast</a></span><br /><span class="lofToc" >4.188&#x00A0;<a 
href="#x1-85007188">HRG NUTS in-sample
prediction</a></span><br /><span class="lofToc" >4.189&#x00A0;<a 
href="#x1-85008189">100 day HRG NUTS out-of-sample forecast</a></span><br /><span class="lofToc" >4.190&#x00A0;<a 
href="#x1-85009190">HL NUTS
in-sample prediction</a></span><br /><span class="lofToc" >4.191&#x00A0;<a 
href="#x1-85010191">100 day HL NUTS out-of-sample forecast</a></span><br /><span class="lofToc" >4.192&#x00A0;<a 
href="#x1-87002192">Machine
Learning Classifier strategy with only upwards forecasts</a></span><br /><span class="lofToc" >4.193&#x00A0;<a 
href="#x1-87004193">Machine Learning
Classifier strategy with upwards and downwards forecasts</a></span><br /><span class="lofToc" >4.194&#x00A0;<a 
href="#x1-87006194">Machine Learning
Classifier strategy with stop loss</a></span><br /><span class="lofToc" >4.195&#x00A0;<a 
href="#x1-88002195">Machine Learning Regression strategy with
only upwards forecasts</a></span><br /><span class="lofToc" >4.196&#x00A0;<a 
href="#x1-88004196">Machine Learning Regression strategy with upwards
and downwards forecasts</a></span><br /><span class="lofToc" >4.197&#x00A0;<a 
href="#x1-88006197">Machine Learning Regression strategy with stop loss</a></span><br />
</div>
                                                                                
                                                                                
<a 
 id="x1-2001r5"></a>
<a 
 id="Q1-1-7"></a>
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-30005"></a>List of Tables</h2> <div class="tableofcontents"><span class="lotToc" >4.1&#x00A0;<a 
href="#x1-550021">Equities Descriptive Statistics</a></span><br /><span class="lotToc" >4.2&#x00A0;<a 
href="#x1-650012">Decision Tree
results</a></span><br /><span class="lotToc" >4.3&#x00A0;<a 
href="#x1-660013">Boosted Decision Tree results</a></span><br /><span class="lotToc" >4.4&#x00A0;<a 
href="#x1-670014">Support Vector Machine
results</a></span><br /><span class="lotToc" >4.5&#x00A0;<a 
href="#x1-680015">Random Forest results</a></span><br /><span class="lotToc" >4.6&#x00A0;<a 
href="#x1-690016">K-Nearest Neighbour results</a></span><br /><span class="lotToc" >4.7&#x00A0;<a 
href="#x1-700017">Logistic
Regression results</a></span><br /><span class="lotToc" >4.8&#x00A0;<a 
href="#x1-710018">Gaussian Naive Bayes results</a></span><br /><span class="lotToc" >4.9&#x00A0;<a 
href="#x1-720019">Bernoulli Naive
Bayes results</a></span><br /><span class="lotToc" >4.10&#x00A0;<a 
href="#x1-7300110">Neural Network results</a></span><br /><span class="lotToc" >4.11&#x00A0;<a 
href="#x1-7400111">Stochastic Gradient Descent
results</a></span><br /><span class="lotToc" >4.12&#x00A0;<a 
href="#x1-8700112">Machine Learning Classifier strategy with only upwards
forecasts</a></span><br /><span class="lotToc" >4.13&#x00A0;<a 
href="#x1-8700313">Machine Learning Classifier strategy with upwarda and
downwards forecasts</a></span><br /><span class="lotToc" >4.14&#x00A0;<a 
href="#x1-8700514">Machine Learning Classifier strategy with
stop loss</a></span><br /><span class="lotToc" >4.15&#x00A0;<a 
href="#x1-8800115">Machine Learning Regression strategy with only upwards
forecasts</a></span><br /><span class="lotToc" >4.16&#x00A0;<a 
href="#x1-8800316">Machine Learning Regression strategy with upwards and
downwards forecasts</a></span><br /><span class="lotToc" >4.17&#x00A0;<a 
href="#x1-8800517">Machine Learning Regression strategy with stop loss</a></span><br />
</div>
                                                                                
                                                                                
                                                                                
                                                                                
<a 
 id="x1-3001r6"></a>
<a 
 id="Q1-1-9"></a>
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-40006"></a>Abbreviations</h2>
<a 
 id="x1-550011"></a><a 
 id="x1-550021"></a><a 
 id="x1-570012"></a><a 
 id="x1-570023"></a><a 
 id="x1-570034"></a><a 
 id="x1-570045"></a><a 
 id="x1-570056"></a><a 
 id="x1-570067"></a><a 
 id="x1-570078"></a><a 
 id="x1-570089"></a><a 
 id="x1-5700910"></a><a 
 id="x1-5701011"></a><a 
 id="x1-5800112"></a><a 
 id="x1-5800213"></a><a 
 id="x1-5800314"></a><a 
 id="x1-5800415"></a><a 
 id="x1-5800516"></a><a 
 id="x1-5800617"></a><a 
 id="x1-5800718"></a><a 
 id="x1-5800819"></a><a 
 id="x1-5800920"></a><a 
 id="x1-5801021"></a><a 
 id="x1-5900122"></a><a 
 id="x1-5900223"></a><a 
 id="x1-5900324"></a><a 
 id="x1-5900425"></a><a 
 id="x1-5900526"></a><a 
 id="x1-5900627"></a><a 
 id="x1-5900728"></a><a 
 id="x1-5900829"></a><a 
 id="x1-5900930"></a><a 
 id="x1-5901031"></a><a 
 id="x1-5901132"></a><a 
 id="x1-5901233"></a><a 
 id="x1-5901334"></a><a 
 id="x1-5901435"></a><a 
 id="x1-5901536"></a><a 
 id="x1-5901637"></a><a 
 id="x1-5901738"></a><a 
 id="x1-5901839"></a><a 
 id="x1-5901940"></a><a 
 id="x1-5902041"></a><a 
 id="x1-6000142"></a><a 
 id="x1-6000243"></a><a 
 id="x1-6000344"></a><a 
 id="x1-6000445"></a><a 
 id="x1-6000546"></a><a 
 id="x1-6000647"></a><a 
 id="x1-6000748"></a><a 
 id="x1-6000849"></a><a 
 id="x1-6000950"></a><a 
 id="x1-6001051"></a><a 
 id="x1-6001152"></a><a 
 id="x1-6001253"></a><a 
 id="x1-6001354"></a><a 
 id="x1-6001455"></a><a 
 id="x1-6001556"></a><a 
 id="x1-6001657"></a><a 
 id="x1-6001758"></a><a 
 id="x1-6001859"></a><a 
 id="x1-6001960"></a><a 
 id="x1-6002061"></a><a 
 id="x1-6100162"></a><a 
 id="x1-6100263"></a><a 
 id="x1-6100364"></a><a 
 id="x1-6100465"></a><a 
 id="x1-6100566"></a><a 
 id="x1-6100667"></a><a 
 id="x1-6100768"></a><a 
 id="x1-6100869"></a><a 
 id="x1-6100970"></a><a 
 id="x1-6101071"></a><a 
 id="x1-6101172"></a><a 
 id="x1-6101273"></a><a 
 id="x1-6101374"></a><a 
 id="x1-6101475"></a><a 
 id="x1-6101576"></a><a 
 id="x1-6101677"></a><a 
 id="x1-6101778"></a><a 
 id="x1-6101879"></a><a 
 id="x1-6101980"></a><a 
 id="x1-6102081"></a><a 
 id="x1-6200182"></a><a 
 id="x1-6200283"></a><a 
 id="x1-6200384"></a><a 
 id="x1-6200485"></a><a 
 id="x1-6200586"></a><a 
 id="x1-6200687"></a><a 
 id="x1-6200788"></a><a 
 id="x1-6200889"></a><a 
 id="x1-6200990"></a><a 
 id="x1-6201091"></a><a 
 id="x1-6201192"></a><a 
 id="x1-6201293"></a><a 
 id="x1-6201394"></a><a 
 id="x1-6201495"></a><a 
 id="x1-6201596"></a><a 
 id="x1-6201697"></a><a 
 id="x1-6201798"></a><a 
 id="x1-6201899"></a><a 
 id="x1-62019100"></a><a 
 id="x1-62020101"></a><a 
 id="x1-650012"></a><a 
 id="x1-660013"></a><a 
 id="x1-670014"></a><a 
 id="x1-680015"></a><a 
 id="x1-690016"></a><a 
 id="x1-700017"></a><a 
 id="x1-710018"></a><a 
 id="x1-720019"></a><a 
 id="x1-7300110"></a><a 
 id="x1-7400111"></a><a 
 id="x1-76001102"></a><a 
 id="x1-76002103"></a><a 
 id="x1-76003104"></a><a 
 id="x1-76004105"></a><a 
 id="x1-76005106"></a><a 
 id="x1-76006107"></a><a 
 id="x1-76007108"></a><a 
 id="x1-76008109"></a><a 
 id="x1-76009110"></a><a 
 id="x1-76010111"></a><a 
 id="x1-77001112"></a><a 
 id="x1-77002113"></a><a 
 id="x1-77003114"></a><a 
 id="x1-77004115"></a><a 
 id="x1-77005116"></a><a 
 id="x1-77006117"></a><a 
 id="x1-77007118"></a><a 
 id="x1-77008119"></a><a 
 id="x1-77009120"></a><a 
 id="x1-77010121"></a><a 
 id="x1-78001122"></a><a 
 id="x1-78002123"></a><a 
 id="x1-78003124"></a><a 
 id="x1-78004125"></a><a 
 id="x1-78005126"></a><a 
 id="x1-78006127"></a><a 
 id="x1-78007128"></a><a 
 id="x1-78008129"></a><a 
 id="x1-78009130"></a><a 
 id="x1-78010131"></a><a 
 id="x1-79001132"></a><a 
 id="x1-79002133"></a><a 
 id="x1-79003134"></a><a 
 id="x1-79004135"></a><a 
 id="x1-79005136"></a><a 
 id="x1-79006137"></a><a 
 id="x1-79007138"></a><a 
 id="x1-79008139"></a><a 
 id="x1-79009140"></a><a 
 id="x1-79010141"></a><a 
 id="x1-80001142"></a><a 
 id="x1-80002143"></a><a 
 id="x1-80003144"></a><a 
 id="x1-80004145"></a><a 
 id="x1-80005146"></a><a 
 id="x1-80006147"></a><a 
 id="x1-80007148"></a><a 
 id="x1-80008149"></a><a 
 id="x1-80009150"></a><a 
 id="x1-80010151"></a><a 
 id="x1-81001152"></a><a 
 id="x1-81002153"></a><a 
 id="x1-81003154"></a><a 
 id="x1-81004155"></a><a 
 id="x1-81005156"></a><a 
 id="x1-81006157"></a><a 
 id="x1-81007158"></a><a 
 id="x1-81008159"></a><a 
 id="x1-81009160"></a><a 
 id="x1-81010161"></a><a 
 id="x1-82001162"></a><a 
 id="x1-82002163"></a><a 
 id="x1-82003164"></a><a 
 id="x1-82004165"></a><a 
 id="x1-82005166"></a><a 
 id="x1-82006167"></a><a 
 id="x1-82007168"></a><a 
 id="x1-82008169"></a><a 
 id="x1-82009170"></a><a 
 id="x1-82010171"></a><a 
 id="x1-84001172"></a><a 
 id="x1-84002173"></a><a 
 id="x1-84003174"></a><a 
 id="x1-84004175"></a><a 
 id="x1-84005176"></a><a 
 id="x1-84006177"></a><a 
 id="x1-84007178"></a><a 
 id="x1-84008179"></a><a 
 id="x1-84009180"></a><a 
 id="x1-84010181"></a><a 
 id="x1-85001182"></a><a 
 id="x1-85002183"></a><a 
 id="x1-85003184"></a><a 
 id="x1-85004185"></a><a 
 id="x1-85005186"></a><a 
 id="x1-85006187"></a><a 
 id="x1-85007188"></a><a 
 id="x1-85008189"></a><a 
 id="x1-85009190"></a><a 
 id="x1-85010191"></a><a 
 id="x1-8700112"></a><a 
 id="x1-87002192"></a><a 
 id="x1-8700313"></a><a 
 id="x1-87004193"></a><a 
 id="x1-8700514"></a><a 
 id="x1-87006194"></a><a 
 id="x1-8800115"></a><a 
 id="x1-88002195"></a><a 
 id="x1-8800316"></a><a 
 id="x1-88004196"></a><a 
 id="x1-8800517"></a><a 
 id="x1-88006197"></a>
<a 
 id="x1-4001r1"></a><!--l. 163--><div class="longtable"><table id="TBL-1" class="longtable" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-1-1g"><col 
id="TBL-1-1"><col 
id="TBL-1-2"></colgroup>
<tr  
 style="vertical-align:baseline;" id="TBL-1-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-1-1"  
class="td11"><span 
class="cmbx-10x-x-109">EMH </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-1-2"  
class="td11"><span 
class="cmbx-10x-x-109">E</span>fficient <span 
class="cmbx-10x-x-109">M</span>arket <span 
class="cmbx-10x-x-109">H</span>ypothesis                                                            </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-2-1"  
class="td11"><span 
class="cmbx-10x-x-109">RWH </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-2-2"  
class="td11"><span 
class="cmbx-10x-x-109">R</span>andom <span 
class="cmbx-10x-x-109">W</span>alk <span 
class="cmbx-10x-x-109">H</span>ypothesis                                                              </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-3-1"  
class="td11"><span 
class="cmbx-10x-x-109">OLS </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-3-2"  
class="td11"><span 
class="cmbx-10x-x-109">O</span>rdinary <span 
class="cmbx-10x-x-109">L</span>east <span 
class="cmbx-10x-x-109">S</span>quares                                                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-4-1"  
class="td11"><span 
class="cmbx-10x-x-109">AR </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-4-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>uto <span 
class="cmbx-10x-x-109">R</span>egressive                                                                            </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-5-1"  
class="td11"><span 
class="cmbx-10x-x-109">MA </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-5-2"  
class="td11"><span 
class="cmbx-10x-x-109">M</span>oving <span 
class="cmbx-10x-x-109">A</span>verage                                                                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-6-1"  
class="td11"><span 
class="cmbx-10x-x-109">ARMA </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-6-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>uto <span 
class="cmbx-10x-x-109">R</span>egressive <span 
class="cmbx-10x-x-109">M</span>oving <span 
class="cmbx-10x-x-109">A</span>verage                                                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-7-1"  
class="td11"><span 
class="cmbx-10x-x-109">ARIMA </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-7-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>uto <span 
class="cmbx-10x-x-109">R</span>egressive <span 
class="cmbx-10x-x-109">I</span>ntegrated <span 
class="cmbx-10x-x-109">M</span>oving <span 
class="cmbx-10x-x-109">A</span>verage                                      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-8-1"  
class="td11"><span 
class="cmbx-10x-x-109">GARCH </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-8-2"  
class="td11"><span 
class="cmbx-10x-x-109">G</span>eneralised <span 
class="cmbx-10x-x-109">A</span>uto <span 
class="cmbx-10x-x-109">R</span>egressive <span 
class="cmbx-10x-x-109">C</span>onditional <span 
class="cmbx-10x-x-109">H</span>eteroskedasticity                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-9-1"  
class="td11"><span 
class="cmbx-10x-x-109">EGARCH</span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-9-2"  
class="td11"><span 
class="cmbx-10x-x-109">E</span>xponential <span 
class="cmbx-10x-x-109">G</span>eneralised <span 
class="cmbx-10x-x-109">A</span>uto <span 
class="cmbx-10x-x-109">R</span>egressive <span 
class="cmbx-10x-x-109">C</span>onditional <span 
class="cmbx-10x-x-109">H</span>eteroskedasticity</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-10-1"  
class="td11"><span 
class="cmbx-10x-x-109">ADF </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-10-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>ugmented <span 
class="cmbx-10x-x-109">D</span>ickey <span 
class="cmbx-10x-x-109">F</span>uller                                                               </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-11-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-11-1"  
class="td11"><span 
class="cmbx-10x-x-109">SVM </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-11-2"  
class="td11"><span 
class="cmbx-10x-x-109">S</span>upport <span 
class="cmbx-10x-x-109">V</span>ector <span 
class="cmbx-10x-x-109">M</span>achine                                                                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-12-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-12-1"  
class="td11"><span 
class="cmbx-10x-x-109">SGD </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-12-2"  
class="td11"><span 
class="cmbx-10x-x-109">S</span>tochastic <span 
class="cmbx-10x-x-109">G</span>radient <span 
class="cmbx-10x-x-109">D</span>escent                                                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-13-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-13-1"  
class="td11"><span 
class="cmbx-10x-x-109">SMA </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-13-2"  
class="td11"><span 
class="cmbx-10x-x-109">S</span>imple <span 
class="cmbx-10x-x-109">M</span>oving <span 
class="cmbx-10x-x-109">A</span>verage                                                                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-14-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-14-1"  
class="td11"><span 
class="cmbx-10x-x-109">AIC </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-14-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>lkaline <span 
class="cmbx-10x-x-109">I</span>nformation <span 
class="cmbx-10x-x-109">C</span>riterion                                                         </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-15-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-15-1"  
class="td11"><span 
class="cmbx-10x-x-109">IC </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-15-2"  
class="td11"><span 
class="cmbx-10x-x-109">I</span>nformation <span 
class="cmbx-10x-x-109">C</span>riterion                                                                     </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-16-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-16-1"  
class="td11"><span 
class="cmbx-10x-x-109">NUTS </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-16-2"  
class="td11"><span 
class="cmbx-10x-x-109">N</span>o-<span 
class="cmbx-10x-x-109">U</span>-<span 
class="cmbx-10x-x-109">T</span>urn <span 
class="cmbx-10x-x-109">S</span>ampler                                                                      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-17-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-17-1"  
class="td11"><span 
class="cmbx-10x-x-109">QQ </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-17-2"  
class="td11"><span 
class="cmbx-10x-x-109">Q</span>uantile-<span 
class="cmbx-10x-x-109">Q</span>uantile                                                                          </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-18-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-18-1"  
class="td11"><span 
class="cmbx-10x-x-109">ETF </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-18-2"  
class="td11"><span 
class="cmbx-10x-x-109">E</span>xchange <span 
class="cmbx-10x-x-109">T</span>raded <span 
class="cmbx-10x-x-109">F</span>und                                                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-19-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-19-1"  
class="td11"><span 
class="cmbx-10x-x-109">ANN </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-19-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>rtificial <span 
class="cmbx-10x-x-109">N</span>eural <span 
class="cmbx-10x-x-109">N</span>etwork                                                               </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-20-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-20-1"  
class="td11"> </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-21-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-21-1"  
class="td11"> </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-22-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-22-1"  
class="td11"> </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-23-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-23-1"  
class="td11"></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-23-2"  
class="td11">
</td></tr>
</table></div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 173--><p class="noindent" >
<!--l. 173--><p class="noindent" ><span 
class="cmsl-12x-x-120">To my parents, Keith &amp; Christine Gauci Maistre. Without them,</span>
<span 
class="cmsl-12x-x-120">and their unconditional love and support, none of this would have</span>
<span 
class="cmsl-12x-x-120">been possible.</span></div>
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;1</span><br /><a 
 id="x1-50001"></a>Introduction</h2>
<!--l. 3--><p class="noindent" >The stock market retains its status as a prime location for investors to invest in the market
and earn a profit, however this is not always easy due to the constantly thriving and changing
nature which follows the stock market. Investors are constantly presented with numerous
profit potential opportunities, however without intensive planning and analysis, these
opportunities could easily turn into losses. This means that it is crucial for every
investor to carry out stock market ananlysis prior to any investment by monitoring
past price movements in order to forecast future trends. Even though past data is
not a clear indication of future movement, it is still proven to provide some useful
insite.
<h3 class="sectionHead"><span class="titlemark">1.1 </span> <a 
 id="x1-60001.1"></a>Project goals and scope</h3>
<!--l. 7--><p class="noindent" >This report aims to disprove two hypotheses, the Efficient Market Hypotheses (EMH), and the
Random Walk Hypothesis (RWH).
<!--l. 9--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">1.1.1 </span> <a 
 id="x1-70001.1.1"></a>Efficient Market Hypothesis</h4>
<!--l. 11--><p class="noindent" >By definition of the EMH, it is impossible for any investor to beat the market as the markets
are efficient and all stock prices always incorporate and reflect all relevant information. This
essentially means that stocks are always traded at their fair value, and their prices only
fluctuate when new information is released to the public. This essentially makes it impossible
for any investors to exploit any inefficiencies in the market and earn a profit. Believers of the
EMH claim that it is pointless to try and search for undervalued stocks, or predict market
trends.
<!--l. 13--><p class="noindent" >Although a vast number of academics find the EMH to be true, there are still a
number of investors who have consistently beaten the market over long periods of
time, something which is impossible by definition according to the EMH. Stock
market crashes, such as that of the financial crisis of 1987, in which the market fell
28.3% in a single day, proving that stock prices can seriously deviate from their fair
values.
<!--l. 15--><p class="noindent" >It is the firm belief of proponents of the EMH that investors are better off investing in low risk
index funds. It is a known fact that only a few number of active managers successfully manage
to outperform passive funds.
                                                                                
                                                                                
<!--l. 17--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">1.1.2 </span> <a 
 id="x1-80001.1.2"></a>Random Walk Hypothesis</h4>
<!--l. 19--><p class="noindent" >According to the RWH, it is not possible to use the past movement or trend of a stock price to
predict its future movement. The main idea of the RWH is all stocks take a random and
unpredictable path. Strong believers of the RWH believe that it is not possible to beat the
market without taking on more risk.
<!--l. 21--><p class="noindent" >There are however a vast number of critics of this theory, who state that stocks do indeed
follow trends over time, making it possible to carry out stock market analysis and carefully
select entry and exit points for stock investments.
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;2</span><br /><a 
 id="x1-90002"></a>Background Theory</h2>
<!--l. 3--><p class="noindent" >Computational finance is a branch of applied computer science that deals with problems of
practical interest in finance. Some slightly different definitions are the study of data and
algorithms currently used in finance and the mathematics of computer programs that realize
financial models or systems. Using computational finance in order to allocate assets in
a portfolio is not at all unheard of and was first documented 1952.[<a 
href="#XMarkowitz:1952aa">1</a>] Markowitz
first introduced the concept of portfolio selection as an exercise in mean-variance
optimisation. This required more computer power than was available at the time, so he
worked on useful algorithms for approximate solutions. He theorised that risk-averse
investors could construct portfolios to optimise or maximise expected return based on
a given level of market risk, emphasising that risk is an inherent part of higher
reward. According to his theory, it&#8217;s possible to construct an &#8221;efficient frontier&#8221; of
optimal portfolios offering the maximum possible expected return for a given level of
risk.
<h3 class="sectionHead"><span class="titlemark">2.1 </span> <a 
 id="x1-100002.1"></a>Time Series Analysis</h3>
<!--l. 7--><p class="noindent" >A time series is a series of data points which may be indexed, listed, or graphed, in a time
order. Most commonly, a time series is a sequence taken at successive equally spaced points in
time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean
tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.
Brockwell et al. provide a formal description of time series as having a set of observations xt,
each one being recorded at a specific time t. A discrete-time time series is one in which the set
T0 of times at which observations are made is a discrete set, as is the case, for example, when
observations are made at fixed time intervals. Continuous time series are obtained when
observations are recorded continuously over some time interval, e.g., when T0 = [0,
1].[<a 
href="#XPeter-J.-Brockwell:2016aa">2</a> ]
<!--l. 9--><p class="noindent" >Noteworthy &#8221;time series momentum&#8221; has likewise been achieved in equity index,
currency, commodity, and bond futures for each of the 58 liquid instruments which were
considered.[<a 
href="#XMoskowitz:2011aa">3</a> ] Moskowitz et al. discover persistence in returns ranging from 1 to 12 months
that partially reverses over longer horizons, consistent with sentiment theories of
initial under-reaction and delayed over-reaction. A diversified portfolio of time series
momentum strategies across all asset classes was found to deliver substantial abnormal
returns with little exposure to standard asset pricing factors and performs best during
extreme markets. After examining the trading activities of speculators and hedgers,
speculators were found to profit mostly from time series momentum at the expense of
hedgers.
                                                                                
                                                                                
<!--l. 11--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.1.1 </span> <a 
 id="x1-110002.1.1"></a>Auto Regressive Moving Average (ARMA)</h4>
<!--l. 13--><p class="noindent" >In the statistical analysis of time series, autoregressivemoving-average (ARMA) models
provide a parsimonious description of a weakly stationary stochastic process in terms of two
polynomials, one for the autoregression and the second for the moving average. The notation
ARMA(p, q) refers to the model with p autoregressive terms and q moving-average terms.
This model contains the AR(p) and MA(q) models,
<!--l. 15--><p class="noindent" ><img 
src="Thesis0x.png" alt="&#x2211;q
xt=c+&#x03B5;t+i=1&#x03C6;iXt- i&#x03B8;i&#x03B5;t- i  "  class="math" >
<!--l. 17--><p class="noindent" >Forecasting interest rates is of great concern for financial researchers, economists, and players
in the fixed income markets. A study was carried out to develop an appropriate model for
forecasting the short-term interest rates, implicit yield on 91 day treasury bill, overnight
MIBOR rate, and call money rate.[<a 
href="#XRadha:2015aa">4</a>] The short-term interest rates are forecasted
using univariate models such as the Random Walk, ARIMA, ARMA-GARCH, and
ARMA-EGARCH. The appropriate model for forecasting is determined considering a six-year
period from 1999. The results show that interest rates time series have volatility clustering
effect and hence GARCH based models are more appropriate to forecast than the other
models. Radha et al. found that for commercial paper rate ARIMA-EGARCH model is the
most appropriate model, while for implicit yield 91 day Treasury bill, overnight MIBOR rate,
and call money rate, the ARIMA-GARCH model is the most appropriate model for
forecasting.
<!--l. 19--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.1.2 </span> <a 
 id="x1-120002.1.2"></a>Auto Regressive Integrated Moving Average (ARIMA)</h4>
<!--l. 21--><p class="noindent" >In time series analysis, an autoregressive integrated moving average (ARIMA) model
is a generalization of an ARMA model. Given a time series of data <span 
class="cmmi-10x-x-109">X</span><sub><span 
class="cmmi-8">t</span></sub> where t
is an integer index and the <span 
class="cmmi-10x-x-109">X</span><sub><span 
class="cmmi-8">t</span></sub> are real numbers, an ARMA(p, q) model is given
by
<!--l. 23--><p class="noindent" ><img 
src="Thesis1x.png" alt="x-&#x03B1;X-...&#x03B1; &#x2032;X   &#x2032; = &#x03B5; + &#x03B8; &#x03F5;   + &#x22C5;&#x22C5;&#x22C5;+ &#x03B8; &#x03F5;
t1t-1   p  t-p    t    1t-1         qt-q  "  class="math" >
<!--l. 25--><p class="noindent" >The aforementioned model is fitted to time series data either to better understand the data or
to predict future points in the series (forecasting). ARIMA models are applied in some cases
                                                                                
                                                                                
where data show evidence of non-stationarity, where an initial differencing step
(corresponding to the &#8221;integrated&#8221; part of the model) can be applied to reduce the
non-stationarity.
<!--l. 27--><p class="noindent" >The existence of weak-form efficiency in the Russian stock market is examined for
the period 1st September 1995 to 1st May 2001 using daily, weekly and monthly
Russian Trading System index time series.[<a 
href="#XAbrosimova:2002aa">5</a>] Several different approaches are used to
assess the predictability of the RTS index time series. Unit root, autocorrelation and
variance ratio tests are conducted for the null hypothesis of a random walk model. The
results support the null hypothesis for the monthly data only. Further analysis is
performed for the daily and weekly data. Linear and non-linear modelling of the serial
dependence is conducted using ARIMA and GARCH models estimated on the in-sample
period 1st September 1995 to 1st January 2001. Forecasts based on the best fitting
models are performed for the out-of-sample period 2nd January 2001 to 1st May
2001. Comparisons of the forecasts reveal that none of the models outperforms the
others, and the most accurate forecasts are obtained for just the first out-of-sample
observation. Whilst our research results provide some limited evidence of short-term
market predictability on the RTS, there is insufficient evidence to suggest that it
would lead to a profitable trading rule, once transaction costs and risk are taken into
account.
<!--l. 29--><p class="noindent" >Darrat et al. set out to investigate with the use of new daily data, whether prices in the two
Chinese stock exchanges (Shanghai and Shenzhen) follow a random walk process as required
by market efficiency.[<a 
href="#XDarrat:2001aa">6</a>] Two different approaches were applied, the standard variance-ratio
test, and a model-comparison test that compares the ex post forecasts from a naive model
with those obtained from several alternative models such as ARIMA, GARCH, and
ANNs. To evaluate ex post forecasts, Darrat et al. made use of several procedures
including root-mean-square error (RMSE), mean absolute error (MAE), uncertainty
coefficient, and encompassing tests. In contrast to the variance-ratio test, results from
the model-comparison approach were quite decisive in rejecting the random-walk
hypothesis in both Chinese stock markets. Moreover, the results showed strong support
for the ANN as a potentially useful device for predicting stock prices in emerging
markets.
<!--l. 31--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">2.2 </span> <a 
 id="x1-130002.2"></a>Statistical Machine Learning</h3>
                                                                                
                                                                                
<!--l. 33--><p class="noindent" >A machine learning algorithm is an algorithm that is able to learn through examples from
data. To understand what an algorithm is, Cormen et al. informally describe algorithms as
&#8221;any well-defined computational procedures which takes some value, or set of values, as input
and produce some value, or set of values, as output. An algorithm is thus a sequence of
computational steps that transform the input into the output.&#8221;[<a 
href="#XCormen:2009aa">7</a>] In simple terms, it
is possible to say that an algorithm is a sequence of steps which allow to solve a
certain task. Similarly to a normal algorithm, a machine learning algorithm as defined
formally by Tom M. Mitchell, states that &#8221;A computer program is said to learn
from experience E with respect to some class of tasks T and performance measure
P, if its performance at tasks in T, as measured by P, improves with experience
E.&#8221;[<a 
href="#XMitchell:1997aa">8</a> ]
<!--l. 35--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.1 </span> <a 
 id="x1-140002.2.1"></a>Classification</h4>
<!--l. 37--><p class="noindent" >Classifiers are a class of machine learning algorithms which identify in which category a new
observation belongs to, on the basis of a training set of data containing observations
whose category membership is known. A model based on discriminant analysis was
sought out to categorise a stock as manipulated or non-manipulated based on certain
key variables that capture the characteristics of the stock.[<a 
href="#XMurugesan:2012aa">9</a>] The model in which
Murugesan et al. chose, helps them identify stocks witnessing activities that are
indicative of potential manipulation irrespective of the type of manipulation, such as
action-based, information-based, or trade-based. The model which they proposed, helps
investigators to arrive at a shortlist of securities that are potentially manipulated
and which could be subject to further detailed investigation to detect the type and
nature of the manipulation, if any. In a market like India, where there are about
5000 plus securities listed on its major exchanges, it becomes extremely difficult to
monitor all securities for potential market abuse. Academics who have earlier used
discriminant analysis have used the Linear Classification Function without validating the
assumption that governs the model. Through their research, they have tested the
assumption on data from the Indian capital market and found that the data does
not comply with the assumptions that govern the use of the linear classification
function. This therefore resulted in them using the Quadratic Classification Function,
which is the appropriate technique for instances where the data does not meet the
sated assumptions, to categorise stocks into two categories, namely manipulated and
non-manipulated.
                                                                                
                                                                                
<!--l. 39--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.2 </span> <a 
 id="x1-150002.2.2"></a>Support Vector Machines</h4>
<!--l. 41--><p class="noindent" >Support vector machines (SVM), are a class of machine learning algorithms that have become
incredibly popular in the past few years. SVMs are very similar to classifiers in the sense that
they also classify data by drawing a line, called a decision boundary, to separate
them. However, SVMs go a step further by calculating a vector from the data point
with the smallest margin to the decision boundary. This is called a support vector.
There exists vast research articles which predict the stock market as well pricing of
stock index financial instruments but most of the proposed models focus on the
accurate forecasting of the levels of the underlying stock index. There is a lack of
studies examining the predictability of the direction of stock index movement. Given
the notion that a prediction with little forecast error does not necessarily translate
into capital gain, the authors of this research attempt to predict the direction of
the S&amp;P CNX NIFTY Market Index of the National Stock Exchange, one of the
fastest growing financial exchanges in developing Asian countries.[<a 
href="#XKumar:2016aa">10</a>] Random forest
and Support Vector Machines (SVM) are very specific type of machine learning
method, and are promising tools for the prediction of financial time series. The tested
classification models, which predict direction, include linear discriminant analysis,
logit, artificial neural network, random forest, and SVM. Empirical experimentation
suggests that the SVM outperforms the other classification methods in terms of
predicting the direction of the stock market movement and random forest method
outperforms neural network, discriminant analysis and logit model used in their
study.
<!--l. 43--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.3 </span> <a 
 id="x1-160002.2.3"></a>Regression</h4>
<!--l. 45--><p class="noindent" >Regression analysis is widely used for prediction and forecasting to understand which among
the independent variables are related to the dependent variable while also exploring the forms
of these relationships. In restricted circumstances, regression analysis can be used to infer
causal relationships between the independent and dependent variables. Regression analysis
helps one understand how the typical value of the dependent variable changes when any one of
the independent variables is varied, while the other independent variables are held fixed.
                                                                                
                                                                                
Kakushadze et al. provide a systematic quantitative framework in what is intended to
be a pedagogical fashion for discussing mean-reversion and optimisation.[<a 
href="#XKakushadze:2015aa">11</a>] In
their paper, they start off their research with pair trading and add complexity by
following the sequence mean-reversion via demeaning, regression, weighted regression,
(constrained) optimization, factor models. They discuss in further detail how to conduct
mean-reversion based on this approach, including common pitfalls encountered in
practical applications, such as the difference between maximising the Sharpe ratio and
minimising an objective function when trading costs are included. Kakushadze et al.
also discuss explicit algorithms for optimization with linear costs, constraints and
bounds, and also illustrate their discussion on an explicit intraday mean-reversion
alpha.
<!--l. 47--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.4 </span> <a 
 id="x1-170002.2.4"></a>Decision Trees</h4>
<!--l. 49--><p class="noindent" >Decision tree learning uses a decision tree as a predictive model which maps observations
about an item, represented in the branches, to conclusions about the item&#8217;s target
value represented in the leaves. Tree models where the target variable can take
a finite set of values are called classification trees; in these tree structures, leaves
represent class labels and branches represent conjunctions of features that lead to those
class labels. Decision trees where the target variable can take continuous values,
typically real numbers, are called regression trees. Creamer et al. propose a multi-stock
automated trading system which relies on a layered structure consisting of a machine
learning algorithm, an online learning utility, and a risk management overlay.[<a 
href="#XCreamer:2010aa">12</a>] An
alternating decision tree (ADT), which is implemented with Logitboost, was chosen
as their underlying algorithm. One of the strengths of their approach is that the
algorithm is able to select the best combination of rules derived from well-known
technical analysis indicators and is also able to select the best parameters of the
technical indicators. Additionally, their online learning layer combines the output of
several ADTs and suggests a short or long position. Finally, the risk management
layer in which they implemented, can validate the trading signal when it exceeds a
specified non-zero threshold and limit the application of their trading strategy when
it is not profitable. They tested the expert weighting algorithm with data of 100
randomly selected companies of the S&amp;P 500 index during the period 20032005. They
found that their algorithm generates abnormal returns during the test period. Their
experiments show that the boosting approach was able to improve the predictive capacity
when indicators were combined and aggregated as a single predictor. Even more, the
                                                                                
                                                                                
combination of indicators of different stocks demonstrated to be adequate in order to
reduce the use of computational resources, and still maintain an adequate predictive
capacity.
<!--l. 51--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">2.3 </span> <a 
 id="x1-180002.3"></a>Bayesian Statistics</h3>
<!--l. 53--><p class="noindent" >Bayesian Statistics, a form of probabilistic programming, describes probabilistic models and
then performs inference in those models. Probabilistic reasoning is a foundational technology
of machine learning and has been used by companies such as Google, Amazon, and Microsoft.
Probabilistic reasoning has been used for predicting stock prices, recommending
movies, diagnosing computers, detecting cyber intrusions, and image detection.
Gelman et al. defines bayesian inference as the process of fitting a probability model
to a set of data and summarising the result by a probability distribution on the
parameters of the model and on unobserved quantities such as predictions for new
observations.[<a 
href="#XGelman:2014aa">13</a>]
<!--l. 55--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.3.1 </span> <a 
 id="x1-190002.3.1"></a>Markov Chain Monte Carlo (MCMC)</h4>
<!--l. 57--><p class="noindent" >In statistics, (MCMC) methods are a class of algorithms for sampling from a probability
distribution based on constructing a Markov chain that has the desired distribution of its
equilibrium distribution. The state of the chain after a number of steps is then used as a
sample of the desired distribution. The quality of the sample improves as a function of the
number of steps.
<!--l. 59--><p class="noindent" >Bond yields today are well below and stock market valuations are well above their historical
average.[<a 
href="#XBlanchett:2013aa">16</a> ] There are no historical periods in the United States where comparable low bond
yields and high equity valuations have occurred simultaneously. Both current bond
yields and stock values have been shown to predict near-term returns. Portfolio
returns in the first decade of retirement have an outsize impact on retirement income
strategies. Traditional Monte Carlo simulation approaches generally do not incorporate
market valuations into their analysis. In order to simulate how retirees will fare in a
low return environment for both stocks and bonds, Blanchett et al. incorporate
the predictive ability of current valuations to simulate its impact on retirement
                                                                                
                                                                                
portfolios. Blanchett et al. estimate bond returns through an autoregressive model that
uses an initial bond yield value where yields drift in the future. Blanchett et al. use
the cyclically adjusted price-to-earnings (CAPE) ratio as an estimate of market
valuation to predict short-run stock performance. Our simulations indicate that the
safety of a given withdrawal strategy is significantly affected by the initial bond yield
and CAPE value at retirement, and that the relative impact varies based on the
portfolio equity allocation. Using valuation measures current as of April 15, 2013, which
is a bond yield of 2.0% and a CAPE of 22, Blanchett et al. find the probability
of success for a 40% equity allocation with a 4% initial withdrawal rate over a 30
year period is approximately 48%. This success rate is materially lower than past
studies and has sobering implications on the likelihood of success for retirees today, as
well as how much those near retirement may need to save to ensure a successful
retirement.
<!--l. 61--><p class="noindent" >Hoffman et al. introduce the &#8217;No-U-Turn-Sampler&#8217;, an extension to the Hamiltonian Monte
Carlo (HMC), which is an MCMC algorithm that avoids the random walk behavioir and
sensitivity to correlated paramaters that plague many MCMC methods by taking a
series of steps informed by first-order gradient information.[<a 
href="#XMatthew-D.-Hoffman:2014aa">18</a>] In their paper, they
claim HMCs performance to be highly sensitive to two user-specified parameters: a
step size, and a desired number of steps. NUTS is an improvement from HMC as
it eliminates the need to set a number of steps. Their algorithm uses a recursive
algorithm to build a set of likely candidate points that spans a wide swath of the target
distribution, stopping automatically when it starts to double back and retrace its
steps.
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;3</span><br /><a 
 id="x1-200003"></a>Experimental Setup</h2>
<h3 class="sectionHead"><span class="titlemark">3.1 </span> <a 
 id="x1-210003.1"></a>Data Tidying</h3>
<!--l. 4--><p class="noindent" >A data set containing end of day stock prices, dividends, and splits for 3,000 US companies,
curated by the Quandl community, and released into the public domain, was used. The date
column in the CSV file was loaded into memory, and said column was converted to a date data
type. The DataFrame was then sorted using the date column, starting from the oldest date,
ending with the latest. The date column was also set to the index. The DataFrame was split
into two, the training data set consisting of 80%, and the test data set consisting of 20% of the
original DataFrame.
<!--l. 6--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.2 </span> <a 
 id="x1-220003.2"></a>Stock Selection</h3>
<!--l. 7--><p class="noindent" >The pairwise correlation of all the columns in the DataFrame was computing and stored in a
new DataFrame. A list of stock pairs with low correlation were extracted.
<!--l. 9--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.3 </span> <a 
 id="x1-230003.3"></a>Time Series Analysis</h3>
<!--l. 10--><p class="noindent" >The selected stocks was extracted from the data set and stored in a DataFrame. The log
returns of the stocks were calculated by calculating the logarithm of the stock&#8217;s adjusted close
price divided by the following day&#8217;s adjusred close price. The resulting values from the
calculation were then stored in a new column in the DataFrame and all infinite values were
dropped from the series.
<!--l. 12--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.1 </span> <a 
 id="x1-240003.3.1"></a>Random Walk</h4>
<!--l. 13--><p class="noindent" >The first difference of the stocks were calculated abd stored in a new column in the Dataframe
and all infinite valued were dropped.
                                                                                
                                                                                
<!--l. 15--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.2 </span> <a 
 id="x1-250003.3.2"></a>Ordinary Least Squares (OLS)</h4>
<!--l. 16--><p class="noindent" >The series was fitted to an OLS model. The 15 day SMA was used as a nobs x k array where
nobs is the number of observations and k is the number of regressors, to train the model, while
the adjusted close price of the stock was used as the dependent variable for the model to
predict. The mean absolute error, mean squared error, median absolute error, and r2 score
were used as metrics in order to rank the performance of the model&#8217;s prediction capabilities in
in-sample testing.
<!--l. 18--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.3 </span> <a 
 id="x1-260003.3.3"></a>Auto Regressive (AR)</h4>
<!--l. 19--><p class="noindent" >The series was fitted to an AR(p) model with a maximum lag value of 30. The IC was used
to fit the AR model in order to select the optimal lag length. No constants were
passed when fitting the AR model to the series. The optimal lag for the fit of the AR
model was then calculated using the same paramaeters passed when fitting the AR
model.
<!--l. 21--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.4 </span> <a 
 id="x1-270003.3.4"></a>Moving Average (MA)</h4>
<!--l. 22--><p class="noindent" >The series was fitted to an MA(p, q) model with an order selected based on the lowest AIC. A
maximum lag of 30 was once again passed to the MA model with no constant. The
exact loglikelihood for the fit of the MA model was maximized via the Kalman
Filter.
<!--l. 24--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.5 </span> <a 
 id="x1-280003.3.5"></a>Auto Regressive Moving Average (ARMA)</h4>
                                                                                
                                                                                
<!--l. 25--><p class="noindent" >The series was fitted to an ARMA(p, q, r) model with an order selected based on the lowest
AIC. No constants were passed to the ARMA model. The exact loglikelihood for the fit of the
ARMA model was maximized via the Kalman Filter.
<!--l. 27--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.6 </span> <a 
 id="x1-290003.3.6"></a>Auto Regressive Integrated Moving Average (ARIMA)</h4>
<!--l. 28--><p class="noindent" >The series was fitted to an ARIMA(p, q, r) model with an order selected based on the lowest
AIC. No constants were passed to the ARIMA model. The exact loglikelihood for the fit of the
ARIMA model was maximized via the Kalman Filter.
<!--l. 30--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.4 </span> <a 
 id="x1-300003.4"></a>Machine Learning</h3>
<!--l. 31--><p class="noindent" >The data set was split for training and testing purposes when fitting and predicting data. The
training data set consisted of 80% of the whole data set, while the test data set consisted of
20%.
<!--l. 33--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.4.1 </span> <a 
 id="x1-310003.4.1"></a>Classification</h4>
<!--l. 34--><p class="noindent" >2, 3, 4, 5, and 6 day SMAs were used as the features to train the models, while a binary value
used to determine whether the current day&#8217;s adjusted close has risen or not from the previous
day was used as the target valued for the model to predict. In-sample testing was carried out
using the models predict function, passing the test data set&#8217;s features in order to predict the
output. The classification report and confusion matrix were used as metrics in order to rank
the performance of the model&#8217;s prediction capabilities in in-sample testing. An algorithm was
developed for out-of-sample testing. The algorithm iterates for a number of n steps, increasing
the index by 1 each step, forecasting the next day&#8217;s outcome, and calculating the
SMAs.
                                                                                
                                                                                
<!--l. 36--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.1 </span> <a 
 id="x1-320003.4.1.1"></a>Decision Tree</h5>
<!--l. 37--><p class="noindent" >The decision tree classifier was fit using the Gini impurity criterion to measure the quality of
the split. The model was given the liberty to select the best strategy in order to split the
tree at each node. The maximum allowed depth of the tree was left unrestricted,
which was the same as for the maximum leaf nodes. A threshold of 1e-7 was used
to terminate the tree growth to determine if a node is a leaf, if the impurity of a
node is below the threshold, the node is a leaf. The minimum number of samples
required to be at a leaf node was set to 1, while the minimum number of samples
required to split an internal node was set to 2. The data fitted to the model was not
pre-sorted, and the random number generator used by the model was that of Numpy&#8217;s
RandomState.
<!--l. 39--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.2 </span> <a 
 id="x1-330003.4.1.2"></a>Boosted Decision Tree</h5>
<!--l. 40--><p class="noindent" >The boosted decision tree was fit using the &#8217;SAME.R&#8217; real algorithm which converges at a
faster rate, achieving a lower test error with fewer boosting iterations. The maximum number
of estimators at which boosting is terminated was set to 50; in case of perfect fit, the learning
procedure is stopped early. The learning rate which is the contribution of each classifier of the
model was shrunk by 1. The best estimator used to fit the data to the model was a Decision
Tree Classifier, and the random number generator used by the model was that of Numpy&#8217;s
RandomState.
<!--l. 42--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.3 </span> <a 
 id="x1-340003.4.1.3"></a>Support Vector Machine (SVM)</h5>
<!--l. 43--><p class="noindent" >The C-Support Vector Classification implementation was used for the SVM model with a 0
penalty parameter of the error term. A kernal type of &#8217;rbf&#8217; was used when fitted the model to
the data, along with a polynomial kernel function degree of 3. The gama &#8217;rbf&#8217; Kernel
coefficient was calculated by dividing the number of features by 1, while no probability
estimates were used when fitting. A shrinking heuristic was used and a tolerance of 1e-3 was
used for stopping criterion. A cache size of 200MB was used for the kernel when
fitting the model, and all classes were assigned a weight of one. Verbose output
                                                                                
                                                                                
was not used, and the random number generator used by the model was that of
Numpy&#8217;s RandomState. No limits were set on the iterations within the solver, and a
one-vs-rest decision function of shape (number of samples, number of classes) was
returned.
<!--l. 45--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.4 </span> <a 
 id="x1-350003.4.1.4"></a>Random Forest</h5>
<!--l. 46--><p class="noindent" >The random forest was fit with bootstrap samples when building the trees, while all the
weights associated were set to 1. The &#8217;gini&#8217; function to measure the quality of a split, and no
maximum depth of the tree was set, allowing the nodes to expand until all leaves are pure or
until all leaves contain less than the minimum split samples. The number of features to
consider when looking for the best split was the square root of the number of passed,
and no limit on the maximum leaf nodes for growing trees was set. A threshold
of 1e-7 was used to terminate the tree growth to determine if a node is a leaf, if
the impurity of a node is below the threshold, the node is a leaf. The minimum
number of samples required to be at a leaf node was set to 1, and the minimum
number of samples required to split an internal node was set to 2. The minimum
weighted fraction of the sum total of weights (of all the input samples) required to be
at a leaf node was set to 0, and the number of trees in the forest was set to 10.
The number of jobs to use for the computation was set to 1, making use of only 1
CPU core, and out-of-bag samples to estimate the generalization accuracy were
not used. The verbosity of the tree building process was not controlled, and the
random number generator used by the model was that of Numpy&#8217;s RandomState.
The model was built using a cold start by not making use of the previous call to
fit and add more estimators to the ensemble, meaning a whole new forest was fit
instead.
<!--l. 48--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.5 </span> <a 
 id="x1-360003.4.1.5"></a>K-Nearest Neighbour</h5>
<!--l. 49--><p class="noindent" >The k-nearest neighbour was fit with a number of 5 neighbors to use for k-neighbours queries,
with a uniform weight making all points in each neighbourhood weighted equally when
carrying out predictions. The model was left at liberty to select the most appropriate
algorithm based on the values passed when fitting the model to the data. The lead size of the
                                                                                
                                                                                
model was set to 30, and a minkowski distance metric with p equal to 2 which equivalent to
the standard Euclidean metric. The number of jobs to use for the computation was set to 1,
making use of only 1 CPU core.
<!--l. 51--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.6 </span> <a 
 id="x1-370003.4.1.6"></a>Logistic Regression</h5>
<!--l. 52--><p class="noindent" >The logistic regression was fit with an inverse of regularization of strength 0 specifying
stronger regularisation, while all the weights associated were set to 1. Dual formulation was
used in conjunction with the l2 penalty with a liblinear solver. A constant (bias or intercept)
was added to the decision function, and the intercept scalar was set to 1. The maximum
number of iterations taken for the solvers to converge were set to 100, while the multiclass
option used was ovr, thus fitting binary problem for each label. The number of jobs to use for
the computation was set to 1, making use of only 1 CPU core, and the random number
generator used by the model was that of Numpy&#8217;s RandomState. The tolerance for stopping
criteria was set to 0.0001, and verbosity was used with a value of 0 for the liblinear solver.
The model was built using a cold start by not making use of the previous call to
fit and add more estimators to the ensemble, meaning a whole new forest was fit
instead.
<!--l. 54--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.7 </span> <a 
 id="x1-380003.4.1.7"></a>Bernoulli Naive Bayes</h5>
<!--l. 55--><p class="noindent" >The model was fit with an additive (Laplace/Lidstone) smoothing parameter of 0 for no
smoothing, and the threshold for binarising (mapping to booleans) of sample features was set
to 0, and was presumed to already consist of binary vectors. The model was set to learn class
prior probabilities, which means that the priors were adjusted according to the
data.
<!--l. 57--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.8 </span> <a 
 id="x1-390003.4.1.8"></a>Gaussian Naive Bayes</h5>
                                                                                
                                                                                
<!--l. 58--><p class="noindent" >The model was fit to the data with no previous prior probabilities of the classes, thus the
priors were not adjusted according to the data.
<!--l. 60--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.9 </span> <a 
 id="x1-400003.4.1.9"></a>Neural Network</h5>
<!--l. 61--><p class="noindent" >The model was fit to the data with 3 hidden layer of 100 neurons each using the rectified
linear unit activation function for the hidden layer. The L2 penalty parameter used, which is
the regularisation term, was set to 0.0001, and the size of minibatches for stochastic
optimisers was set to the minimum of the two arguments being the number 200 and the
number of samples. The solver used for weight optimisation was the &#8217;adam&#8217; solver,
which refers to a stochastic gradient-based optimiser. Since the &#8217;adam&#8217; solver was
used, the exponential decay rate for estimates of first moment vector was set to 0.9,
while the second moment vector was set to 0.999. Early stopping was not used to
terminate training when validation score is not improving whilst fitting the data to
the model, and the value for numerical stability in the &#8217;adam&#8217; solver was set to
1e-08. A constant learning rate was used for weight updates in conjunction with the
initial learning rate, which controls the step-size in updating the weights, and was
set to 0.001. The maximum number of iterations which the solver iterates until
convergence was set to 200, and the momentum for gradient descent update was set
to 0.9. Nesterov&#8217;s momentum was used, and the exponent for the inverse scaling
learning rate, which is used in updating the effective learning rate, was set to 0.5.
A random number generator was not used, and the samples were shuffled in each
iteration. The tolerance for optimisation was set to 0.0001 when the loss or score is not
improving by at least the tolerance score set for two consecutive iterations in which
convergence is considered to be reached and training is stopped. The validation
fraction, which is the portion of training data to set aside as validation set for early
stopping, was set to 0.1. The model was built using a cold start by not making
use of the previous call to fit as initialisation, meaning the previous solution was
erased.
<!--l. 63--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.10 </span> <a 
 id="x1-410003.4.1.10"></a>Stochastic Gradient Descent</h5>
<!--l. 64--><p class="noindent" >The model was fit to the data using the ordinary least squares fit squared epsilon insensitive
                                                                                
                                                                                
loss function, which ignores errors less than epsilon and is linear past that; this is the loss
function used in SVR. The penalty used, also referred to regularisation term, was &#8217;l2&#8217; which is
the standard regularizer for linear SVM models, and the constant used to multiply the
regularisation term was 0.0001. The elastic net mixing paramater was set to 0.15, while the fit
intercept was estimated, meaning the data was not assumed to be already centered. A
total of 5 passes over the training data, also known as epochs, were used, and the
training data was shuffled after each epoch. No seed from the pseudo random number
generated was used while shuffling the data, and a level 0 verbosity was used. The
epsilon threshold in the epsilon-insensitive loss functions was set to 0.1, and any
differences between the current prediction and the correct label were ignored if they were
less than the threshold. The learning rate schedule used was &#8217;invscaling&#8217;, and the
initial learning rate was set to 0.01. The exponent for inverse scaling learning rate
was set to 0.25, and the model was built using a cold start by not making use of
the previous call to fit as initialisation. The SGD weights of the model were not
averaged.
<!--l. 66--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.4.2 </span> <a 
 id="x1-420003.4.2"></a>Regression</h4>
<!--l. 67--><p class="noindent" >15 and 50 day SMAs were used as the features to train the models, while the adjusted close
price of the stock was used as the target valued for the model to predict. In-sample testing was
carried out using the models predict function, passing the test data set&#8217;s features in order to
predict the output. The mean absolute error, mean squared error, median absolute error, and
<span 
class="cmmi-10x-x-109">R</span><sup><span 
class="cmr-8">2</span> </sup> (coefficient of determination) score were used as metrics in order to rank the performance
of the model&#8217;s prediction capabilities in in-sample testing. An algorithm was developed for
out-of-sample testing. The algorithm iterates for a number of n steps, increasing
the index by 1 each step, forecasting the next day&#8217;s outcome, and calculating the
SMAs.
<!--l. 69--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.1 </span> <a 
 id="x1-430003.4.2.1"></a>Decision Tree</h5>
<!--l. 70--><p class="noindent" >The decision tree model was fit with a mean squared error, which is equal to variance
reduction as feature selection criterion, and mae for the mean absolute error. The
maximum allowed depth of the tree was left unrestricted, which was the same as for the
                                                                                
                                                                                
maximum leaf nodes. A threshold of 1e-7 was used to terminate the tree growth to
determine if a node is a leaf, if the impurity of a node is below the threshold, the
node is a leaf. The minimum number of samples required to be at a leaf node was
set to 1, while the minimum number of samples required to split an internal node
was set to 2. The data fitted to the model was not pre-sorted, and the random
number generator used by the model was that of Numpy&#8217;s RandomState. The model
was given the liberty to select the best strategy in order to split the tree at each
node.
<!--l. 72--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.2 </span> <a 
 id="x1-440003.4.2.2"></a>Boosted Decision Tree</h5>
<!--l. 73--><p class="noindent" >The boosted decision tree was fit with with a decision tree regressor as the base estimator
from which the boosted ensemble is built. The maximum number of estimators at which the
boosting is terminated was set to 50, and the learning rate which shrinks the contribution of
each regressor was set to 1.0. The loss function used when updating the weights after each
boosting iteration was set to linear, and the random number generator used by the model was
that of Numpy&#8217;s RandomState.
<!--l. 75--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.3 </span> <a 
 id="x1-450003.4.2.3"></a>Random Forest</h5>
<!--l. 76--><p class="noindent" >The random forest was fit with a mean absolute error, and bootstrap samples were used when
building trees. The maximum features to consider when looking for the best split were left to
the model to select the best number, and the number of jobs to run in parallel for both the
fitting of the model and its predictions. The model was built using a cold start by not making
use of the previous call to fit and add more estimators to the ensemble, meaning a whole new
forest was fit instead. A threshold of 1e-7 was used to terminate the tree growth to determine
if a node is a leaf, if the impurity of a ode is below the threshold, the node is a
leaf. The minimum number of samples required to be at a leaf node was set to 1,
while the minimum number of samples required to split an internal node was set to
2.
                                                                                
                                                                                
<!--l. 78--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.4 </span> <a 
 id="x1-460003.4.2.4"></a>Linear Regression</h5>
<!--l. 79--><p class="noindent" >The linear regression was fit with no normalised regressors, note that this makes the
hyperparameters learnt more robust and almost independent of the number of samples. The
number of jobs to use for the computation was set to 1, making use of only 1 CPU core.
The intercept of the model was calculated which centres the data being fit to the
model.
<!--l. 81--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.5 </span> <a 
 id="x1-470003.4.2.5"></a>Neural Network</h5>
<!--l. 82--><p class="noindent" >The model was fit to the data with the same paramaters as the Neural Network
classifier.
<!--l. 84--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.6 </span> <a 
 id="x1-480003.4.2.6"></a>Stochastic Gradient Descent</h5>
<!--l. 85--><p class="noindent" >The model was fit to the data with the same paramaters as the SGD classifier.
<!--l. 87--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.5 </span> <a 
 id="x1-490003.5"></a>Bayesian Statistics</h3>
<!--l. 88--><p class="noindent" >The last 500 rows of the selected stocks were extracted from the data set and stored into a
DataFrame. The log returns were calculated by dividing each day&#8217;s adjusted close with the
adjusted close of the following day, in logarithmic form. The resulting values from
the said calculation were then stored in a new column in the DataFrame and all
infinite values were dropped from the series. The sharpe ratio of the original and
predicted price returns was calculated to serve as an accuracy score in the in-sample
tests.
                                                                                
                                                                                
<!--l. 90--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.5.1 </span> <a 
 id="x1-500003.5.1"></a>No-U-Turn Sampler (NUTS)</h4>
<!--l. 91--><p class="noindent" >The model returns were modeled with a Student-t distribution with an unknown degrees of
freedom paramater, and a scale paramater determined by a latent process.
<!--l. 93--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.5.2 </span> <a 
 id="x1-510003.5.2"></a>Metropolis-Hastings</h4>
<!--l. 94--><p class="noindent" >This model was fit to the data with the same paramaters as the NUTS algorithm except that
the model was optimised using the L-BFGS-B algorithm.
<!--l. 96--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.6 </span> <a 
 id="x1-520003.6"></a>Strategy</h3>
<!--l. 97--><p class="noindent" >The automated algorithmic strategies were run over the period of 01/05/2014. An ordered
dictionary was used to store a series of data frames container the price data of the stocks
selected. The data was tidied where the columns &#8217;open&#8217;, &#8217;high&#8217;, &#8217;low&#8217;, &#8217;close&#8217;, &#8217;ex-dividend&#8217;, and
&#8217;split_ratio&#8217; were dropped from each data frame and the columns &#8217;ticker&#8217;, &#8217;adj_open&#8217;,
&#8217;adj_high&#8217;, &#8217;adj_low&#8217;, and &#8217;adj_close&#8217; were renamed to &#8217;sid&#8217;, &#8217;open&#8217;, &#8217;high&#8217;, &#8217;low&#8217;, and &#8217;close&#8217;
respectively. The ordered dictionary was converted into a panel and passed to the trading
algorithm. The allocation amount of stock to invest in was determined based on the size of the
portfolio.
<!--l. 99--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.6.1 </span> <a 
 id="x1-530003.6.1"></a>Classification</h4>
<!--l. 100--><p class="noindent" >An ordered dictionary was used to store the day&#8217;s open and close price for each stock
as the backtester simulated each trading day. The algorithm was only allowed to
run once there was enough price data, the amount chosen was 6. The 2, 3, 4, 5,
and 6 day SMAs were calculated and each stored in an array. The six arrays were
converted into an array of tuples, where the i-th tuple contains the i-th element
                                                                                
                                                                                
from each of the argument sequences or iterables. An array was created to store a
1 if the cloase price increased from the open price, while a 0 if it decreased. The
independent variables, the SMAs, and the dependant variables, the binary outcome for
the particular stock, were passed to the machine learning algorithm to predict the
next day&#8217;s close price. An order was placed on a stock if the algorithm predicted
an increase in the following day&#8217;s price. The stock was shorted if the algorithm
predicted a decrease in the following day&#8217;s price. A stop loss of 80% was placed on each
position.
<!--l. 102--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.6.2 </span> <a 
 id="x1-540003.6.2"></a>Regression</h4>
<!--l. 103--><p class="noindent" >An ordered dictionary was used to store the day&#8217;s close price for each stock as the backtester
simulated each trading day. The algorithm was only allowed to run once there was enough
price data, the amount chosen was 50. The 15 and 50 day SMAs were calculated and each
stored in an array. The two arrays were converted into an array of tuples, where
the i-th tuple contains the i-th element from each of the argument sequences or
iterables. The independent variables, the SMAs, and the dependant variables, the close
prices for the particular stock, were passed to the machine learning algorithm to
predict the next day&#8217;s close price. An order was placed on a stock if the predicted
price was higher the current day&#8217;s price. The stock was shorted if the predicted
price was lower than the current day&#8217;s price. A stop loss of 80% was placed on each
position.
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;4</span><br /><a 
 id="x1-550004"></a>Research Findings</h2> For the purpose of benchmarking the performance of the
algorithms, a total of five stocks from the basket of uncorrelated stocks were selected. These
were MSFT, CDE, NAVB, HRG, and HL.
<div class="center" 
>
<!--l. 4--><p class="noindent" >

<!--l. 5--><p class="noindent" ><img 
src="Figures/stocks.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.1: </span><span  
class="content">Basket of stocks</span></div><!--tex4ht:label?: x1-550011 -->
</div>
<div class="center" 
>
<!--l. 10--><p class="noindent" >
<div class="tabular"><table id="TBL-2" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"></colgroup><colgroup id="TBL-2-2g"><col 
id="TBL-2-2"></colgroup><colgroup id="TBL-2-3g"><col 
id="TBL-2-3"></colgroup><colgroup id="TBL-2-4g"><col 
id="TBL-2-4"></colgroup><colgroup id="TBL-2-5g"><col 
id="TBL-2-5"></colgroup><colgroup id="TBL-2-6g"><col 
id="TBL-2-6"></colgroup><colgroup id="TBL-2-7g"><col 
id="TBL-2-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-1"  
class="td11">                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-2"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-3"  
class="td11">CDE     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-4"  
class="td11">NAVB</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-5"  
class="td11">HRG   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-6"  
class="td11">HL    </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-1"  
class="td11">Mean </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-2"  
class="td11">16.983 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-3"  
class="td11">57.716 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-4"  
class="td11">3.000 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-5"  
class="td11">13.328 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-6"  
class="td11">7.154</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-1"  
class="td11">Median                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-2"  
class="td11">18.656 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-3"  
class="td11">36.700   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-4"  
class="td11">1.250  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-5"  
class="td11">7.190   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-6"  
class="td11">6.196 </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-1"  
class="td11">Maximum </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-2"  
class="td11">63.620 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-3"  
class="td11">213.178 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-4"  
class="td11">22.000</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-5"  
class="td11">89.340 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-6"  
class="td11">23.830</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-1"  
class="td11">Minimum             </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-2"  
class="td11">0.061   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-3"  
class="td11">1.730    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-4"  
class="td11">0.080  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-5"  
class="td11">1.725   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-6"  
class="td11">0.486 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-1"  
class="td11">Variance              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-2"  
class="td11">204.707</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-3"  
class="td11">2807.852</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-4"  
class="td11">18.445</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-5"  
class="td11">249.105</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-6"  
class="td11">18.445</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-1"  
class="td11">Standard Deviation</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-2"  
class="td11">14.308 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-3"  
class="td11">52.989   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-4"  
class="td11">4.29   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-5"  
class="td11">15.783 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-6"  
class="td11">4.295 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-8-1"  
class="td11">Skewness              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-8-2"  
class="td11">0.716   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-8-3"  
class="td11">0.918    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-8-4"  
class="td11">2.213  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-8-5"  
class="td11">2.708   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-8-6"  
class="td11">0.651 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-9-1"  
class="td11">Kurtosis               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-9-2"  
class="td11">0.250   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-9-3"  
class="td11">-0.550    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-9-4"  
class="td11">4.193  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-9-5"  
class="td11">6.865   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-9-6"  
class="td11">-0.081 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-10-1"  
class="td11">                </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.1: </span><span  
class="content">Equities Descriptive Statistics</span></div><!--tex4ht:label?: x1-550021 -->
</div>
<h3 class="sectionHead"><span class="titlemark">4.1 </span> <a 
 id="x1-560004.1"></a>Time Series Analysis</h3>
<!--l. 30--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.1 </span> <a 
 id="x1-570004.1.1"></a>Random Walk</h4>
<div class="center" 
>
<!--l. 32--><p class="noindent" >

                                                                                
                                                                                
<!--l. 33--><p class="noindent" ><img 
src="Figures/MSFT-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.2: </span><span  
class="content">MSFT time series analysis</span></div><!--tex4ht:label?: x1-570012 -->
</div>
<div class="center" 
>
<!--l. 38--><p class="noindent" >

<!--l. 39--><p class="noindent" ><img 
src="Figures/MSFT-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.3: </span><span  
class="content">MSFT histogram of returns</span></div><!--tex4ht:label?: x1-570023 -->
</div>
<div class="center" 
>
<!--l. 44--><p class="noindent" >

<!--l. 45--><p class="noindent" ><img 
src="Figures/CDE-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.4: </span><span  
class="content">CDE time series analysis</span></div><!--tex4ht:label?: x1-570034 -->
</div>
<div class="center" 
>
<!--l. 50--><p class="noindent" >

<!--l. 51--><p class="noindent" ><img 
src="Figures/CDE-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.5: </span><span  
class="content">CDE histogram of returns</span></div><!--tex4ht:label?: x1-570045 -->
</div>
<div class="center" 
>
<!--l. 56--><p class="noindent" >

                                                                                
                                                                                
<!--l. 58--><p class="noindent" ><img 
src="Figures/NAVB-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.6: </span><span  
class="content">NAVB time series analysis</span></div><!--tex4ht:label?: x1-570056 -->
</div>
<div class="center" 
>
<!--l. 63--><p class="noindent" >

<!--l. 65--><p class="noindent" ><img 
src="Figures/NAVB-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.7: </span><span  
class="content">NAVB histogram of returns</span></div><!--tex4ht:label?: x1-570067 -->
</div>
<div class="center" 
>
<!--l. 70--><p class="noindent" >

<!--l. 71--><p class="noindent" ><img 
src="Figures/HRG-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.8: </span><span  
class="content">HRG time series analysis</span></div><!--tex4ht:label?: x1-570078 -->
</div>
<div class="center" 
>
<!--l. 76--><p class="noindent" >

<!--l. 77--><p class="noindent" ><img 
src="Figures/HRG-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.9: </span><span  
class="content">HRG histogram of returns</span></div><!--tex4ht:label?: x1-570089 -->
</div>
<div class="center" 
>
<!--l. 82--><p class="noindent" >

                                                                                
                                                                                
<!--l. 83--><p class="noindent" ><img 
src="Figures/HL-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.10: </span><span  
class="content">HL time series analysis</span></div><!--tex4ht:label?: x1-5700910 -->
</div>
<div class="center" 
>
<!--l. 88--><p class="noindent" >

<!--l. 89--><p class="noindent" ><img 
src="Figures/HL-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.11: </span><span  
class="content">HL histogram of returns</span></div><!--tex4ht:label?: x1-5701011 -->
</div>
<!--l. 94--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.2 </span> <a 
 id="x1-580004.1.2"></a>Ordinary Least Squares (OLS)</h4>
<!--l. 95--><p class="noindent" >MSFT scored a mean absolute error regression loss of 0.810, and a coefficient of determination
of 0.991.
<div class="center" 
>
<!--l. 97--><p class="noindent" >

<!--l. 98--><p class="noindent" ><img 
src="Figures/MSFT-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.12: </span><span  
class="content">MSFT OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5800112 -->
</div>
<div class="center" 
>
<!--l. 103--><p class="noindent" >

<!--l. 104--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
                                                                                
                                                                                
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.13: </span><span  
class="content">100 day MSFT OLS in-sample forecast</span></div><!--tex4ht:label?: x1-5800213 -->
</div>
<!--l. 109--><p class="noindent" >CDE scored a mean absolute error regression loss of 0.985, and a coefficient of determination
of 0.973.
<div class="center" 
>
<!--l. 111--><p class="noindent" >

<!--l. 112--><p class="noindent" ><img 
src="Figures/CDE-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.14: </span><span  
class="content">CDE OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5800314 -->
</div>
<div class="center" 
>
<!--l. 117--><p class="noindent" >

<!--l. 118--><p class="noindent" ><img 
src="Figures/100-Day-CDE-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.15: </span><span  
class="content">100 Day CDE OLS out of sample forecast</span></div><!--tex4ht:label?: x1-5800415 -->
</div>
<!--l. 123--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.124, and a coefficient of determination
of 0.966.
<div class="center" 
>
<!--l. 125--><p class="noindent" >

<!--l. 126--><p class="noindent" ><img 
src="Figures/NAVB-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.16: </span><span  
class="content">NAVB OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5800516 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 131--><p class="noindent" >

<!--l. 133--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.17: </span><span  
class="content">100 day NAVB OLS in-sample forecast</span></div><!--tex4ht:label?: x1-5800617 -->
</div>
<!--l. 138--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.291, and a coefficient of determination
of 0.986.
<div class="center" 
>
<!--l. 140--><p class="noindent" >

<!--l. 141--><p class="noindent" ><img 
src="Figures/HRG-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.18: </span><span  
class="content">HRG OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5800718 -->
</div>
<div class="center" 
>
<!--l. 146--><p class="noindent" >

<!--l. 147--><p class="noindent" ><img 
src="Figures/100-Day-HRG-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.19: </span><span  
class="content">100 day HRG OLS in-sample forecast</span></div><!--tex4ht:label?: x1-5800819 -->
</div>
<!--l. 152--><p class="noindent" >HL scored a mean absolute error regression loss of 0.336, and a coefficient of determination of
0.963.
<div class="center" 
>
<!--l. 154--><p class="noindent" >

                                                                                
                                                                                
<!--l. 155--><p class="noindent" ><img 
src="Figures/HL-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.20: </span><span  
class="content">HL OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5800920 -->
</div>
<div class="center" 
>
<!--l. 160--><p class="noindent" >

<!--l. 161--><p class="noindent" ><img 
src="Figures/100-Day-HL-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.21: </span><span  
class="content">100 day HL OLS in-sample forecast</span></div><!--tex4ht:label?: x1-5801021 -->
</div>
<!--l. 166--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.3 </span> <a 
 id="x1-590004.1.3"></a>Auto Regressive (AR)</h4>
<!--l. 168--><p class="noindent" >MSFT scored sharpe ratios of 1.152 for the original returns, and -34.641 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 170--><p class="noindent" >

<!--l. 171--><p class="noindent" ><img 
src="Figures/MSFT-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.22: </span><span  
class="content">MSFT AR time series analysis</span></div><!--tex4ht:label?: x1-5900122 -->
</div>
<div class="center" 
>
<!--l. 176--><p class="noindent" >

<!--l. 177--><p class="noindent" ><img 
src="Figures/MSFT-AR-histogram.png" alt="PIC"  
>
                                                                                
                                                                                
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.23: </span><span  
class="content">MSFT AR histogram of returns</span></div><!--tex4ht:label?: x1-5900223 -->
</div>
<div class="center" 
>
<!--l. 182--><p class="noindent" >

<!--l. 183--><p class="noindent" ><img 
src="Figures/MSFT-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.24: </span><span  
class="content">MSFT AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-5900324 -->
</div>
<div class="center" 
>
<!--l. 188--><p class="noindent" >

<!--l. 189--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.25: </span><span  
class="content">100 day MSFT AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-5900425 -->
</div>
<!--l. 194--><p class="noindent" >CDE scored sharpe ratios of -1.050 for the original returns, and -0.239 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 196--><p class="noindent" >

<!--l. 197--><p class="noindent" ><img 
src="Figures/CDE-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.26: </span><span  
class="content">CDE AR time series analysis</span></div><!--tex4ht:label?: x1-5900526 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 202--><p class="noindent" >

<!--l. 203--><p class="noindent" ><img 
src="Figures/CDE-AR-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.27: </span><span  
class="content">CDE AR histogram of returns</span></div><!--tex4ht:label?: x1-5900627 -->
</div>
<div class="center" 
>
<!--l. 208--><p class="noindent" >

<!--l. 209--><p class="noindent" ><img 
src="Figures/CDE-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.28: </span><span  
class="content">CDE AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-5900728 -->
</div>
<div class="center" 
>
<!--l. 214--><p class="noindent" >

<!--l. 215--><p class="noindent" ><img 
src="Figures/100-Day-CDE-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.29: </span><span  
class="content">100 day CDE AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-5900829 -->
</div>
<!--l. 220--><p class="noindent" >NAVB scored sharpe ratios of -0.410 for the original returns, and 0.124 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 222--><p class="noindent" >

<!--l. 223--><p class="noindent" ><img 
src="Figures/NAVB-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.30: </span><span  
class="content">NAVB AR time series analysis</span></div><!--tex4ht:label?: x1-5900930 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 228--><p class="noindent" >

<!--l. 229--><p class="noindent" ><img 
src="Figures/NAVB-AR-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.31: </span><span  
class="content">NAVB AR histogram of returns</span></div><!--tex4ht:label?: x1-5901031 -->
</div>
<div class="center" 
>
<!--l. 234--><p class="noindent" >

<!--l. 235--><p class="noindent" ><img 
src="Figures/NAVB-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.32: </span><span  
class="content">NAVB AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-5901132 -->
</div>
<div class="center" 
>
<!--l. 240--><p class="noindent" >

<!--l. 241--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.33: </span><span  
class="content">100 day NAVB AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-5901233 -->
</div>
<!--l. 246--><p class="noindent" >HRG scored sharpe ratios of 0.627 for the original returns, and -2.603 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 248--><p class="noindent" >

                                                                                
                                                                                
<!--l. 249--><p class="noindent" ><img 
src="Figures/HRG-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.34: </span><span  
class="content">HRG AR time series analysis</span></div><!--tex4ht:label?: x1-5901334 -->
</div>
<div class="center" 
>
<!--l. 254--><p class="noindent" >

<!--l. 256--><p class="noindent" ><img 
src="Figures/HRG-AR-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.35: </span><span  
class="content">HRG AR histogram of returns</span></div><!--tex4ht:label?: x1-5901435 -->
</div>
<div class="center" 
>
<!--l. 261--><p class="noindent" >

<!--l. 262--><p class="noindent" ><img 
src="Figures/HRG-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.36: </span><span  
class="content">HL AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-5901536 -->
</div>
<div class="center" 
>
<!--l. 267--><p class="noindent" >

<!--l. 268--><p class="noindent" ><img 
src="Figures/100-Day-HRG-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.37: </span><span  
class="content">100 day HRG AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-5901637 -->
</div>
<!--l. 273--><p class="noindent" >HL scored sharpe ratios of 0.695 for the original returns, and -1.880 for the predicted returns
in the in-sample test.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 275--><p class="noindent" >

<!--l. 276--><p class="noindent" ><img 
src="Figures/HL-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.38: </span><span  
class="content">HL AR time series analysis</span></div><!--tex4ht:label?: x1-5901738 -->
</div>
<div class="center" 
>
<!--l. 281--><p class="noindent" >

<!--l. 282--><p class="noindent" ><img 
src="Figures/HL-AR-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.39: </span><span  
class="content">HL AR histogram of returns</span></div><!--tex4ht:label?: x1-5901839 -->
</div>
<div class="center" 
>
<!--l. 287--><p class="noindent" >

<!--l. 289--><p class="noindent" ><img 
src="Figures/HL-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.40: </span><span  
class="content">HL AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-5901940 -->
</div>
<div class="center" 
>
<!--l. 294--><p class="noindent" >

<!--l. 296--><p class="noindent" ><img 
src="Figures/100-Day-HL-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.41: </span><span  
class="content">100 day HL AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-5902041 -->
</div>
                                                                                
                                                                                
<!--l. 301--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.4 </span> <a 
 id="x1-600004.1.4"></a>Moving Average (MA)</h4>
<!--l. 303--><p class="noindent" >MSFT scored sharpe ratios of 0.358 for the original returns, and -5.610 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 305--><p class="noindent" >

<!--l. 306--><p class="noindent" ><img 
src="Figures/MSFT-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.42: </span><span  
class="content">MSFT MA time series analysis</span></div><!--tex4ht:label?: x1-6000142 -->
</div>
<div class="center" 
>
<!--l. 311--><p class="noindent" >

<!--l. 312--><p class="noindent" ><img 
src="Figures/MSFT-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.43: </span><span  
class="content">MSFT MA histogram of returns</span></div><!--tex4ht:label?: x1-6000243 -->
</div>
<div class="center" 
>
<!--l. 317--><p class="noindent" >

<!--l. 318--><p class="noindent" ><img 
src="Figures/MSFT-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.44: </span><span  
class="content">MSFT MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6000344 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 323--><p class="noindent" >

<!--l. 324--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.45: </span><span  
class="content">100 day MSFT MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6000445 -->
</div>
<!--l. 329--><p class="noindent" >CDE scored sharpe ratios of -0.199 for the original returns, and -0.760 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 331--><p class="noindent" >

<!--l. 332--><p class="noindent" ><img 
src="Figures/CDE-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.46: </span><span  
class="content">CDE MA time series analysis</span></div><!--tex4ht:label?: x1-6000546 -->
</div>
<div class="center" 
>
<!--l. 337--><p class="noindent" >

<!--l. 338--><p class="noindent" ><img 
src="Figures/CDE-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.47: </span><span  
class="content">CDE MA histogram of returns</span></div><!--tex4ht:label?: x1-6000647 -->
</div>
<div class="center" 
>
<!--l. 343--><p class="noindent" >

<!--l. 344--><p class="noindent" ><img 
src="Figures/CDE-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.48: </span><span  
class="content">CDE MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6000748 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 349--><p class="noindent" >

<!--l. 350--><p class="noindent" ><img 
src="Figures/100-Day-CDE-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.49: </span><span  
class="content">100 day CDE MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6000849 -->
</div>
<!--l. 355--><p class="noindent" >NAVB scored sharpe ratios of -0.067 for the original returns, and 0.590 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 357--><p class="noindent" >

<!--l. 358--><p class="noindent" ><img 
src="Figures/NAVB-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.50: </span><span  
class="content">NAVB MA time series analysis</span></div><!--tex4ht:label?: x1-6000950 -->
</div>
<div class="center" 
>
<!--l. 363--><p class="noindent" >

<!--l. 364--><p class="noindent" ><img 
src="Figures/NAVB-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.51: </span><span  
class="content">NAVB MA histogram of returns</span></div><!--tex4ht:label?: x1-6001051 -->
</div>
<div class="center" 
>
<!--l. 369--><p class="noindent" >

                                                                                
                                                                                
<!--l. 370--><p class="noindent" ><img 
src="Figures/NAVB-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.52: </span><span  
class="content">NAVB MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6001152 -->
</div>
<div class="center" 
>
<!--l. 375--><p class="noindent" >

<!--l. 376--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.53: </span><span  
class="content">100 day NAVB MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6001253 -->
</div>
<!--l. 381--><p class="noindent" >HRG scored sharpe ratios of 0.084 for the original returns, and -1.020 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 383--><p class="noindent" >

<!--l. 384--><p class="noindent" ><img 
src="Figures/HRG-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.54: </span><span  
class="content">HRG MA time series analysis</span></div><!--tex4ht:label?: x1-6001354 -->
</div>
<div class="center" 
>
<!--l. 389--><p class="noindent" >

<!--l. 390--><p class="noindent" ><img 
src="Figures/HRG-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.55: </span><span  
class="content">HRG MA histogram of returns</span></div><!--tex4ht:label?: x1-6001455 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 395--><p class="noindent" >

<!--l. 396--><p class="noindent" ><img 
src="Figures/HRG-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.56: </span><span  
class="content">HRG MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6001556 -->
</div>
<div class="center" 
>
<!--l. 401--><p class="noindent" >

<!--l. 402--><p class="noindent" ><img 
src="Figures/100-Day-HRG-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.57: </span><span  
class="content">100 day HRG MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6001657 -->
</div>
<!--l. 407--><p class="noindent" >HL scored sharpe ratios of -0.128 for the original returns, and -0.977 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 409--><p class="noindent" >

<!--l. 410--><p class="noindent" ><img 
src="Figures/HL-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.58: </span><span  
class="content">HL MA time series analysis</span></div><!--tex4ht:label?: x1-6001758 -->
</div>
<div class="center" 
>
<!--l. 415--><p class="noindent" >

<!--l. 416--><p class="noindent" ><img 
src="Figures/HL-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.59: </span><span  
class="content">HL MA histogram of returns</span></div><!--tex4ht:label?: x1-6001859 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 421--><p class="noindent" >

<!--l. 422--><p class="noindent" ><img 
src="Figures/HL-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.60: </span><span  
class="content">HL MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6001960 -->
</div>
<div class="center" 
>
<!--l. 427--><p class="noindent" >

<!--l. 428--><p class="noindent" ><img 
src="Figures/100-Day-HL-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.61: </span><span  
class="content">100 day HL MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6002061 -->
</div>
<!--l. 433--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.5 </span> <a 
 id="x1-610004.1.5"></a>Auto Regressive Moving Average (ARMA)</h4>
<!--l. 435--><p class="noindent" >HL scored sharpe ratios of 0.537 for the original returns, and -6.485 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 437--><p class="noindent" >

<!--l. 438--><p class="noindent" ><img 
src="Figures/MSFT-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.62: </span><span  
class="content">MSFT ARMA time series analysis</span></div><!--tex4ht:label?: x1-6100162 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 443--><p class="noindent" >

<!--l. 444--><p class="noindent" ><img 
src="Figures/MSFT-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.63: </span><span  
class="content">MSFT ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6100263 -->
</div>
<div class="center" 
>
<!--l. 449--><p class="noindent" >

<!--l. 450--><p class="noindent" ><img 
src="Figures/MSFT-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.64: </span><span  
class="content">MSFT ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6100364 -->
</div>
<div class="center" 
>
<!--l. 455--><p class="noindent" >

<!--l. 456--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.65: </span><span  
class="content">100 day MSFT ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6100465 -->
</div>
<!--l. 461--><p class="noindent" >MSFT scored sharpe ratios of -0.128 for the original returns, and -0.977 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 463--><p class="noindent" >

<!--l. 464--><p class="noindent" ><img 
src="Figures/CDE-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.66: </span><span  
class="content">CDE ARMA time series analysis</span></div><!--tex4ht:label?: x1-6100566 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 469--><p class="noindent" >

<!--l. 470--><p class="noindent" ><img 
src="Figures/CDE-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.67: </span><span  
class="content">CDE ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6100667 -->
</div>
<div class="center" 
>
<!--l. 475--><p class="noindent" >

<!--l. 476--><p class="noindent" ><img 
src="Figures/CDE-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.68: </span><span  
class="content">CDE ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6100768 -->
</div>
<div class="center" 
>
<!--l. 481--><p class="noindent" >

<!--l. 482--><p class="noindent" ><img 
src="Figures/100-Day-CDE-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.69: </span><span  
class="content">100 day CDE ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6100869 -->
</div>
<!--l. 487--><p class="noindent" >CDE scored sharpe ratios of -0.245 for the original returns, and -0.614 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 489--><p class="noindent" >

                                                                                
                                                                                
<!--l. 490--><p class="noindent" ><img 
src="Figures/NAVB-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.70: </span><span  
class="content">NAVB ARMA time series analysis</span></div><!--tex4ht:label?: x1-6100970 -->
</div>
<div class="center" 
>
<!--l. 495--><p class="noindent" >

<!--l. 496--><p class="noindent" ><img 
src="Figures/NAVB-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.71: </span><span  
class="content">NAVB ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6101071 -->
</div>
<div class="center" 
>
<!--l. 501--><p class="noindent" >

<!--l. 502--><p class="noindent" ><img 
src="Figures/NAVB-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.72: </span><span  
class="content">NAVB ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6101172 -->
</div>
<div class="center" 
>
<!--l. 507--><p class="noindent" >

<!--l. 508--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.73: </span><span  
class="content">100 day NAVB ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6101273 -->
</div>
<!--l. 513--><p class="noindent" >NAVB scored sharpe ratios of -0.032 for the original returns, and -0.641 for the predicted
returns in the in-sample test.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 515--><p class="noindent" >

<!--l. 516--><p class="noindent" ><img 
src="Figures/HRG-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.74: </span><span  
class="content">HRG ARMA time series analysis</span></div><!--tex4ht:label?: x1-6101374 -->
</div>
<div class="center" 
>
<!--l. 521--><p class="noindent" >

<!--l. 522--><p class="noindent" ><img 
src="Figures/HRG-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.75: </span><span  
class="content">HRG ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6101475 -->
</div>
<div class="center" 
>
<!--l. 527--><p class="noindent" >

<!--l. 528--><p class="noindent" ><img 
src="Figures/HRG-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.76: </span><span  
class="content">HRG ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6101576 -->
</div>
<div class="center" 
>
<!--l. 533--><p class="noindent" >

<!--l. 534--><p class="noindent" ><img 
src="Figures/100-Day-HRG-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.77: </span><span  
class="content">100 day HRG ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6101677 -->
</div>
                                                                                
                                                                                
<!--l. 539--><p class="noindent" >HL scored sharpe ratios of 0.111 for the original returns, and -1.023 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 541--><p class="noindent" >

<!--l. 543--><p class="noindent" ><img 
src="Figures/HL-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.78: </span><span  
class="content">HL ARMA time series analysis</span></div><!--tex4ht:label?: x1-6101778 -->
</div>
<div class="center" 
>
<!--l. 548--><p class="noindent" >

<!--l. 549--><p class="noindent" ><img 
src="Figures/HL-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.79: </span><span  
class="content">HL ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6101879 -->
</div>
<div class="center" 
>
<!--l. 554--><p class="noindent" >

<!--l. 555--><p class="noindent" ><img 
src="Figures/HL-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.80: </span><span  
class="content">HL ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6101980 -->
</div>
<div class="center" 
>
<!--l. 560--><p class="noindent" >

                                                                                
                                                                                
<!--l. 561--><p class="noindent" ><img 
src="Figures/100-Day-HL-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.81: </span><span  
class="content">100 day HL ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6102081 -->
</div>
<!--l. 566--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.6 </span> <a 
 id="x1-620004.1.6"></a>Auto Regressive Integrated Moving Average (ARIMA)</h4>
<!--l. 568--><p class="noindent" >MSFT scored sharpe ratios of 0.123 for the original returns, and -4.904 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 570--><p class="noindent" >

<!--l. 571--><p class="noindent" ><img 
src="Figures/MSFT-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.82: </span><span  
class="content">MSFT ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6200182 -->
</div>
<div class="center" 
>
<!--l. 576--><p class="noindent" >

<!--l. 577--><p class="noindent" ><img 
src="Figures/MSFT-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.83: </span><span  
class="content">MSFT ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6200283 -->
</div>
<div class="center" 
>
<!--l. 582--><p class="noindent" >

<!--l. 583--><p class="noindent" ><img 
src="Figures/MSFT-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
                                                                                
                                                                                
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.84: </span><span  
class="content">MSFT ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6200384 -->
</div>
<div class="center" 
>
<!--l. 588--><p class="noindent" >

<!--l. 589--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.85: </span><span  
class="content">100 day MSFT ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6200485 -->
</div>
<!--l. 594--><p class="noindent" >CDE scored sharpe ratios of -0.133 for the original returns, and -0.698 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 596--><p class="noindent" >

<!--l. 597--><p class="noindent" ><img 
src="Figures/CDE-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.86: </span><span  
class="content">CDE ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6200586 -->
</div>
<div class="center" 
>
<!--l. 602--><p class="noindent" >

<!--l. 603--><p class="noindent" ><img 
src="Figures/CDE-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.87: </span><span  
class="content">CDE ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6200687 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 608--><p class="noindent" >

<!--l. 609--><p class="noindent" ><img 
src="Figures/CDE-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.88: </span><span  
class="content">CDE ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6200788 -->
</div>
<div class="center" 
>
<!--l. 614--><p class="noindent" >

<!--l. 615--><p class="noindent" ><img 
src="Figures/100-Day-CDE-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.89: </span><span  
class="content">100 day CDE ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6200889 -->
</div>
<!--l. 620--><p class="noindent" >NAVB scored sharpe ratios of 0.015 for the original returns, and -0.708 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 622--><p class="noindent" >

<!--l. 623--><p class="noindent" ><img 
src="Figures/NAVB-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.90: </span><span  
class="content">NAVB ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6200990 -->
</div>
<div class="center" 
>
<!--l. 628--><p class="noindent" >

<!--l. 629--><p class="noindent" ><img 
src="Figures/NAVB-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.91: </span><span  
class="content">NAVB ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6201091 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 634--><p class="noindent" >

<!--l. 635--><p class="noindent" ><img 
src="Figures/NAVB-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.92: </span><span  
class="content">NAVB ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6201192 -->
</div>
<div class="center" 
>
<!--l. 640--><p class="noindent" >

<!--l. 641--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.93: </span><span  
class="content">100 day NAVB ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6201293 -->
</div>
<!--l. 646--><p class="noindent" >HRG scored sharpe ratios of 0.401 for the original returns, and -1.354 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 648--><p class="noindent" >

<!--l. 649--><p class="noindent" ><img 
src="Figures/HRG-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.94: </span><span  
class="content">HRG ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6201394 -->
</div>
<div class="center" 
>
<!--l. 654--><p class="noindent" >

                                                                                
                                                                                
<!--l. 655--><p class="noindent" ><img 
src="Figures/HRG-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.95: </span><span  
class="content">HRG ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6201495 -->
</div>
<div class="center" 
>
<!--l. 660--><p class="noindent" >

<!--l. 661--><p class="noindent" ><img 
src="Figures/HRG-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.96: </span><span  
class="content">HRG ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6201596 -->
</div>
<div class="center" 
>
<!--l. 666--><p class="noindent" >

<!--l. 667--><p class="noindent" ><img 
src="Figures/100-Day-HRG-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.97: </span><span  
class="content">100 day HRG ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6201697 -->
</div>
<!--l. 672--><p class="noindent" >HL scored sharpe ratios of -0.127 for the original returns, and -0.974 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 674--><p class="noindent" >

<!--l. 675--><p class="noindent" ><img 
src="Figures/HL-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.98: </span><span  
class="content">HL ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6201798 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 680--><p class="noindent" >

<!--l. 681--><p class="noindent" ><img 
src="Figures/HL-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.99: </span><span  
class="content">HL ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6201899 -->
</div>
<div class="center" 
>
<!--l. 686--><p class="noindent" >

<!--l. 687--><p class="noindent" ><img 
src="Figures/HL-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.100: </span><span  
class="content">HL ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-62019100 -->
</div>
<div class="center" 
>
<!--l. 692--><p class="noindent" >

<!--l. 693--><p class="noindent" ><img 
src="Figures/100-Day-HL-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.101: </span><span  
class="content">100 day HL ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-62020101 -->
</div>
<!--l. 698--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">4.2 </span> <a 
 id="x1-630004.2"></a>Machine Learning</h3>
<!--l. 700--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.2.1 </span> <a 
 id="x1-640004.2.1"></a>Classification</h4>
                                                                                
                                                                                
<!--l. 702--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.1 </span> <a 
 id="x1-650004.2.1.1"></a>Decision Tree</h5>
<div class="center" 
>
<!--l. 704--><p class="noindent" >
<div class="tabular"><table id="TBL-3" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-3-1g"><col 
id="TBL-3-1"></colgroup><colgroup id="TBL-3-2g"><col 
id="TBL-3-2"></colgroup><colgroup id="TBL-3-3g"><col 
id="TBL-3-3"></colgroup><colgroup id="TBL-3-4g"><col 
id="TBL-3-4"></colgroup><colgroup id="TBL-3-5g"><col 
id="TBL-3-5"></colgroup><colgroup id="TBL-3-6g"><col 
id="TBL-3-6"></colgroup><colgroup id="TBL-3-7g"><col 
id="TBL-3-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-4"  
class="td11">131                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-5"  
class="td11">547              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-3"  
class="td11">565 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-4"  
class="td11">144 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-5"  
class="td11">504 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-6"  
class="td11">133</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-3"  
class="td11">592               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-4"  
class="td11">165                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-5"  
class="td11">331              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-6"  
class="td11">126               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-3"  
class="td11">553               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-4"  
class="td11">210                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-5"  
class="td11">462              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-6"  
class="td11">135               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-2"  
class="td11">0.79       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-3"  
class="td11">603               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-4"  
class="td11">135                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-5"  
class="td11">475              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-6"  
class="td11">147               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.2: </span><span  
class="content">Decision Tree results</span></div><!--tex4ht:label?: x1-650012 -->
</div>
<!--l. 719--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.2 </span> <a 
 id="x1-660004.2.1.2"></a>Boosted Decision Tree</h5>
<div class="center" 
>
<!--l. 721--><p class="noindent" >
<div class="tabular"><table id="TBL-4" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-4-1g"><col 
id="TBL-4-1"></colgroup><colgroup id="TBL-4-2g"><col 
id="TBL-4-2"></colgroup><colgroup id="TBL-4-3g"><col 
id="TBL-4-3"></colgroup><colgroup id="TBL-4-4g"><col 
id="TBL-4-4"></colgroup><colgroup id="TBL-4-5g"><col 
id="TBL-4-5"></colgroup><colgroup id="TBL-4-6g"><col 
id="TBL-4-6"></colgroup><colgroup id="TBL-4-7g"><col 
id="TBL-4-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-3"  
class="td11">564 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-4"  
class="td11">143 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-5"  
class="td11">505 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-6"  
class="td11">134</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-3"  
class="td11">588               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-4"  
class="td11">164                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-5"  
class="td11">332              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-6"  
class="td11">130               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-3"  
class="td11">542               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-4"  
class="td11">191                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-5"  
class="td11">481              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-6"  
class="td11">146               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-2"  
class="td11">0.79       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-3"  
class="td11">602               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-4"  
class="td11">132                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-5"  
class="td11">478              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-6"  
class="td11">149               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.3: </span><span  
class="content">Boosted Decision Tree results</span></div><!--tex4ht:label?: x1-660013 -->
</div>
                                                                                
                                                                                
<!--l. 736--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.3 </span> <a 
 id="x1-670004.2.1.3"></a>Support Vector Machine (SVM)</h5>
<div class="center" 
>
<!--l. 738--><p class="noindent" >
<div class="tabular"><table id="TBL-5" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-5-1g"><col 
id="TBL-5-1"></colgroup><colgroup id="TBL-5-2g"><col 
id="TBL-5-2"></colgroup><colgroup id="TBL-5-3g"><col 
id="TBL-5-3"></colgroup><colgroup id="TBL-5-4g"><col 
id="TBL-5-4"></colgroup><colgroup id="TBL-5-5g"><col 
id="TBL-5-5"></colgroup><colgroup id="TBL-5-6g"><col 
id="TBL-5-6"></colgroup><colgroup id="TBL-5-7g"><col 
id="TBL-5-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-3"  
class="td11">564 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-4"  
class="td11">143 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-5"  
class="td11">505 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-6"  
class="td11">134</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-3"  
class="td11">593               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-4"  
class="td11">169                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-5"  
class="td11">327              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-6"  
class="td11">125               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-3"  
class="td11">542               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-4"  
class="td11">191                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-5"  
class="td11">481              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-6"  
class="td11">146               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-2"  
class="td11">0.81       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-3"  
class="td11">579               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-4"  
class="td11">98                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-5"  
class="td11">512              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-6"  
class="td11">171               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.4: </span><span  
class="content">Support Vector Machine results</span></div><!--tex4ht:label?: x1-670014 -->
</div>
<!--l. 753--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.4 </span> <a 
 id="x1-680004.2.1.4"></a>Random Forest</h5>
<div class="center" 
>
<!--l. 755--><p class="noindent" >
<div class="tabular"><table id="TBL-6" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-6-1g"><col 
id="TBL-6-1"></colgroup><colgroup id="TBL-6-2g"><col 
id="TBL-6-2"></colgroup><colgroup id="TBL-6-3g"><col 
id="TBL-6-3"></colgroup><colgroup id="TBL-6-4g"><col 
id="TBL-6-4"></colgroup><colgroup id="TBL-6-5g"><col 
id="TBL-6-5"></colgroup><colgroup id="TBL-6-6g"><col 
id="TBL-6-6"></colgroup><colgroup id="TBL-6-7g"><col 
id="TBL-6-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-4"  
class="td11">131                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-5"  
class="td11">547              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-3"  
class="td11">566 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-4"  
class="td11">145 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-5"  
class="td11">503 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-6"  
class="td11">132</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-3"  
class="td11">590               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-4"  
class="td11">164                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-5"  
class="td11">332              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-6"  
class="td11">128               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-3"  
class="td11">554               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-4"  
class="td11">210                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-5"  
class="td11">462              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-6"  
class="td11">134               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-2"  
class="td11">0.81       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-3"  
class="td11">581               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-4"  
class="td11">101                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-5"  
class="td11">509              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-6"  
class="td11">169               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.5: </span><span  
class="content">Random Forest results</span></div><!--tex4ht:label?: x1-680015 -->
</div>
                                                                                
                                                                                
<!--l. 770--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.5 </span> <a 
 id="x1-690004.2.1.5"></a>K-Nearest Neighbour</h5>
<div class="center" 
>
<!--l. 772--><p class="noindent" >
<div class="tabular"><table id="TBL-7" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-7-1g"><col 
id="TBL-7-1"></colgroup><colgroup id="TBL-7-2g"><col 
id="TBL-7-2"></colgroup><colgroup id="TBL-7-3g"><col 
id="TBL-7-3"></colgroup><colgroup id="TBL-7-4g"><col 
id="TBL-7-4"></colgroup><colgroup id="TBL-7-5g"><col 
id="TBL-7-5"></colgroup><colgroup id="TBL-7-6g"><col 
id="TBL-7-6"></colgroup><colgroup id="TBL-7-7g"><col 
id="TBL-7-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-3"  
class="td11">489               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-6"  
class="td11">194               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-2"  
class="td11">0.80 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-3"  
class="td11">574 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-4"  
class="td11">147 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-5"  
class="td11">501 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-6"  
class="td11">126</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-3"  
class="td11">573               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-4"  
class="td11">161                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-5"  
class="td11">335              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-6"  
class="td11">145               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-2"  
class="td11">0.74       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-3"  
class="td11">560               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-4"  
class="td11">233                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-5"  
class="td11">439              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-6"  
class="td11">128               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-2"  
class="td11">0.81       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-3"  
class="td11">581               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-4"  
class="td11">101                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-5"  
class="td11">509              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-6"  
class="td11">169               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.6: </span><span  
class="content">K-Nearest Neighbour results</span></div><!--tex4ht:label?: x1-690016 -->
</div>
<!--l. 787--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.6 </span> <a 
 id="x1-700004.2.1.6"></a>Logistic Regression</h5>
<div class="center" 
>
<!--l. 789--><p class="noindent" >
<div class="tabular"><table id="TBL-8" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-8-1g"><col 
id="TBL-8-1"></colgroup><colgroup id="TBL-8-2g"><col 
id="TBL-8-2"></colgroup><colgroup id="TBL-8-3g"><col 
id="TBL-8-3"></colgroup><colgroup id="TBL-8-4g"><col 
id="TBL-8-4"></colgroup><colgroup id="TBL-8-5g"><col 
id="TBL-8-5"></colgroup><colgroup id="TBL-8-6g"><col 
id="TBL-8-6"></colgroup><colgroup id="TBL-8-7g"><col 
id="TBL-8-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-3"  
class="td11">564 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-4"  
class="td11">143 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-5"  
class="td11">505 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-6"  
class="td11">134</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-3"  
class="td11">588               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-4"  
class="td11">164                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-5"  
class="td11">332              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-6"  
class="td11">130               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-3"  
class="td11">542               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-4"  
class="td11">191                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-5"  
class="td11">481              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-6"  
class="td11">146               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-2"  
class="td11">0.79       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-3"  
class="td11">601               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-4"  
class="td11">132                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-5"  
class="td11">478              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-6"  
class="td11">149               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.7: </span><span  
class="content">Logistic Regression results</span></div><!--tex4ht:label?: x1-700017 -->
</div>
                                                                                
                                                                                
<!--l. 804--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.7 </span> <a 
 id="x1-710004.2.1.7"></a>Gaussian Naive Bayes</h5>
<div class="center" 
>
<!--l. 806--><p class="noindent" >
<div class="tabular"><table id="TBL-9" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-9-1g"><col 
id="TBL-9-1"></colgroup><colgroup id="TBL-9-2g"><col 
id="TBL-9-2"></colgroup><colgroup id="TBL-9-3g"><col 
id="TBL-9-3"></colgroup><colgroup id="TBL-9-4g"><col 
id="TBL-9-4"></colgroup><colgroup id="TBL-9-5g"><col 
id="TBL-9-5"></colgroup><colgroup id="TBL-9-6g"><col 
id="TBL-9-6"></colgroup><colgroup id="TBL-9-7g"><col 
id="TBL-9-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-2"  
class="td11">0.73       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-3"  
class="td11">478               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-4"  
class="td11">168                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-5"  
class="td11">510              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-6"  
class="td11">205               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-2"  
class="td11">0.76 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-3"  
class="td11">555 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-4"  
class="td11">187 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-5"  
class="td11">461 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-6"  
class="td11">143</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-2"  
class="td11">0.74       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-3"  
class="td11">553               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-4"  
class="td11">153                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-5"  
class="td11">343              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-6"  
class="td11">165               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-2"  
class="td11">0.73       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-3"  
class="td11">491               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-4"  
class="td11">170                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-5"  
class="td11">502              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-6"  
class="td11">197               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-3"  
class="td11">590               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-4"  
class="td11">157                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-5"  
class="td11">453              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-6"  
class="td11">160               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.8: </span><span  
class="content">Gaussian Naive Bayes results</span></div><!--tex4ht:label?: x1-710018 -->
</div>
<!--l. 821--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.8 </span> <a 
 id="x1-720004.2.1.8"></a>Bernoulli Naive Bayes</h5>
<div class="center" 
>
<!--l. 823--><p class="noindent" >
<div class="tabular"><table id="TBL-10" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-10-1g"><col 
id="TBL-10-1"></colgroup><colgroup id="TBL-10-2g"><col 
id="TBL-10-2"></colgroup><colgroup id="TBL-10-3g"><col 
id="TBL-10-3"></colgroup><colgroup id="TBL-10-4g"><col 
id="TBL-10-4"></colgroup><colgroup id="TBL-10-5g"><col 
id="TBL-10-5"></colgroup><colgroup id="TBL-10-6g"><col 
id="TBL-10-6"></colgroup><colgroup id="TBL-10-7g"><col 
id="TBL-10-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-2"  
class="td11">0.73       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-3"  
class="td11">479               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-4"  
class="td11">169                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-5"  
class="td11">509              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-6"  
class="td11">204               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-2"  
class="td11">0.76 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-3"  
class="td11">558 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-4"  
class="td11">187 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-5"  
class="td11">461 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-6"  
class="td11">140</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-2"  
class="td11">0.74       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-3"  
class="td11">553               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-4"  
class="td11">153                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-5"  
class="td11">343              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-6"  
class="td11">165               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-2"  
class="td11">0.73       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-3"  
class="td11">492               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-4"  
class="td11">170                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-5"  
class="td11">502              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-6"  
class="td11">196               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-3"  
class="td11">590               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-4"  
class="td11">158                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-5"  
class="td11">452              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-6"  
class="td11">160               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.9: </span><span  
class="content">Bernoulli Naive Bayes results</span></div><!--tex4ht:label?: x1-720019 -->
</div>
                                                                                
                                                                                
<!--l. 838--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.9 </span> <a 
 id="x1-730004.2.1.9"></a>Neural Network</h5>
<div class="center" 
>
<!--l. 840--><p class="noindent" >
<div class="tabular"><table id="TBL-11" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-11-1g"><col 
id="TBL-11-1"></colgroup><colgroup id="TBL-11-2g"><col 
id="TBL-11-2"></colgroup><colgroup id="TBL-11-3g"><col 
id="TBL-11-3"></colgroup><colgroup id="TBL-11-4g"><col 
id="TBL-11-4"></colgroup><colgroup id="TBL-11-5g"><col 
id="TBL-11-5"></colgroup><colgroup id="TBL-11-6g"><col 
id="TBL-11-6"></colgroup><colgroup id="TBL-11-7g"><col 
id="TBL-11-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-3"  
class="td11">685               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-4"  
class="td11">181                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-5"  
class="td11">787              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-6"  
class="td11">258               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-3"  
class="td11">616 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-4"  
class="td11">156 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-5"  
class="td11">542 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-6"  
class="td11">152</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-3"  
class="td11">691               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-4"  
class="td11">183                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-5"  
class="td11">403              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-6"  
class="td11">153               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-2"  
class="td11">0.74       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-3"  
class="td11">690               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-4"  
class="td11">269                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-5"  
class="td11">542              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-6"  
class="td11">164               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-3"  
class="td11">1059              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-4"  
class="td11">270                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-5"  
class="td11">848              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-6"  
class="td11">164               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.10: </span><span  
class="content">Neural Network results</span></div><!--tex4ht:label?: x1-7300110 -->
</div>
<!--l. 855--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.10 </span> <a 
 id="x1-740004.2.1.10"></a>Stochastic Gradient Descent</h5>
<div class="center" 
>
<!--l. 857--><p class="noindent" >
<div class="tabular"><table id="TBL-12" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-12-1g"><col 
id="TBL-12-1"></colgroup><colgroup id="TBL-12-2g"><col 
id="TBL-12-2"></colgroup><colgroup id="TBL-12-3g"><col 
id="TBL-12-3"></colgroup><colgroup id="TBL-12-4g"><col 
id="TBL-12-4"></colgroup><colgroup id="TBL-12-5g"><col 
id="TBL-12-5"></colgroup><colgroup id="TBL-12-6g"><col 
id="TBL-12-6"></colgroup><colgroup id="TBL-12-7g"><col 
id="TBL-12-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-3-2"  
class="td11">0.76 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-3-3"  
class="td11">462 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-3-4"  
class="td11">106 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-3-5"  
class="td11">542 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-3-6"  
class="td11">236</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-4-2"  
class="td11">0.59       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-4-3"  
class="td11">1032              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-4-4"  
class="td11">707                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-4-5"  
class="td11">118              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-4-6"  
class="td11">80                </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-5-2"  
class="td11">0.72       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-5-3"  
class="td11">761               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-5-4"  
class="td11">225                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-5-5"  
class="td11">1059             </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-5-6"  
class="td11">518               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-6-2"  
class="td11">0.80       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-6-3"  
class="td11">535               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-6-4"  
class="td11">83                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-6-5"  
class="td11">527              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-6-6"  
class="td11">215               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.11: </span><span  
class="content">Stochastic Gradient Descent results</span></div><!--tex4ht:label?: x1-7400111 -->
</div>
<!--l. 872--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.2.2 </span> <a 
 id="x1-750004.2.2"></a>Regression</h4>
                                                                                
                                                                                
<!--l. 874--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.1 </span> <a 
 id="x1-760004.2.2.1"></a>Decision Tree</h5>
<!--l. 875--><p class="noindent" >MSFT scored a mean absolute error regression loss of 5.606, and a coefficient of determination
of 0.458.
<div class="center" 
>
<!--l. 877--><p class="noindent" >

<!--l. 878--><p class="noindent" ><img 
src="Figures/MSFT-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.102: </span><span  
class="content">MSFT Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-76001102 -->
</div>
<div class="center" 
>
<!--l. 883--><p class="noindent" >

<!--l. 884--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.103: </span><span  
class="content">100 day MSFT Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-76002103 -->
</div>
<!--l. 889--><p class="noindent" >CDE scored a mean absolute error regression loss of 2.906, and a coefficient of determination
of 0.816.
<div class="center" 
>
<!--l. 891--><p class="noindent" >

<!--l. 892--><p class="noindent" ><img 
src="Figures/CDE-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.104: </span><span  
class="content">CDE Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-76003104 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 897--><p class="noindent" >

<!--l. 898--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.105: </span><span  
class="content">100 day CDE Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-76004105 -->
</div>
<!--l. 903--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.422, and a coefficient of determination
of 0.693.
<div class="center" 
>
<!--l. 905--><p class="noindent" >

<!--l. 906--><p class="noindent" ><img 
src="Figures/NAVB-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.106: </span><span  
class="content">NAVB Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-76005106 -->
</div>
<div class="center" 
>
<!--l. 911--><p class="noindent" >

<!--l. 912--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.107: </span><span  
class="content">100 day NAVB Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-76006107 -->
</div>
<!--l. 917--><p class="noindent" >HRG scored a mean absolute error regression loss of 1.415, and a coefficient of determination
of 0.464.
<div class="center" 
>
<!--l. 919--><p class="noindent" >

                                                                                
                                                                                
<!--l. 920--><p class="noindent" ><img 
src="Figures/HRG-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.108: </span><span  
class="content">HRG Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-76007108 -->
</div>
<div class="center" 
>
<!--l. 925--><p class="noindent" >

<!--l. 926--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.109: </span><span  
class="content">100 day HRG Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-76008109 -->
</div>
<!--l. 931--><p class="noindent" >HL scored a mean absolute error regression loss of 0.506, and a coefficient of determination of
0.923.
<div class="center" 
>
<!--l. 933--><p class="noindent" >

<!--l. 934--><p class="noindent" ><img 
src="Figures/HL-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.110: </span><span  
class="content">HL Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-76009110 -->
</div>
<div class="center" 
>
<!--l. 939--><p class="noindent" >

<!--l. 940--><p class="noindent" ><img 
src="Figures/100-Day-HL-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.111: </span><span  
class="content">100 day HL Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-76010111 -->
</div>
                                                                                
                                                                                
<!--l. 945--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.2 </span> <a 
 id="x1-770004.2.2.2"></a>Boosted Decision Tree</h5>
<!--l. 946--><p class="noindent" >MSFT scored a mean absolute error regression loss of 4.426, and a coefficient of determination
of 0.580.
<div class="center" 
>
<!--l. 948--><p class="noindent" >

<!--l. 949--><p class="noindent" ><img 
src="Figures/MSFT-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.112: </span><span  
class="content">MSFT Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77001112 -->
</div>
<div class="center" 
>
<!--l. 954--><p class="noindent" >

<!--l. 955--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.113: </span><span  
class="content">100 day MSFT Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77002113 -->
</div>
<!--l. 960--><p class="noindent" >CDE scored a mean absolute error regression loss of 5.452, and a coefficient of determination
of 0.581.
<div class="center" 
>
<!--l. 962--><p class="noindent" >

<!--l. 963--><p class="noindent" ><img 
src="Figures/CDE-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.114: </span><span  
class="content">CDE Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77003114 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 968--><p class="noindent" >

<!--l. 969--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.115: </span><span  
class="content">100 day CDE Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77004115 -->
</div>
<!--l. 974--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.579, and a coefficient of determination
of 0.509.
<div class="center" 
>
<!--l. 976--><p class="noindent" >

<!--l. 977--><p class="noindent" ><img 
src="Figures/NAVB-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.116: </span><span  
class="content">NAVB Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77005116 -->
</div>
<div class="center" 
>
<!--l. 982--><p class="noindent" >

<!--l. 983--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.117: </span><span  
class="content">100 day NAVB Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77006117 -->
</div>
<!--l. 988--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.980, and a coefficient of determination
of 0.855.
<div class="center" 
>
<!--l. 990--><p class="noindent" >

                                                                                
                                                                                
<!--l. 991--><p class="noindent" ><img 
src="Figures/HRG-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.118: </span><span  
class="content">HRG Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77007118 -->
</div>
<div class="center" 
>
<!--l. 996--><p class="noindent" >

<!--l. 997--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.119: </span><span  
class="content">100 day HRG Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77008119 -->
</div>
<!--l. 1002--><p class="noindent" >HL scored a mean absolute error regression loss of 0.410, and a coefficient of determination of
0.952.
<div class="center" 
>
<!--l. 1004--><p class="noindent" >

<!--l. 1005--><p class="noindent" ><img 
src="Figures/HL-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.120: </span><span  
class="content">HL Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77009120 -->
</div>
<div class="center" 
>
<!--l. 1010--><p class="noindent" >

<!--l. 1011--><p class="noindent" ><img 
src="Figures/100-Day-HL-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.121: </span><span  
class="content">100 day HL Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77010121 -->
</div>
                                                                                
                                                                                
<!--l. 1016--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.3 </span> <a 
 id="x1-780004.2.2.3"></a>K-Nearest Neighbour</h5>
<!--l. 1017--><p class="noindent" >MSFT scored a mean absolute error regression loss of 4.426, and a coefficient of determination
of 0.580.
<div class="center" 
>
<!--l. 1019--><p class="noindent" >

<!--l. 1020--><p class="noindent" ><img 
src="Figures/MSFT-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.122: </span><span  
class="content">MSFT K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-78001122 -->
</div>
<div class="center" 
>
<!--l. 1025--><p class="noindent" >

<!--l. 1026--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.123: </span><span  
class="content">100 day MSFT K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-78002123 -->
</div>
<!--l. 1031--><p class="noindent" >CDE scored a mean absolute error regression loss of 5.452, and a coefficient of determination
of 0.581.
<div class="center" 
>
<!--l. 1033--><p class="noindent" >

<!--l. 1034--><p class="noindent" ><img 
src="Figures/CDE-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.124: </span><span  
class="content">CDE K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-78003124 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1039--><p class="noindent" >

<!--l. 1040--><p class="noindent" ><img 
src="Figures/100-Day-CDE-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.125: </span><span  
class="content">100 day CDE K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-78004125 -->
</div>
<!--l. 1045--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.579, and a coefficient of determination
of 0.509.
<div class="center" 
>
<!--l. 1047--><p class="noindent" >

<!--l. 1048--><p class="noindent" ><img 
src="Figures/NAVB-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.126: </span><span  
class="content">NAVB K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-78005126 -->
</div>
<div class="center" 
>
<!--l. 1053--><p class="noindent" >

<!--l. 1054--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.127: </span><span  
class="content">100 day NAVB K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-78006127 -->
</div>
<!--l. 1059--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.980, and a coefficient of determination
of 0.855.
<div class="center" 
>
<!--l. 1061--><p class="noindent" >

                                                                                
                                                                                
<!--l. 1062--><p class="noindent" ><img 
src="Figures/HRG-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.128: </span><span  
class="content">HRG K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-78007128 -->
</div>
<div class="center" 
>
<!--l. 1067--><p class="noindent" >

<!--l. 1068--><p class="noindent" ><img 
src="Figures/100-Day-HRG-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.129: </span><span  
class="content">100 day HRG K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-78008129 -->
</div>
<!--l. 1073--><p class="noindent" >HL scored a mean absolute error regression loss of 0.410, and a coefficient of determination of
0.952.
<div class="center" 
>
<!--l. 1075--><p class="noindent" >

<!--l. 1076--><p class="noindent" ><img 
src="Figures/HL-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.130: </span><span  
class="content">HL K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-78009130 -->
</div>
<div class="center" 
>
<!--l. 1081--><p class="noindent" >

<!--l. 1082--><p class="noindent" ><img 
src="Figures/100-Day-HL-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.131: </span><span  
class="content">100 day HL K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-78010131 -->
</div>
                                                                                
                                                                                
<!--l. 1087--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.4 </span> <a 
 id="x1-790004.2.2.4"></a>Random Forest</h5>
<!--l. 1088--><p class="noindent" >MSFT scored a mean absolute error regression loss of 5.199, and a coefficient of determination
of 0.483.
<div class="center" 
>
<!--l. 1090--><p class="noindent" >

<!--l. 1091--><p class="noindent" ><img 
src="Figures/MSFT-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.132: </span><span  
class="content">MSFT Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-79001132 -->
</div>
<div class="center" 
>
<!--l. 1096--><p class="noindent" >

<!--l. 1097--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.133: </span><span  
class="content">100 day MSFT Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-79002133 -->
</div>
<!--l. 1102--><p class="noindent" >CDE scored a mean absolute error regression loss of 2.505, and a coefficient of determination
of 0.861.
<div class="center" 
>
<!--l. 1104--><p class="noindent" >

<!--l. 1105--><p class="noindent" ><img 
src="Figures/CDE-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.134: </span><span  
class="content">CDE Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-79003134 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1110--><p class="noindent" >

<!--l. 1111--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.135: </span><span  
class="content">100 day CDE Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-79004135 -->
</div>
<!--l. 1116--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.293, and a coefficient of determination
of 0.856.
<div class="center" 
>
<!--l. 1118--><p class="noindent" >

<!--l. 1119--><p class="noindent" ><img 
src="Figures/NAVB-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.136: </span><span  
class="content">NAVB Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-79005136 -->
</div>
<div class="center" 
>
<!--l. 1124--><p class="noindent" >

<!--l. 1125--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.137: </span><span  
class="content">100 day NAVB Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-79006137 -->
</div>
<!--l. 1130--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.675, and a coefficient of determination
of 0.907.
<div class="center" 
>
<!--l. 1132--><p class="noindent" >

                                                                                
                                                                                
<!--l. 1133--><p class="noindent" ><img 
src="Figures/HRG-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.138: </span><span  
class="content">HRG Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-79007138 -->
</div>
<div class="center" 
>
<!--l. 1138--><p class="noindent" >

<!--l. 1139--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.139: </span><span  
class="content">100 day HRG Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-79008139 -->
</div>
<!--l. 1144--><p class="noindent" >HL scored a mean absolute error regression loss of 0.434, and a coefficient of determination of
0.922.
<div class="center" 
>
<!--l. 1146--><p class="noindent" >

<!--l. 1147--><p class="noindent" ><img 
src="Figures/HL-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.140: </span><span  
class="content">HL Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-79009140 -->
</div>
<div class="center" 
>
<!--l. 1152--><p class="noindent" >

<!--l. 1153--><p class="noindent" ><img 
src="Figures/100-Day-HL-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.141: </span><span  
class="content">100 day HL Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-79010141 -->
</div>
                                                                                
                                                                                
<!--l. 1158--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.5 </span> <a 
 id="x1-800004.2.2.5"></a>Linear Regression</h5>
<!--l. 1159--><p class="noindent" >MSFT scored a mean absolute error regression loss of 0.844, and a coefficient of determination
of 0.990.
<div class="center" 
>
<!--l. 1161--><p class="noindent" >

<!--l. 1162--><p class="noindent" ><img 
src="Figures/MSFT-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.142: </span><span  
class="content">MSFT Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-80001142 -->
</div>
<div class="center" 
>
<!--l. 1167--><p class="noindent" >

<!--l. 1168--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.143: </span><span  
class="content">100 day MSFT Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-80002143 -->
</div>
<!--l. 1173--><p class="noindent" >CDE scored a mean absolute error regression loss of 0.990, and a coefficient of determination
of 0.972.
<div class="center" 
>
<!--l. 1175--><p class="noindent" >

<!--l. 1176--><p class="noindent" ><img 
src="Figures/CDE-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.144: </span><span  
class="content">CDE Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-80003144 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1181--><p class="noindent" >

<!--l. 1182--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.145: </span><span  
class="content">100 day CDE Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-80004145 -->
</div>
<!--l. 1187--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.128, and a coefficient of determination
of 0.962.
<!--l. 1189--><p class="noindent" ><img 
src="Figures/NAVB-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<!--l. 1191--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<div class="center" 
>
<!--l. 1193--><p class="noindent" >

<!--l. 1194--><p class="noindent" ><img 
src="Figures/CDE-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.146: </span><span  
class="content">CDE Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-80005146 -->
</div>
<div class="center" 
>
<!--l. 1199--><p class="noindent" >

<!--l. 1200--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.147: </span><span  
class="content">100 day CDE Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-80006147 -->
</div>
<!--l. 1205--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.324, and a coefficient of determination
of 0.984.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1207--><p class="noindent" >

<!--l. 1208--><p class="noindent" ><img 
src="Figures/HRG-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.148: </span><span  
class="content">HRG Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-80007148 -->
</div>
<div class="center" 
>
<!--l. 1213--><p class="noindent" >

<!--l. 1214--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.149: </span><span  
class="content">100 day HRG Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-80008149 -->
</div>
<!--l. 1219--><p class="noindent" >HL scored a mean absolute error regression loss of 0.243, and a coefficient of determination of
0.964.
<div class="center" 
>
<!--l. 1221--><p class="noindent" >

<!--l. 1222--><p class="noindent" ><img 
src="Figures/HL-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.150: </span><span  
class="content">HL Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-80009150 -->
</div>
<div class="center" 
>
<!--l. 1227--><p class="noindent" >

<!--l. 1228--><p class="noindent" ><img 
src="Figures/100-Day-HL-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.151: </span><span  
class="content">100 day HL Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-80010151 -->
</div>
                                                                                
                                                                                
<!--l. 1233--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.6 </span> <a 
 id="x1-810004.2.2.6"></a>Neural Network</h5>
<!--l. 1234--><p class="noindent" >MSFT scored a mean absolute error regression loss of 1.073, and a coefficient of determination
of 0.992.
<div class="center" 
>
<!--l. 1236--><p class="noindent" >

<!--l. 1237--><p class="noindent" ><img 
src="Figures/MSFT-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.152: </span><span  
class="content">MSFT Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-81001152 -->
</div>
<div class="center" 
>
<!--l. 1242--><p class="noindent" >

<!--l. 1243--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.153: </span><span  
class="content">100 day MSFT Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-81002153 -->
</div>
<!--l. 1248--><p class="noindent" >CDE scored a mean absolute error regression loss of 2.906, and a coefficient of determination
of 0.816.
<!--l. 1250--><p class="noindent" ><img 
src="Figures/CDE-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<!--l. 1252--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<div class="center" 
>
<!--l. 1254--><p class="noindent" >

                                                                                
                                                                                
<!--l. 1255--><p class="noindent" ><img 
src="Figures/CDE-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.154: </span><span  
class="content">CDE Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-81003154 -->
</div>
<div class="center" 
>
<!--l. 1260--><p class="noindent" >

<!--l. 1261--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.155: </span><span  
class="content">100 day CDE Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-81004155 -->
</div>
<!--l. 1266--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.422, and a coefficient of determination
of 0.693.
<div class="center" 
>
<!--l. 1268--><p class="noindent" >

<!--l. 1269--><p class="noindent" ><img 
src="Figures/NAVB-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.156: </span><span  
class="content">NAVB Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-81005156 -->
</div>
<div class="center" 
>
<!--l. 1274--><p class="noindent" >

<!--l. 1275--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.157: </span><span  
class="content">100 day NAVB Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-81006157 -->
</div>
<!--l. 1280--><p class="noindent" >HRG scored a mean absolute error regression loss of 1.415, and a coefficient of determination
of 0.464.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1282--><p class="noindent" >

<!--l. 1283--><p class="noindent" ><img 
src="Figures/HRG-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.158: </span><span  
class="content">HRG Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-81007158 -->
</div>
<div class="center" 
>
<!--l. 1288--><p class="noindent" >

<!--l. 1289--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.159: </span><span  
class="content">100 day HRG Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-81008159 -->
</div>
<!--l. 1294--><p class="noindent" >HL scored a mean absolute error regression loss of 0.506, and a coefficient of determination of
0.923.
<div class="center" 
>
<!--l. 1296--><p class="noindent" >

<!--l. 1297--><p class="noindent" ><img 
src="Figures/HL-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.160: </span><span  
class="content">HL Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-81009160 -->
</div>
<div class="center" 
>
<!--l. 1302--><p class="noindent" >

<!--l. 1303--><p class="noindent" ><img 
src="Figures/100-Day-HL-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.161: </span><span  
class="content">100 day HL Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-81010161 -->
</div>
                                                                                
                                                                                
<!--l. 1308--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.7 </span> <a 
 id="x1-820004.2.2.7"></a>Stochastic Gradient Descent</h5>
<!--l. 1309--><p class="noindent" >MSFT scored a mean absolute error regression loss of 0.829, and a coefficient of determination
of 0.990.
<div class="center" 
>
<!--l. 1311--><p class="noindent" >

<!--l. 1312--><p class="noindent" ><img 
src="Figures/MSFT-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.162: </span><span  
class="content">MSFT SGD in-sample prediction</span></div><!--tex4ht:label?: x1-82001162 -->
</div>
<div class="center" 
>
<!--l. 1317--><p class="noindent" >

<!--l. 1318--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.163: </span><span  
class="content">100 day MSFT SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-82002163 -->
</div>
<!--l. 1323--><p class="noindent" >CDE scored a mean absolute error regression loss of 4.724, and a coefficient of determination
of 0.788.
<div class="center" 
>
<!--l. 1325--><p class="noindent" >

<!--l. 1326--><p class="noindent" ><img 
src="Figures/CDE-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.164: </span><span  
class="content">CDE SGD in-sample prediction</span></div><!--tex4ht:label?: x1-82003164 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1331--><p class="noindent" >

<!--l. 1332--><p class="noindent" ><img 
src="Figures/100-Day-CDE-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.165: </span><span  
class="content">100 day CDE SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-82004165 -->
</div>
<!--l. 1337--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.142, and a coefficient of determination
of 0.955.
<div class="center" 
>
<!--l. 1339--><p class="noindent" >

<!--l. 1340--><p class="noindent" ><img 
src="Figures/NAVB-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.166: </span><span  
class="content">NAVB SGD in-sample prediction</span></div><!--tex4ht:label?: x1-82005166 -->
</div>
<div class="center" 
>
<!--l. 1345--><p class="noindent" >

<!--l. 1346--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.167: </span><span  
class="content">100 day NAVB SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-82006167 -->
</div>
<!--l. 1351--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.292, and a coefficient of determination
of 0.987.
<div class="center" 
>
                                                                                
                                                                                
<!--l. 1353--><p class="noindent" >

<!--l. 1354--><p class="noindent" ><img 
src="Figures/HRG-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.168: </span><span  
class="content">HRG SGD in-sample prediction</span></div><!--tex4ht:label?: x1-82007168 -->
</div>
<div class="center" 
>
<!--l. 1359--><p class="noindent" >

<!--l. 1360--><p class="noindent" ><img 
src="Figures/100-Day-HRG-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.169: </span><span  
class="content">100 day HRG SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-82008169 -->
</div>
<!--l. 1365--><p class="noindent" >HL scored a mean absolute error regression loss of 0.275, and a coefficient of determination of
0.962.
<div class="center" 
>
<!--l. 1367--><p class="noindent" >

<!--l. 1368--><p class="noindent" ><img 
src="Figures/HL-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.170: </span><span  
class="content">HL SGD in-sample prediction</span></div><!--tex4ht:label?: x1-82009170 -->
</div>
<div class="center" 
>
<!--l. 1373--><p class="noindent" >

<!--l. 1374--><p class="noindent" ><img 
src="Figures/100-Day-HL-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.171: </span><span  
class="content">100 day HL SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-82010171 -->
</div>
                                                                                
                                                                                
<!--l. 1379--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">4.3 </span> <a 
 id="x1-830004.3"></a>Bayesian Statistics</h3>
<!--l. 1381--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.3.1 </span> <a 
 id="x1-840004.3.1"></a>Metropolis-Hastings</h4>
<!--l. 1382--><p class="noindent" >MSFT scored sharpe ratios of 0.439 for the original returns, and -4.796 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 1384--><p class="noindent" >

<!--l. 1385--><p class="noindent" ><img 
src="Figures/MSFT-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.172: </span><span  
class="content">MSFT Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-84001172 -->
</div>
<div class="center" 
>
<!--l. 1390--><p class="noindent" >

<!--l. 1391--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.173: </span><span  
class="content">100 day MSFT Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-84002173 -->
</div>
<!--l. 1396--><p class="noindent" >CDE scored sharpe ratios of 2.028 for the original returns, and 4.075 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 1398--><p class="noindent" >

                                                                                
                                                                                
<!--l. 1399--><p class="noindent" ><img 
src="Figures/CDE-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.174: </span><span  
class="content">CDE Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-84003174 -->
</div>
<div class="center" 
>
<!--l. 1404--><p class="noindent" >

<!--l. 1405--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.175: </span><span  
class="content">100 day CDE Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-84004175 -->
</div>
<!--l. 1410--><p class="noindent" >NAVB scored sharpe ratios of -2.800 for the original returns, and 2.998 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 1412--><p class="noindent" >

<!--l. 1413--><p class="noindent" ><img 
src="Figures/NAVB-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.176: </span><span  
class="content">NAVB Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-84005176 -->
</div>
<div class="center" 
>
<!--l. 1418--><p class="noindent" >

<!--l. 1419--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.177: </span><span  
class="content">100 day NAVB Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-84006177 -->
</div>
<!--l. 1424--><p class="noindent" >HRG scored sharpe ratios of 1.362 for the original returns, and -2.173 for the predicted returns
in the in-sample test.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1426--><p class="noindent" >

<!--l. 1427--><p class="noindent" ><img 
src="Figures/HRG-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.178: </span><span  
class="content">HRG Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-84007178 -->
</div>
<div class="center" 
>
<!--l. 1432--><p class="noindent" >

<!--l. 1433--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.179: </span><span  
class="content">100 day HRG Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-84008179 -->
</div>
<!--l. 1438--><p class="noindent" >HL scored sharpe ratios of 0.439 for the original returns, and -2.283 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 1440--><p class="noindent" >

<!--l. 1441--><p class="noindent" ><img 
src="Figures/HL-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.180: </span><span  
class="content">HL Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-84009180 -->
</div>
<div class="center" 
>
<!--l. 1446--><p class="noindent" >

<!--l. 1447--><p class="noindent" ><img 
src="Figures/100-Day-HL-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.181: </span><span  
class="content">100 day HL Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-84010181 -->
</div>
                                                                                
                                                                                
<!--l. 1452--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.3.2 </span> <a 
 id="x1-850004.3.2"></a>No-U-Turn Sampler (NUTS)</h4>
<!--l. 1453--><p class="noindent" >MSFT scored sharpe ratios of 2.499 for the original returns, and -1.355 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 1455--><p class="noindent" >

<!--l. 1456--><p class="noindent" ><img 
src="Figures/MSFT-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.182: </span><span  
class="content">MSFT NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-85001182 -->
</div>
<div class="center" 
>
<!--l. 1461--><p class="noindent" >

<!--l. 1462--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.183: </span><span  
class="content">100 day MSFT NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-85002183 -->
</div>
<!--l. 1467--><p class="noindent" >CDE scored sharpe ratios of 2.028 for the original returns, and 3.066 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 1469--><p class="noindent" >

<!--l. 1470--><p class="noindent" ><img 
src="Figures/CDE-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.184: </span><span  
class="content">CDE NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-85003184 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1475--><p class="noindent" >

<!--l. 1476--><p class="noindent" ><img 
src="Figures/100-Day-CDE-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.185: </span><span  
class="content">100 day CDE NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-85004185 -->
</div>
<!--l. 1481--><p class="noindent" >NAVB scored sharpe ratios of -2.800 for the original returns, and -2.914 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 1483--><p class="noindent" >

<!--l. 1484--><p class="noindent" ><img 
src="Figures/NAVB-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.186: </span><span  
class="content">NAVB NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-85005186 -->
</div>
<div class="center" 
>
<!--l. 1489--><p class="noindent" >

<!--l. 1490--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.187: </span><span  
class="content">100 day NAVB NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-85006187 -->
</div>
<!--l. 1495--><p class="noindent" >HRG scored sharpe ratios of 1.362 for the original returns, and -1.812 for the predicted returns
in the in-sample test.
<div class="center" 
>
                                                                                
                                                                                
<!--l. 1497--><p class="noindent" >

<!--l. 1498--><p class="noindent" ><img 
src="Figures/HRG-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.188: </span><span  
class="content">HRG NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-85007188 -->
</div>
<div class="center" 
>
<!--l. 1503--><p class="noindent" >

<!--l. 1504--><p class="noindent" ><img 
src="Figures/100-Day-HRG-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.189: </span><span  
class="content">100 day HRG NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-85008189 -->
</div>
<!--l. 1509--><p class="noindent" >HL scored sharpe ratios of 0.506 for the original returns, and 0.923 for the predicted returns in
the in-sample test.
<div class="center" 
>
<!--l. 1511--><p class="noindent" >

<!--l. 1512--><p class="noindent" ><img 
src="Figures/HL-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.190: </span><span  
class="content">HL NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-85009190 -->
</div>
<div class="center" 
>
<!--l. 1517--><p class="noindent" >

<!--l. 1518--><p class="noindent" ><img 
src="Figures/100-Day-HL-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.191: </span><span  
class="content">100 day HL NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-85010191 -->
</div>
                                                                                
                                                                                
<!--l. 1523--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">4.4 </span> <a 
 id="x1-860004.4"></a>Strategy</h3>
<!--l. 1525--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.4.1 </span> <a 
 id="x1-870004.4.1"></a>Classification</h4>
<div class="center" 
>
<!--l. 1527--><p class="noindent" >
<div class="tabular"><table id="TBL-13" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-13-1g"><col 
id="TBL-13-1"></colgroup><colgroup id="TBL-13-2g"><col 
id="TBL-13-2"></colgroup><colgroup id="TBL-13-3g"><col 
id="TBL-13-3"></colgroup><colgroup id="TBL-13-4g"><col 
id="TBL-13-4"></colgroup><colgroup id="TBL-13-5g"><col 
id="TBL-13-5"></colgroup><colgroup id="TBL-13-6g"><col 
id="TBL-13-6"></colgroup><colgroup id="TBL-13-7g"><col 
id="TBL-13-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-1-1"  
class="td11">Starting Capital            </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-1-2"  
class="td11">$100,000     </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-2-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-2-2"  
class="td11">$103,110.65 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-3-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-3-2"  
class="td11">0.346          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-4-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-4-2"  
class="td11">$149,126.306</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-5-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-5-2"  
class="td11">0.491          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-6-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-6-2"  
class="td11">1.008          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-7-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-7-2"  
class="td11">0.272          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-8-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-8-2"  
class="td11">0.156          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-9-1"  
class="td11">                      </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.12: </span><span  
class="content">Machine Learning Classifier strategy with only upwards forecasts</span></div><!--tex4ht:label?: x1-8700112 -->
</div>
<div class="center" 
>
<!--l. 1545--><p class="noindent" >

<!--l. 1546--><p class="noindent" ><img 
src="Figures/MLC-Portfolio-Benchmark-Up.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.192: </span><span  
class="content">Machine Learning Classifier strategy with only upwards forecasts</span></div><!--tex4ht:label?: x1-87002192 -->
</div>
<div class="center" 
>
<!--l. 1551--><p class="noindent" >
                                                                                
                                                                                
<div class="tabular"><table id="TBL-14" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-14-1g"><col 
id="TBL-14-1"></colgroup><colgroup id="TBL-14-2g"><col 
id="TBL-14-2"></colgroup><colgroup id="TBL-14-3g"><col 
id="TBL-14-3"></colgroup><colgroup id="TBL-14-4g"><col 
id="TBL-14-4"></colgroup><colgroup id="TBL-14-5g"><col 
id="TBL-14-5"></colgroup><colgroup id="TBL-14-6g"><col 
id="TBL-14-6"></colgroup><colgroup id="TBL-14-7g"><col 
id="TBL-14-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-1-1"  
class="td11">Starting Capital            </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-1-2"  
class="td11">$100,000    </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-2-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-2-2"  
class="td11">$76,853.53 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-3-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-3-2"  
class="td11">-0.548       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-4-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-4-2"  
class="td11">$20,134.519</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-5-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-5-2"  
class="td11">-0.799       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-6-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-6-2"  
class="td11">1.008        </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-7-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-7-2"  
class="td11">0.323        </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-8-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-8-2"  
class="td11">0.156        </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-9-1"  
class="td11">                      </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.13: </span><span  
class="content">Machine Learning Classifier strategy with upwarda and downwards forecasts</span></div><!--tex4ht:label?: x1-8700313 -->
</div>
<div class="center" 
>
<!--l. 1568--><p class="noindent" >

<!--l. 1569--><p class="noindent" ><img 
src="Figures/MLC-Portfolio-Benchmark-Up-and-Down.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.193:  </span><span  
class="content">Machine  Learning  Classifier  strategy  with  upwards  and  downwards
forecasts</span></div><!--tex4ht:label?: x1-87004193 -->
</div>
<div class="center" 
>
<!--l. 1574--><p class="noindent" >
<div class="tabular"><table id="TBL-15" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-15-1g"><col 
id="TBL-15-1"></colgroup><colgroup id="TBL-15-2g"><col 
id="TBL-15-2"></colgroup><colgroup id="TBL-15-3g"><col 
id="TBL-15-3"></colgroup><colgroup id="TBL-15-4g"><col 
id="TBL-15-4"></colgroup><colgroup id="TBL-15-5g"><col 
id="TBL-15-5"></colgroup><colgroup id="TBL-15-6g"><col 
id="TBL-15-6"></colgroup><colgroup id="TBL-15-7g"><col 
id="TBL-15-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-1-1"  
class="td11">Starting Capital            </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-1-2"  
class="td11">$100,000     </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-2-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-2-2"  
class="td11">$295,844.66 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-3-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-3-2"  
class="td11">0.385          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-4-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-4-2"  
class="td11">$167,474.009</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-5-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-5-2"  
class="td11">0.675          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-6-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-6-2"  
class="td11">1.008          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-7-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-7-2"  
class="td11">0.393          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-8-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-8-2"  
class="td11">0.156          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-9-1"  
class="td11">                      </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.14: </span><span  
class="content">Machine Learning Classifier strategy with stop loss</span></div><!--tex4ht:label?: x1-8700514 -->
</div>
<div class="center" 
>
<!--l. 1591--><p class="noindent" >

                                                                                
                                                                                
<!--l. 1592--><p class="noindent" ><img 
src="Figures/MLC-Portfolio-Benchmark-Stop-Loss.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.194: </span><span  
class="content">Machine Learning Classifier strategy with stop loss</span></div><!--tex4ht:label?: x1-87006194 -->
</div>
<!--l. 1597--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.4.2 </span> <a 
 id="x1-880004.4.2"></a>Regression</h4>
<div class="center" 
>
<!--l. 1599--><p class="noindent" >
<div class="tabular"><table id="TBL-16" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-16-1g"><col 
id="TBL-16-1"></colgroup><colgroup id="TBL-16-2g"><col 
id="TBL-16-2"></colgroup><colgroup id="TBL-16-3g"><col 
id="TBL-16-3"></colgroup><colgroup id="TBL-16-4g"><col 
id="TBL-16-4"></colgroup><colgroup id="TBL-16-5g"><col 
id="TBL-16-5"></colgroup><colgroup id="TBL-16-6g"><col 
id="TBL-16-6"></colgroup><colgroup id="TBL-16-7g"><col 
id="TBL-16-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-16-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-16-1-1"  
class="td11">Starting Capital            </td><td  style="white-space:nowrap; text-align:left;" id="TBL-16-1-2"  
class="td11">$100,000     </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-16-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-16-2-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-16-2-2"  
class="td11">$229,547.84 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-16-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-16-3-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-16-3-2"  
class="td11">0.420          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-16-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-16-4-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-16-4-2"  
class="td11">$183,714.616</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-16-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-16-5-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-16-5-2"  
class="td11">0.837          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-16-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-16-6-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-16-6-2"  
class="td11">1.008          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-16-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-16-7-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-16-7-2"  
class="td11">0.373          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-16-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-16-8-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-16-8-2"  
class="td11">0.156          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-16-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-16-9-1"  
class="td11">                      </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.15: </span><span  
class="content">Machine Learning Regression strategy with only upwards forecasts</span></div><!--tex4ht:label?: x1-8800115 -->
</div>
<div class="center" 
>
<!--l. 1616--><p class="noindent" >

<!--l. 1617--><p class="noindent" ><img 
src="Figures/MLR-Portfolio-Benchmark-Up.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.195: </span><span  
class="content">Machine Learning Regression strategy with only upwards forecasts</span></div><!--tex4ht:label?: x1-88002195 -->
</div>
<div class="center" 
>
<!--l. 1622--><p class="noindent" >
                                                                                
                                                                                
<div class="tabular"><table id="TBL-17" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-17-1g"><col 
id="TBL-17-1"></colgroup><colgroup id="TBL-17-2g"><col 
id="TBL-17-2"></colgroup><colgroup id="TBL-17-3g"><col 
id="TBL-17-3"></colgroup><colgroup id="TBL-17-4g"><col 
id="TBL-17-4"></colgroup><colgroup id="TBL-17-5g"><col 
id="TBL-17-5"></colgroup><colgroup id="TBL-17-6g"><col 
id="TBL-17-6"></colgroup><colgroup id="TBL-17-7g"><col 
id="TBL-17-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-17-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-17-1-1"  
class="td11">Starting Capital            </td><td  style="white-space:nowrap; text-align:left;" id="TBL-17-1-2"  
class="td11">$100,000  </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-17-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-17-2-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-17-2-2"  
class="td11">$80,121.51</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-17-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-17-3-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-17-3-2"  
class="td11">-0.258      </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-17-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-17-4-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-17-4-2"  
class="td11">$19,628.34</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-17-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-17-5-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-17-5-2"  
class="td11">-0.804      </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-17-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-17-6-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-17-6-2"  
class="td11">1.008       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-17-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-17-7-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-17-7-2"  
class="td11">0.493       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-17-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-17-8-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-17-8-2"  
class="td11">0.156       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-17-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-17-9-1"  
class="td11">                      </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.16: </span><span  
class="content">Machine  Learning  Regression  strategy  with  upwards  and  downwards
forecasts</span></div><!--tex4ht:label?: x1-8800316 -->
</div>
<div class="center" 
>
<!--l. 1639--><p class="noindent" >

<!--l. 1640--><p class="noindent" ><img 
src="Figures/MLR-Portfolio-Benchmark-Up-and-Down.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.196:  </span><span  
class="content">Machine  Learning  Regression  strategy  with  upwards  and  downwards
forecasts</span></div><!--tex4ht:label?: x1-88004196 -->
</div>
<div class="center" 
>
<!--l. 1645--><p class="noindent" >
<div class="tabular"><table id="TBL-18" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-18-1g"><col 
id="TBL-18-1"></colgroup><colgroup id="TBL-18-2g"><col 
id="TBL-18-2"></colgroup><colgroup id="TBL-18-3g"><col 
id="TBL-18-3"></colgroup><colgroup id="TBL-18-4g"><col 
id="TBL-18-4"></colgroup><colgroup id="TBL-18-5g"><col 
id="TBL-18-5"></colgroup><colgroup id="TBL-18-6g"><col 
id="TBL-18-6"></colgroup><colgroup id="TBL-18-7g"><col 
id="TBL-18-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-18-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-18-1-1"  
class="td11">Starting Capital            </td><td  style="white-space:nowrap; text-align:left;" id="TBL-18-1-2"  
class="td11">$100,000     </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-18-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-18-2-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-18-2-2"  
class="td11">$229,547.84 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-18-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-18-3-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-18-3-2"  
class="td11">0.420          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-18-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-18-4-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-18-4-2"  
class="td11">$183,714.616</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-18-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-18-5-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-18-5-2"  
class="td11">0.837          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-18-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-18-6-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-18-6-2"  
class="td11">1.008          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-18-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-18-7-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-18-7-2"  
class="td11">0.373          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-18-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-18-8-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-18-8-2"  
class="td11">0.156          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-18-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-18-9-1"  
class="td11">                      </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.17: </span><span  
class="content">Machine Learning Regression strategy with stop loss</span></div><!--tex4ht:label?: x1-8800517 -->
</div>
<div class="center" 
>
                                                                                
                                                                                
<!--l. 1662--><p class="noindent" >

<!--l. 1663--><p class="noindent" ><img 
src="Figures/MLR-Portfolio-Benchmark-Stop-Loss.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.197: </span><span  
class="content">Machine Learning Regression strategy with stop loss</span></div><!--tex4ht:label?: x1-88006197 -->
</div>
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;5</span><br /><a 
 id="x1-890005"></a>Discussion and Suggestions for Future Research</h2>
<h3 class="sectionHead"><span class="titlemark">5.1 </span> <a 
 id="x1-900005.1"></a>Time Series Analysis</h3>
<!--l. 5--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.1 </span> <a 
 id="x1-910005.1.1"></a>Random Walk</h4>
<!--l. 6--><p class="noindent" >The random walk theory suggests that stock price changes have the same distribution and are
independent of each other, so the past movement or trend of a stock price or market cannot be
used to predict its future movement. In short, this is the idea that stocks take a random and
unpredictable path.
<!--l. 8--><p class="noindent" >As can be seen in figures 4.4, 4.6, 4.8, and 4.10, the correlation plots show this theory to be
false and that past movement is related to future movement.
<!--l. 10--><p class="noindent" >The histogram of returns show that the stocks follow a normal distribution, where some stocks
showed higher returns than others as can be seen from a wider x-axis.
<!--l. 12--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.2 </span> <a 
 id="x1-920005.1.2"></a>Ordinary Least Squares (OLS)</h4>
<!--l. 13--><p class="noindent" >This algorithm achieved a good fit in the in-sample tests, having a prediction rate above 96%
for all the stocks tested.
<!--l. 15--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.3 </span> <a 
 id="x1-930005.1.3"></a>Auto Regressive (AR)</h4>
<!--l. 16--><p class="noindent" >This algorithm failed to achieve a good fit in the in-sample tests, having a huge difference
in sharpe ratios based on the original price returns and in-sample predicted price
returns. The algorithm failed completely in forecasting price returns in out-of-sample
testing.
                                                                                
                                                                                
<!--l. 18--><p class="noindent" >The histogram of returns further showed that the algorithm did not achieve a good fit, as the
histogram itself was not completely symmetrical.
<!--l. 20--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.4 </span> <a 
 id="x1-940005.1.4"></a>Moving Average (MA)</h4>
<!--l. 21--><p class="noindent" >This algorithm faired better than AR having a lower difference in sharpe ratios based on the
original price returns and in-sample predicted price returns, however still failed to achieve a
good fit in the in-sample tests.
<!--l. 23--><p class="noindent" >Although achieving a better fit, the histogram of returns still showed flaws as they were not
perfectly symmetrical.
<!--l. 25--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.5 </span> <a 
 id="x1-950005.1.5"></a>Auto Regressive Moving Average (ARMA)</h4>
<!--l. 26--><p class="noindent" >This algorithm once again faired better than the previous algorithm, MA, having an even
lower difference in sharpe ratios based on the original price returns and in-sample predicted
price returns, however still failed to achieve a good fit in the in-sample tests. As can be seen in
the time series analysis plots, ARMA showed to have very heavy tails in the QQ and
probability plots. The algorithm also failed to forecast future price returns for certain stocks,
as is evident in figure 4.81.
<!--l. 28--><p class="noindent" >As can be seen from the histogram of returns, a better fit was achieved as the histogram was
more symmetrical than those of the previous algorithms.
<!--l. 30--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.6 </span> <a 
 id="x1-960005.1.6"></a>Auto Regressive Integrated Moving Average (ARIMA)</h4>
<!--l. 31--><p class="noindent" >This algorithm faired worse than the previous algorithm, ARMA, having a higher difference
in sharpe ratios based on the original price returns and in-sample predicted price
returns.
<!--l. 33--><p class="noindent" >This can also be seen from the histogram of returns, in which its distribution was less normal
than that of the previous algorithm.
                                                                                
                                                                                
<!--l. 35--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">5.2 </span> <a 
 id="x1-970005.2"></a>Machine Learning</h3>
<!--l. 37--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.2.1 </span> <a 
 id="x1-980005.2.1"></a>Classification</h4>
<!--l. 39--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.1 </span> <a 
 id="x1-990005.2.1.1"></a>Decision Tree</h5>
<!--l. 40--><p class="noindent" >This algorithm faired well in predicting stock price movements in the in-sample tests, having
an accuracy score above 75% for all the stocks prices that were forecast.
<!--l. 42--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.2 </span> <a 
 id="x1-1000005.2.1.2"></a>Boosted Decision Tree</h5>
<!--l. 43--><p class="noindent" >This algorithm yielded slightly better results when compared to the previous algorithm in
predicting stock price movements in the in-sample tests.
<!--l. 45--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.3 </span> <a 
 id="x1-1010005.2.1.3"></a>Support Vector Machine (SVM)</h5>
<!--l. 46--><p class="noindent" >This algorithm once again yielded slightly better results when compared to the previous
algorithm in predicting stock price movements in the in-sample tests.
                                                                                
                                                                                
<!--l. 48--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.4 </span> <a 
 id="x1-1020005.2.1.4"></a>Random Forest</h5>
<!--l. 49--><p class="noindent" >This algorithm scored the same accuracy as the previous algorithm.
<!--l. 51--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.5 </span> <a 
 id="x1-1030005.2.1.5"></a>K-Nearest Neighbour</h5>
<!--l. 52--><p class="noindent" >This algorithm&#8217;s accuracy was not consistent as it scored mixed results when tested against all
the stocks, where the accuracy is some was much lower than others.
<!--l. 54--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.6 </span> <a 
 id="x1-1040005.2.1.6"></a>Logistic Regression</h5>
<!--l. 55--><p class="noindent" >This algorithm faired worse than the others having not achieved an accuracy score of above
80% when tested against all the stocks.
<!--l. 57--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.7 </span> <a 
 id="x1-1050005.2.1.7"></a>Bernoulli Naive Bayes</h5>
<!--l. 58--><p class="noindent" >This algorithm faired the worst from the alogrithms tested so far, scoring accuracy scores in
the lower 70th percentile.
<!--l. 60--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.8 </span> <a 
 id="x1-1060005.2.1.8"></a>Gaussian Naive Bayes</h5>
<!--l. 61--><p class="noindent" >This algorithm also scored a lower accuracy score, showing that Naive Bayes isn&#8217;t the best
form of forecasting stock prices.
                                                                                
                                                                                
<!--l. 63--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.9 </span> <a 
 id="x1-1070005.2.1.9"></a>Neural Network</h5>
<!--l. 64--><p class="noindent" >This algorithm, although did not score the best accuracy score from the other algorithms, still
faired well in correctly predicting the price movments of the stocks.
<!--l. 66--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.10 </span> <a 
 id="x1-1080005.2.1.10"></a>Stochastic Gradient Descent</h5>
<!--l. 67--><p class="noindent" >This algorithm faired the worst from the lot, varying heavily in accuracy scores, in which
one stock had an accuracy score of 59%. Even though it faired miuch better in
correctly predicting the price movements of other stocks, this algorithm proved to be
unreliable.
<!--l. 69--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.2.2 </span> <a 
 id="x1-1090005.2.2"></a>Regression</h4>
<!--l. 71--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.1 </span> <a 
 id="x1-1100005.2.2.1"></a>Decision Tree</h5>
<!--l. 72--><p class="noindent" >As can be seen in figure 4.102, the algorithm not only failed to achieve a good fit in the
in-sample test, but also failed completely in predicting any values at all. Figure 4.103 also
shoes that the algorithm predicted a sharp fall at the end of the out-of-sample test,
which could be related to the incorrect fit achieved in the in-sample test. This was
reflected in the metric score, having a high number of data losses, and low accuracy
scores.
<!--l. 74--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.2 </span> <a 
 id="x1-1110005.2.2.2"></a>Boosted Decision Tree</h5>
                                                                                
                                                                                
<!--l. 75--><p class="noindent" >This algorithm also did not do very well, as can be seen in figures 4.112, 4.114, 4.116, and
4.118, the algorithm not only failed to achieve a good fit in the in-sample test, but also failed
completely in predicting any values at all. The algorithm also predicted some sharp inclines at
the end of the test, as can be seen in figure 4.115. The bad fit achieved is also reflected in the
metric scores, in which there was a high number of data losses, and low accuracy
scores.
<!--l. 77--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.3 </span> <a 
 id="x1-1120005.2.2.3"></a>K-Nearest Neighbour</h5>
<!--l. 78--><p class="noindent" >This algorithm also didn&#8217;t fair too well, showing bad fits in figures 4.122, 4.124, 4.126, 4.128,
and 4.130. Similarly to the previous algorithm, the alforithm failed completely in predicting
any values at all in some parts of the in-sample test.
<!--l. 80--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.4 </span> <a 
 id="x1-1130005.2.2.4"></a>Random Forest</h5>
<!--l. 81--><p class="noindent" >Similarly to previous algorithms, this algorithm failed to achieve a good fit as can be seen in
figures 4.132, 4.134, 4.136, 4,138, and 4.140. The algorithm also failed to predict any values at
all in the of the in-sample tests, and also predicted sharp inclines and declines at the end some
of the out-of-sample tests.
<!--l. 83--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.5 </span> <a 
 id="x1-1140005.2.2.5"></a>Linear Regression</h5>
<!--l. 84--><p class="noindent" >This algorithm performed very well in the in-sample tests, achieving a good fit with high
accuracy scores above 95% with a low number of data losses.
<!--l. 86--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.6 </span> <a 
 id="x1-1150005.2.2.6"></a>Neural Network</h5>
                                                                                
                                                                                
<!--l. 87--><p class="noindent" >This algorithm, although scored very well in some of the in-sample tests, was not very reliable
as accuracy scores varied between each test, performing extremely well in some, however not
too well in others.
<!--l. 89--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.7 </span> <a 
 id="x1-1160005.2.2.7"></a>Stochastic Gradient Descent</h5>
<!--l. 90--><p class="noindent" >This algorithm performed fairly well in most in-sample tests, having high accuracy scores and
low data losses escept for CDE as can be seen in figure 4.164.
<!--l. 92--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">5.3 </span> <a 
 id="x1-1170005.3"></a>Bayesian Statistics</h3>
<!--l. 94--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.3.0.1 </span> <a 
 id="x1-1180005.3.0.1"></a>Metropolis-Hastings</h5>
<!--l. 95--><p class="noindent" >This algorithm failed to achieve a good fit in the in-sample tests, having a huge difference
in sharpe ratios based on the original price returns and in-sample predicted price
returns.
<!--l. 97--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.3.0.2 </span> <a 
 id="x1-1190005.3.0.2"></a>No-U-Turn-Sampler (NUTS)</h5>
<!--l. 98--><p class="noindent" >The algorithm achieved a good fit in the in-sample tests, having very similar sharpe ratios
based on the original price returns and in-sample predicted price returns.
                                                                                
                                                                                
<!--l. 100--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">5.4 </span> <a 
 id="x1-1200005.4"></a>Strategy</h3>
<!--l. 102--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.4.1 </span> <a 
 id="x1-1210005.4.1"></a>Classification</h4>
<!--l. 103--><p class="noindent" >When tasked with predicting rises in stock prices, the algorithm did fairly well, marking a
total return of 49.1%, this however did not beat the benchmark&#8217;s return of 100.8%. The
agorithm underperformed immensely when tasked at also predicting stock price falls, marking
a negative total return of -80%. To compensate for this, a stop loss was added to sell
all positions if the stock in question falls below -20%, this resulted in a profit of
67.5%.
<!--l. 105--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.4.2 </span> <a 
 id="x1-1220005.4.2"></a>Regression</h4>
<!--l. 106--><p class="noindent" >The performance of this algorithm was extremely similar to that of the previous
one, this goes to show that both classifiers and regressors can be used for stock
price prediction. When tasked with predicting rises in stock prices, the algorithm
did fairly well, marking a slightly lower total return of 43%. The agorithm also
underperformed immensely when tasked at also predicting stock price falls, marking a
negative total return of -80.4%. To compensate for this, a stop loss was added to sell
all positions if the stock in question falls below -20%, this resulted in a profit of
83.7%.
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;6</span><br /><a 
 id="x1-1230006"></a>Conclusion</h2>
<!--l. 3--><p class="noindent" >Three financial forecasting methods were presented in this report, two of which showed little
to no potential of ever producing any statistically significant result when the correct
methodology was applied. The third method, machine learning, showed some potential in the
tests carried out, which is why this method was built into an automated algorithmic strategy
to trade with. The algorithm proved to be successful in forecasting future prices, using both
classification and regression methods. However, the backtesting proved this method to
fail in forecasting price falls. Once this factor was removed from the equation, the
algorithms were very successful and reported a profit by the end of the test. This is
however not always ideal as stocks which could fall in price could be catastrophic to
the strategy. A stop loss would be ideal in insuring that no positions are held in
downward falling stocks. It was also evident that regression methods were more
successful in forecasting future price movements when compared to classification
methods.
<!--l. 5--><p class="noindent" >If there is anything that this report shows, is that profitable stock market prediction is an
extremely tough problem. Even though the strategies reported a profit by the end of the
backtest, they still did not beat the market. Whether it is at all possible to use such methods
to outperform the market&#8217;s returns, ultimately remains an open question. These
findings support the Efficient Market Hypothesis, proving that casual investors are
better off investing in passive buy and hold strategies consisting of index funds and
ETFs. However, there was some evidence found showing that the Random Walk
Hypothesis does not hold true for all cases, as some stocks did show signs of repeating
trends.
                                                                                
                                                                                
<a 
 id="x1-123001r343"></a>
<h2 class="appendixHead"><span class="titlemark">Appendix&#x00A0;A</span><br /><a 
 id="x1-124000A"></a>An Appendix</h2>
<!--l. 3--><p class="noindent" >Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus at pulvinar nisi. Phasellus
hendrerit, diam placerat interdum iaculis, mauris justo cursus risus, in viverra purus eros at
ligula. Ut metus justo, consequat a tristique posuere, laoreet nec nibh. Etiam et
scelerisque mauris. Phasellus vel massa magna. Ut non neque id tortor pharetra
bibendum vitae sit amet nisi. Duis nec quam quam, sed euismod justo. Pellentesque eu
tellus vitae ante tempus malesuada. Nunc accumsan, quam in congue consequat,
lectus lectus dapibus erat, id aliquet urna neque at massa. Nulla facilisi. Morbi
ullamcorper eleifend posuere. Donec libero leo, faucibus nec bibendum at, mattis et urna.
Proin consectetur, nunc ut imperdiet lobortis, magna neque tincidunt lectus, id
iaculis nisi justo id nibh. Pellentesque vel sem in erat vulputate faucibus molestie ut
lorem.
<!--l. 5--><p class="noindent" >Quisque tristique urna in lorem laoreet at laoreet quam congue. Donec dolor turpis, blandit
non imperdiet aliquet, blandit et felis. In lorem nisi, pretium sit amet vestibulum sed, tempus
et sem. Proin non ante turpis. Nulla imperdiet fringilla convallis. Vivamus vel bibendum nisl.
Pellentesque justo lectus, molestie vel luctus sed, lobortis in libero. Nulla facilisi. Aliquam erat
volutpat. Suspendisse vitae nunc nunc. Sed aliquet est suscipit sapien rhoncus non
adipiscing nibh consequat. Aliquam metus urna, faucibus eu vulputate non, luctus eu
justo.
<!--l. 7--><p class="noindent" >Donec urna leo, vulputate vitae porta eu, vehicula blandit libero. Phasellus eget massa et leo
condimentum mollis. Nullam molestie, justo at pellentesque vulputate, sapien velit ornare
diam, nec gravida lacus augue non diam. Integer mattis lacus id libero ultrices sit amet mollis
neque molestie. Integer ut leo eget mi volutpat congue. Vivamus sodales, turpis id venenatis
placerat, tellus purus adipiscing magna, eu aliquam nibh dolor id nibh. Pellentesque habitant
morbi tristique senectus et netus et malesuada fames ac turpis egestas. Sed cursus
convallis quam nec vehicula. Sed vulputate neque eget odio fringilla ac sodales urna
feugiat.
<!--l. 9--><p class="noindent" >Phasellus nisi quam, volutpat non ullamcorper eget, congue fringilla leo. Cras et erat et nibh
placerat commodo id ornare est. Nulla facilisi. Aenean pulvinar scelerisque eros
eget interdum. Nunc pulvinar magna ut felis varius in hendrerit dolor accumsan.
Nunc pellentesque magna quis magna bibendum non laoreet erat tincidunt. Nulla
facilisi.
<!--l. 11--><p class="noindent" >Duis eget massa sem, gravida interdum ipsum. Nulla nunc nisl, hendrerit sit amet commodo
vel, varius id tellus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc ac dolor est.
Suspendisse ultrices tincidunt metus eget accumsan. Nullam facilisis, justo vitae convallis
                                                                                
                                                                                
sollicitudin, eros augue malesuada metus, nec sagittis diam nibh ut sapien. Duis
blandit lectus vitae lorem aliquam nec euismod nisi volutpat. Vestibulum ornare
dictum tortor, at faucibus justo tempor non. Nulla facilisi. Cras non massa nunc,
eget euismod purus. Nunc metus ipsum, euismod a consectetur vel, hendrerit nec
nunc.
                                                                                
                                                                                
<a 
 id="x1-124001r7"></a>
<a 
 id="Q1-1-345"></a>
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-1250007"></a>Bibliography</h2>
<div class="thebibliography">
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMarkowitz:1952aa"></a>[1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Harry Markowitz. Portfolio selection. <span 
class="cmti-10x-x-109">American Finance Association</span>, 1952.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XPeter-J.-Brockwell:2016aa"></a>[2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Peter&#x00A0;J. Brockwell and Richard&#x00A0;A. Davis.  <span 
class="cmti-10x-x-109">Introduction to Time Series and</span>
<span 
class="cmti-10x-x-109">Forecasting</span>. Springer, 2016.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMoskowitz:2011aa"></a>[3]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tobias  Moskowitz,  Yao  Hua&#x00A0;Ooi,  and  Lasse&#x00A0;H.  Pedersen.     Time  series
momentum. <span 
class="cmti-10x-x-109">Chicago Booth Research</span>, September 2011.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XRadha:2015aa"></a>[4]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>S.&#x00A0;Radha  and  M.&#x00A0;Thenmozhi.   Forecasting  short  term  interest  rates  using
arma, arma-garch and arma-egarch models. <span 
class="cmti-10x-x-109">Indian Institute of Capital Markets 9th</span>
<span 
class="cmti-10x-x-109">Capital Markets Conference Paper</span>, January 2006.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XAbrosimova:2002aa"></a>[5]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Natalia Abrosimova, Gishan Dissanaike, and Dirk Linowski. Testing weak-form
efficiency of the russian stock market. In <span 
class="cmti-10x-x-109">EFA 2002 Berlin Meetings</span>, 2002.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XDarrat:2001aa"></a>[6]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ali&#x00A0;F. Darrat and Maosen Zhong.  On testing the random walk hypothesis: A
model-comparison approach, 2001.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XCormen:2009aa"></a>[7]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Thomas&#x00A0;H. Cormen and Chales&#x00A0;E. Leiserson. <span 
class="cmti-10x-x-109">Introduction to Algorithms</span>. The
MIT Press, 2009.
                                                                                
                                                                                
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMitchell:1997aa"></a>[8]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tom&#x00A0;M. Mitchell. <span 
class="cmti-10x-x-109">Machine Learning</span>. McGraw Hill, 1997.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMurugesan:2012aa"></a>[9]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Punniyamoorthy Murugesan and Jose&#x00A0;Joy Thoppan.  Detection of stock price
manipulation using discriminant analysis, June 2012.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XKumar:2016aa"></a>[10]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Manish  Kumar  and  M.&#x00A0;Thenmozhi.   Forecasting  stock  index  movement:  A
comparison of support vector machines and random forest, June 2016.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XKakushadze:2015aa"></a>[11]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zura Kakushadze.     Mean-reversion  and  optimization  mean-reversion  and
optimization. <span 
class="cmti-10x-x-109">Journal of Asset Management</span>, 16(1):14&#8211;40, 2015.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XCreamer:2010aa"></a>[12]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Germn&#x00A0;G. Creamer and Yoav Freund. Automated trading with boosting and
expert weighting. <span 
class="cmti-10x-x-109">Quantitative Finance</span>, Vol. 4(No. 10):pp. 401&#8211;420, April 2010.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XGelman:2014aa"></a>[13]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Andrew Gelman, John&#x00A0;B. Carlin, Hal&#x00A0;S. Stern, David&#x00A0;B. Dunson, Aki Vehtari,
and Donald&#x00A0;B. Rubin. <span 
class="cmti-10x-x-109">Bayesian Data Analysis</span>. CRC Press, 2014.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XSavvides:1994aa"></a>[14]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Savvakis&#x00A0;C.  Savvides.    Risk  analysis  in  investment  appraisal  risk  analysis
in investment appraisal risk analysis in investment appraisal.  <span 
class="cmti-10x-x-109">Project Appraisal</span>
<span 
class="cmti-10x-x-109">Journal</span>, Vol. 9(No. 1), March 1994.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XAndersen:2007aa"></a>[15]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Leif B.&#x00A0;G. Andersen.  Efficient simulation of the heston stochastic volatility
model, 2007.
</p>
                                                                                
                                                                                
<p class="bibitem" ><span class="biblabel">
<a 
 id="XBlanchett:2013aa"></a>[16]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>David Blanchett, Michael&#x00A0;S. Finke, and Wade&#x00A0;D. Pfau.  Asset valuations and
safe portfolio withdrawal rates, 2013.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XGamba:2003aa"></a>[17]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Andrea Gamba. Real options valuation: A monte carlo approach, 2003.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMatthew-D.-Hoffman:2014aa"></a>[18]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Matthew&#x00A0;D. Hoffman and Andrew Gelman. The no-u-turn sampler: Adaptively
setting path lengths in hamiltonian monte carlo.   <span 
class="cmti-10x-x-109">Journal of Machine Learning</span>
<span 
class="cmti-10x-x-109">Research</span>, 15, 2014.
</p>
</div>
 
</body></html> 

                                                                                


