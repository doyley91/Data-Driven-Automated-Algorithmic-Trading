<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Data Driven Automated Algorithmic Trading</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html,htex4ht --> 
<meta name="src" content="Thesis.tex"> 
<meta name="date" content="2017-06-02 20:43:00"> 
<link rel="stylesheet" type="text/css" href="Thesis.css"> 
</head><body 
>
                                                                                
                                                                                
<div class="maketitle">
                                                                                
                                                                                
                                                                                
                                                                                
<a 
href="University" Web Site URL Here (include http://mcast.edu.mt/) ><span 
class="cmbx-12">MCAST</span></a>

<h2 class="titleHead">Data Driven Automated Algorithmic
Trading</h2>
<span 
class="cmr-17">by</span>
<a 
href="gabriel@gaucimaistre.com" ><span 
class="cmr-17">Gabriel Gauci Maistre</span></a>
<span 
class="cmr-12">A thesis submitted in partial fulfillment for the</span>
<span 
class="cmr-12">degree of Bachelor of Science</span>
<span 
class="cmr-12">in the</span>
<a 
href="Faculty" Web Site URL Here (include http://ict.mcast.edu.mt/) ><span 
class="cmr-12">Information and Communications Technology</span></a>
<a 
href="Department" or School Web Site URL Here (include http://mcast.edu.mt/) ><span 
class="cmr-12">Malta College of Art, Science, and Technology</span></a>
<div class="date" ><span 
class="cmr-12x-x-120">June 2017</span></div>
                                                                                
                                                                                
                                                                                
                                                                                
</div>
<a 
 id="x1-2r1"></a>
<a 
 id="Q1-1-1"></a>
<div class="center" 
>
<!--l. 71--><p class="noindent" >
<!--l. 71--><p class="noindent" ><span 
class="cmbx-12x-x-172">Declaration of Authorship</span>
</div>
<!--l. 71--><p class="noindent" >I, Gabriel Gauci Maistre, declare that this thesis titled, &#8216;Data Driven Investing: Advanced
Risk and Portfolio Management in Quantitative Finance&#8217; and the work presented in it are my
own. I confirm that:
<ul class="itemize1">
<li class="itemize">This work was done wholly or mainly while in candidature for a research degree
at this University.
</li>
<li class="itemize">Where any part of this thesis has previously been submitted for a degree or any
other qualification at this University or any other institution, this has been clearly
stated.
</li>
<li class="itemize">Where I  have  consulted  the  published  work  of  others,  this  is  always  clearly
attributed.
</li>
<li class="itemize">Where I have quoted from the work of others, the source is always given. With the
exception of such quotations, this thesis is entirely my own work.
</li>
<li class="itemize">I have acknowledged all main sources of help.
                                                                                
                                                                                
</li>
<li class="itemize">Where the thesis is based on work done by myself jointly with others, I have made
clear exactly what was done by others and what I have contributed myself. <br 
class="newline" /></li></ul>
<!--l. 71--><p class="noindent" >Signed:<br 
class="newline" />____________________________________________________
<!--l. 71--><p class="noindent" >Date:<br 
class="newline" />____________________________________________________
                                                                                
                                                                                
<!--l. 80--><p class="noindent" ><span 
class="cmti-10x-x-109">&#8220;If past history was all there was to the game, the richest people would be librarians.&#8221;</span>
                                                                  <div class="flushright" 
>
<!--l. 82--><p class="noindent" >
Warren Buffett</div>
                                                                                
                                                                                
<a 
 id="x1-3r2"></a>
<a 
 id="Q1-1-2"></a>
<div class="center" 
>
<!--l. 92--><p class="noindent" >
<!--l. 92--><p class="noindent" ><a 
href="University" Web Site URL Here (include http://mcast.edu.mt/) >MCAST</a>
<!--l. 92--><p class="noindent" ><span 
class="cmti-12x-x-172">Abstract</span>
<!--l. 92--><p class="noindent" ><a 
href="Faculty" Web Site URL Here (include http://ict.mcast.edu.mt/) >Information and Communications Technology</a>
<!--l. 92--><p class="noindent" ><a 
href="Department" or School Web Site URL Here (include http://mcast.edu.mt/) >Malta College of Art, Science, and Technology</a>
<!--l. 92--><p class="noindent" >Bachelor of Science
<!--l. 92--><p class="noindent" >by <a 
href="gabriel@gaucimaistre.com" >Gabriel Gauci Maistre</a>
</div>
<!--l. 95--><p class="noindent" >Various existing stock market prediction methods were analysed in this report. Three methods
were applied towards the problem making use of Technical Analysis: Time Series Analysis,
Machine Learning, and Bayesian Statistics. Evidence was found in support of the weak form of
the Efficient Market Hypothesis, that the historic price does not contain useful information
but out of sample data may be predictive. We show that Technical Analysis and Machine
Learning could be used to guide an investors decision. A common flow in technical anylsis
methodology was demonstrated and shown to produce limited useful information. Based on
the findings, automated trading algorithms were developed and simulated using Quantopian
Zipline.
                                                                                
                                                                                
<a 
 id="x1-4r3"></a>
<a 
 id="Q1-1-3"></a>
<div class="center" 
>
<!--l. 118--><p class="noindent" >
<!--l. 118--><p class="noindent" ><span 
class="cmti-12x-x-172">Acknowledgements</span>
</div>
<!--l. 118--><p class="noindent" >I would to express my special thanks of gratitude to my supervisor, Alan Gatt, for the patient
guidance, encouragement, and advice he has provided throughout my time as his
student.
<!--l. 118--><p class="noindent" >I would also like to thank Luke Vella Critien, for guiding me towards the right parth in the
early stages of my research and for recommending Alan Gatt as my tutor.
<!--l. 118--><p class="noindent" >My gratitude is also extended to Emma Galea and Miguel attard for their valuable input
while carrying out my research.
<!--l. 118--><p class="noindent" >Completing this work would have been all the more difficult were it not for the support and
friendship provided by the other members of the Malta College of Arts, Sciences, and
Technology, and the institute of Information and Technology. I am indebted to them for their
help.
<!--l. 118--><p class="noindent" >Finally, I would like to thank my family who have supported me throughout the stressful final
months of the degree course.
                                                                                
                                                                                
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-10003"></a>Contents</h2> <div class="tableofcontents">
<span class="chapterToc" ><a 
href="#Q1-1-1">Declaration of Authorship</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-2">Abstract</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-3">Acknowledgements</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-5">List of Figures</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-7">List of Tables</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-9">Abbreviations</a></span>
<br /><span class="chapterToc" >1 <a 
href="#x1-50001" id="QQ2-1-11">Introduction</a></span>
<br />&#x00A0;<span class="sectionToc" >1.1 <a 
href="#x1-60001.1" id="QQ2-1-12">Efficient Market Hypothesis - EMH</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >1.1.1 <a 
href="#x1-70001.1.1" id="QQ2-1-13">Breaking down EMH</a></span>
<br />&#x00A0;<span class="sectionToc" >1.2 <a 
href="#x1-80001.2" id="QQ2-1-14">Zero-Sum Game</a></span>
<br />&#x00A0;<span class="sectionToc" >1.3 <a 
href="#x1-90001.3" id="QQ2-1-15">Random Walk Hypothesis</a></span>
<br /><span class="chapterToc" >2 <a 
href="#x1-100002" id="QQ2-1-16">Background Theory</a></span>
<br />&#x00A0;<span class="sectionToc" >2.1 <a 
href="#x1-110002.1" id="QQ2-1-17">Time Series Analysis</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.1.1 <a 
href="#x1-120002.1.1" id="QQ2-1-18">Auto Regressive Moving Average (ARMA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.1.2 <a 
href="#x1-130002.1.2" id="QQ2-1-19">Auto Regressive Integrated Moving Average (ARIMA)</a></span>
<br />&#x00A0;<span class="sectionToc" >2.2 <a 
href="#x1-140002.2" id="QQ2-1-20">Statistical Machine Learning</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.2.1 <a 
href="#x1-150002.2.1" id="QQ2-1-21">Classification</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.2.2 <a 
href="#x1-160002.2.2" id="QQ2-1-22">Support Vector Machines</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.2.3 <a 
href="#x1-170002.2.3" id="QQ2-1-23">Regression</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.2.4 <a 
href="#x1-180002.2.4" id="QQ2-1-24">Decision Trees</a></span>
                                                                                
                                                                                
<br />&#x00A0;<span class="sectionToc" >2.3 <a 
href="#x1-190002.3" id="QQ2-1-25">Bayesian Statistics</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >2.3.1 <a 
href="#x1-200002.3.1" id="QQ2-1-26">Markov Chain Monte Carlo (MCMC)</a></span>
<br /><span class="chapterToc" >3 <a 
href="#x1-210003" id="QQ2-1-27">Experminetal Setup</a></span>
<br />&#x00A0;<span class="sectionToc" >3.1 <a 
href="#x1-220003.1" id="QQ2-1-28">Data Tidying</a></span>
<br />&#x00A0;<span class="sectionToc" >3.2 <a 
href="#x1-230003.2" id="QQ2-1-29">Stock Selection</a></span>
<br />&#x00A0;<span class="sectionToc" >3.3 <a 
href="#x1-240003.3" id="QQ2-1-30">Time Series Analysis</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.1 <a 
href="#x1-250003.3.1" id="QQ2-1-31">Random Walk</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.2 <a 
href="#x1-260003.3.2" id="QQ2-1-32">Ordinary Least Squares (OLS)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.3 <a 
href="#x1-270003.3.3" id="QQ2-1-33">Auto Regressive (AR)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.4 <a 
href="#x1-280003.3.4" id="QQ2-1-34">Moving Average (MA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.5 <a 
href="#x1-290003.3.5" id="QQ2-1-35">Auto Regressive Moving Average (ARMA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.3.6 <a 
href="#x1-300003.3.6" id="QQ2-1-36">Auto Regressive Integrated Moving Average (ARIMA)</a></span>
<br />&#x00A0;<span class="sectionToc" >3.4 <a 
href="#x1-310003.4" id="QQ2-1-37">Machine Learning</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.4.1 <a 
href="#x1-320003.4.1" id="QQ2-1-38">Classification</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.1 <a 
href="#x1-330003.4.1.1" id="QQ2-1-39">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.2 <a 
href="#x1-340003.4.1.2" id="QQ2-1-40">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.3 <a 
href="#x1-350003.4.1.3" id="QQ2-1-41">Support Vector Machine (SVM)</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.4 <a 
href="#x1-360003.4.1.4" id="QQ2-1-42">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.5 <a 
href="#x1-370003.4.1.5" id="QQ2-1-43">K-Nearest Neighbour</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.6 <a 
href="#x1-380003.4.1.6" id="QQ2-1-44">Logistic Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.7 <a 
href="#x1-390003.4.1.7" id="QQ2-1-45">Bernoulli Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.8 <a 
href="#x1-400003.4.1.8" id="QQ2-1-46">Gaussian Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.9 <a 
href="#x1-410003.4.1.9" id="QQ2-1-47">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.1.10 <a 
href="#x1-420003.4.1.10" id="QQ2-1-48">Stochastic Gradient Descent</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.4.2 <a 
href="#x1-430003.4.2" id="QQ2-1-49">Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.1 <a 
href="#x1-440003.4.2.1" id="QQ2-1-50">Decision Tree</a></span>
                                                                                
                                                                                
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.2 <a 
href="#x1-450003.4.2.2" id="QQ2-1-51">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.3 <a 
href="#x1-460003.4.2.3" id="QQ2-1-52">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.4 <a 
href="#x1-470003.4.2.4" id="QQ2-1-53">Linear Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.5 <a 
href="#x1-480003.4.2.5" id="QQ2-1-54">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >3.4.2.6 <a 
href="#x1-490003.4.2.6" id="QQ2-1-55">Stochastic Gradient Descent</a></span>
<br />&#x00A0;<span class="sectionToc" >3.5 <a 
href="#x1-500003.5" id="QQ2-1-56">Bayesian Statistics</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.5.1 <a 
href="#x1-510003.5.1" id="QQ2-1-57">No-U-Turn Sampler (NUTS)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.5.2 <a 
href="#x1-520003.5.2" id="QQ2-1-58">Metropolis-Hastings</a></span>
<br />&#x00A0;<span class="sectionToc" >3.6 <a 
href="#x1-530003.6" id="QQ2-1-59">Strategy</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.6.1 <a 
href="#x1-540003.6.1" id="QQ2-1-60">Classification</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >3.6.2 <a 
href="#x1-550003.6.2" id="QQ2-1-61">Regression</a></span>
<br /><span class="chapterToc" >4 <a 
href="#x1-560004" id="QQ2-1-62">Research Findings</a></span>
<br />&#x00A0;<span class="sectionToc" >4.1 <a 
href="#x1-570004.1" id="QQ2-1-64">Time Series Analysis</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.1 <a 
href="#x1-580004.1.1" id="QQ2-1-65">Random Walk</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.2 <a 
href="#x1-590004.1.2" id="QQ2-1-76">Ordinary Least Squares (OLS)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.3 <a 
href="#x1-600004.1.3" id="QQ2-1-87">Auto Regressive (AR)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.4 <a 
href="#x1-610004.1.4" id="QQ2-1-108">Moving Average (MA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.5 <a 
href="#x1-620004.1.5" id="QQ2-1-129">Auto Regressive Moving Average (ARMA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.1.6 <a 
href="#x1-630004.1.6" id="QQ2-1-150">Auto Regressive Integrated Moving Average (ARIMA)</a></span>
<br />&#x00A0;<span class="sectionToc" >4.2 <a 
href="#x1-640004.2" id="QQ2-1-171">Machine Learning</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.2.1 <a 
href="#x1-650004.2.1" id="QQ2-1-172">Classification</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.1 <a 
href="#x1-660004.2.1.1" id="QQ2-1-173">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.2 <a 
href="#x1-670004.2.1.2" id="QQ2-1-175">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.3 <a 
href="#x1-680004.2.1.3" id="QQ2-1-177">Support Vector Machine (SVM)</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.4 <a 
href="#x1-690004.2.1.4" id="QQ2-1-179">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.5 <a 
href="#x1-700004.2.1.5" id="QQ2-1-181">K-Nearest Neighbour</a></span>
                                                                                
                                                                                
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.6 <a 
href="#x1-710004.2.1.6" id="QQ2-1-183">Logistic Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.7 <a 
href="#x1-720004.2.1.7" id="QQ2-1-185">Gaussian Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.8 <a 
href="#x1-730004.2.1.8" id="QQ2-1-187">Bernoulli Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.9 <a 
href="#x1-740004.2.1.9" id="QQ2-1-189">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1.10 <a 
href="#x1-750004.2.1.10" id="QQ2-1-191">Stochastic Gradient Descent</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.2.2 <a 
href="#x1-760004.2.2" id="QQ2-1-193">Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.1 <a 
href="#x1-770004.2.2.1" id="QQ2-1-194">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.2 <a 
href="#x1-780004.2.2.2" id="QQ2-1-205">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.3 <a 
href="#x1-790004.2.2.3" id="QQ2-1-216">K-Nearest Neighbour</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.4 <a 
href="#x1-800004.2.2.4" id="QQ2-1-227">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.5 <a 
href="#x1-810004.2.2.5" id="QQ2-1-238">Linear Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.6 <a 
href="#x1-820004.2.2.6" id="QQ2-1-249">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2.7 <a 
href="#x1-830004.2.2.7" id="QQ2-1-260">Stochastic Gradient Descent</a></span>
<br />&#x00A0;<span class="sectionToc" >4.3 <a 
href="#x1-840004.3" id="QQ2-1-271">Bayesian Statistics</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.3.1 <a 
href="#x1-850004.3.1" id="QQ2-1-272">Metropolis-Hastings</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.3.2 <a 
href="#x1-860004.3.2" id="QQ2-1-283">No-U-Turn Sampler (NUTS)</a></span>
<br />&#x00A0;<span class="sectionToc" >4.4 <a 
href="#x1-870004.4" id="QQ2-1-294">Strategy</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.4.1 <a 
href="#x1-880004.4.1" id="QQ2-1-295">Classification</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >4.4.2 <a 
href="#x1-890004.4.2" id="QQ2-1-300">Regression</a></span>
<br /><span class="chapterToc" >5 <a 
href="#x1-900005" id="QQ2-1-305">Discussion and Suggestions for Future Research</a></span>
<br />&#x00A0;<span class="sectionToc" >5.1 <a 
href="#x1-910005.1" id="QQ2-1-306">Time Series Analysis</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.1 <a 
href="#x1-920005.1.1" id="QQ2-1-307">Random Walk</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.2 <a 
href="#x1-930005.1.2" id="QQ2-1-308">Ordinary Least Squares (OLS)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.3 <a 
href="#x1-940005.1.3" id="QQ2-1-309">Auto Regressive (AR)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.4 <a 
href="#x1-950005.1.4" id="QQ2-1-310">Moving Average (MA)</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.5 <a 
href="#x1-960005.1.5" id="QQ2-1-311">Auto Regressive Moving Average (ARMA)</a></span>
                                                                                
                                                                                
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.1.6 <a 
href="#x1-970005.1.6" id="QQ2-1-312">Auto Regressive Integrated Moving Average (ARIMA)</a></span>
<br />&#x00A0;<span class="sectionToc" >5.2 <a 
href="#x1-980005.2" id="QQ2-1-313">Machine Learning</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.2.1 <a 
href="#x1-990005.2.1" id="QQ2-1-314">Classification</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.1 <a 
href="#x1-1000005.2.1.1" id="QQ2-1-315">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.2 <a 
href="#x1-1010005.2.1.2" id="QQ2-1-316">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.3 <a 
href="#x1-1020005.2.1.3" id="QQ2-1-317">Support Vector Machine (SVM)</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.4 <a 
href="#x1-1030005.2.1.4" id="QQ2-1-318">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.5 <a 
href="#x1-1040005.2.1.5" id="QQ2-1-319">K-Nearest Neighbour</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.6 <a 
href="#x1-1050005.2.1.6" id="QQ2-1-320">Logistic Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.7 <a 
href="#x1-1060005.2.1.7" id="QQ2-1-321">Bernoulli Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.8 <a 
href="#x1-1070005.2.1.8" id="QQ2-1-322">Gaussian Naive Bayes</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.9 <a 
href="#x1-1080005.2.1.9" id="QQ2-1-323">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.1.10 <a 
href="#x1-1090005.2.1.10" id="QQ2-1-324">Stochastic Gradient Descent</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.2.2 <a 
href="#x1-1100005.2.2" id="QQ2-1-325">Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.1 <a 
href="#x1-1110005.2.2.1" id="QQ2-1-326">Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.2 <a 
href="#x1-1120005.2.2.2" id="QQ2-1-327">Boosted Decision Tree</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.3 <a 
href="#x1-1130005.2.2.3" id="QQ2-1-328">K-Nearest Neighbour</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.4 <a 
href="#x1-1140005.2.2.4" id="QQ2-1-329">Random Forest</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.5 <a 
href="#x1-1150005.2.2.5" id="QQ2-1-330">Linear Regression</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.6 <a 
href="#x1-1160005.2.2.6" id="QQ2-1-331">Neural Network</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.2.2.7 <a 
href="#x1-1170005.2.2.7" id="QQ2-1-332">Stochastic Gradient Descent</a></span>
<br />&#x00A0;<span class="sectionToc" >5.3 <a 
href="#x1-1180005.3" id="QQ2-1-333">Bayesian Statistics</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.3.0.1 <a 
href="#x1-1190005.3.0.1" id="QQ2-1-334">Metropolis-Hastings</a></span>
<br />&#x00A0;&#x00A0;&#x00A0;<span class="subsubsectionToc" >5.3.0.2 <a 
href="#x1-1200005.3.0.2" id="QQ2-1-335">No-U-Turn-Sampler (NUTS)</a></span>
<br />&#x00A0;<span class="sectionToc" >5.4 <a 
href="#x1-1210005.4" id="QQ2-1-336">Strategy</a></span>
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.4.1 <a 
href="#x1-1220005.4.1" id="QQ2-1-337">Classification</a></span>
                                                                                
                                                                                
<br />&#x00A0;&#x00A0;<span class="subsectionToc" >5.4.2 <a 
href="#x1-1230005.4.2" id="QQ2-1-338">Regression</a></span>
<br /><span class="chapterToc" >6 <a 
href="#x1-1240006" id="QQ2-1-339">Conclusion</a></span>
<br /><span class="appendixToc" >A <a 
href="#x1-125000A" id="QQ2-1-340">An Appendix</a></span>
<br /><span class="chapterToc" ><a 
href="#Q1-1-341">Bibliography</a></span>
</div>
<a 
 id="x1-1001r4"></a>
<a 
 id="Q1-1-5"></a>
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-20004"></a>List of Figures</h2> <div class="tableofcontents"><span class="lofToc" >4.1&#x00A0;<a 
href="#x1-560011">Basket of stocks</a></span><br /><span class="lofToc" >4.2&#x00A0;<a 
href="#x1-580012">MSFT time series analysis</a></span><br /><span class="lofToc" >4.3&#x00A0;<a 
href="#x1-580023">MSFT
histogram of returns</a></span><br /><span class="lofToc" >4.4&#x00A0;<a 
href="#x1-580034">CDE time series analysis</a></span><br /><span class="lofToc" >4.5&#x00A0;<a 
href="#x1-580045">CDE histogram of
returns</a></span><br /><span class="lofToc" >4.6&#x00A0;<a 
href="#x1-580056">NAVB time series analysis</a></span><br /><span class="lofToc" >4.7&#x00A0;<a 
href="#x1-580067">NAVB histogram of returns</a></span><br /><span class="lofToc" >4.8&#x00A0;<a 
href="#x1-580078">HRG time
series analysis</a></span><br /><span class="lofToc" >4.9&#x00A0;<a 
href="#x1-580089">HRG histogram of returns</a></span><br /><span class="lofToc" >4.10&#x00A0;<a 
href="#x1-5800910">HL time series analysis</a></span><br /><span class="lofToc" >4.11&#x00A0;<a 
href="#x1-5801011">HL
histogram of returns</a></span><br /><span class="lofToc" >4.12&#x00A0;<a 
href="#x1-5900112">MSFT OLS in-sample prediction</a></span><br /><span class="lofToc" >4.13&#x00A0;<a 
href="#x1-5900213">100 day MSFT
OLS in-sample forecast</a></span><br /><span class="lofToc" >4.14&#x00A0;<a 
href="#x1-5900314">CDE OLS in-sample prediction</a></span><br /><span class="lofToc" >4.15&#x00A0;<a 
href="#x1-5900415">100 Day CDE
OLS out of sample forecast</a></span><br /><span class="lofToc" >4.16&#x00A0;<a 
href="#x1-5900516">NAVB OLS in-sample prediction</a></span><br /><span class="lofToc" >4.17&#x00A0;<a 
href="#x1-5900617">100 day
NAVB OLS in-sample forecast</a></span><br /><span class="lofToc" >4.18&#x00A0;<a 
href="#x1-5900718">HRG OLS in-sample prediction</a></span><br /><span class="lofToc" >4.19&#x00A0;<a 
href="#x1-5900819">100 day
HRG OLS in-sample forecast</a></span><br /><span class="lofToc" >4.20&#x00A0;<a 
href="#x1-5900920">HL OLS in-sample prediction</a></span><br /><span class="lofToc" >4.21&#x00A0;<a 
href="#x1-5901021">100 day
HL OLS in-sample forecast</a></span><br /><span class="lofToc" >4.22&#x00A0;<a 
href="#x1-6000122">MSFT AR time series analysis</a></span><br /><span class="lofToc" >4.23&#x00A0;<a 
href="#x1-6000223">MSFT AR
histogram of returns</a></span><br /><span class="lofToc" >4.24&#x00A0;<a 
href="#x1-6000324">MSFT AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.25&#x00A0;<a 
href="#x1-6000425">100 day
MSFT AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.26&#x00A0;<a 
href="#x1-6000526">CDE AR time series analysis</a></span><br /><span class="lofToc" >4.27&#x00A0;<a 
href="#x1-6000627">CDE
AR histogram of returns</a></span><br /><span class="lofToc" >4.28&#x00A0;<a 
href="#x1-6000728">CDE AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.29&#x00A0;<a 
href="#x1-6000829">100 day
CDE AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.30&#x00A0;<a 
href="#x1-6000930">NAVB AR time series analysis</a></span><br /><span class="lofToc" >4.31&#x00A0;<a 
href="#x1-6001031">NAVB
AR histogram of returns</a></span><br /><span class="lofToc" >4.32&#x00A0;<a 
href="#x1-6001132">NAVB AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.33&#x00A0;<a 
href="#x1-6001233">100 day
NAVB AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.34&#x00A0;<a 
href="#x1-6001334">HRG AR time series analysis</a></span><br /><span class="lofToc" >4.35&#x00A0;<a 
href="#x1-6001435">HRG
AR histogram of returns</a></span><br /><span class="lofToc" >4.36&#x00A0;<a 
href="#x1-6001536">HL AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.37&#x00A0;<a 
href="#x1-6001637">100 day
HRG AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.38&#x00A0;<a 
href="#x1-6001738">HL AR time series analysis</a></span><br /><span class="lofToc" >4.39&#x00A0;<a 
href="#x1-6001839">HL AR
histogram of returns</a></span><br /><span class="lofToc" >4.40&#x00A0;<a 
href="#x1-6001940">HL AR in-sample returns prediction</a></span><br /><span class="lofToc" >4.41&#x00A0;<a 
href="#x1-6002041">100 day HL
AR in-sample returns forecast</a></span><br /><span class="lofToc" >4.42&#x00A0;<a 
href="#x1-6100142">MSFT MA time series analysis</a></span><br /><span class="lofToc" >4.43&#x00A0;<a 
href="#x1-6100243">MSFT MA
histogram of returns</a></span><br /><span class="lofToc" >4.44&#x00A0;<a 
href="#x1-6100344">MSFT MA in-sample returns prediction</a></span><br /><span class="lofToc" >4.45&#x00A0;<a 
href="#x1-6100445">100 day
MSFT MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.46&#x00A0;<a 
href="#x1-6100546">CDE MA time series analysis</a></span><br /><span class="lofToc" >4.47&#x00A0;<a 
href="#x1-6100647">CDE
MA histogram of returns</a></span><br /><span class="lofToc" >4.48&#x00A0;<a 
href="#x1-6100748">CDE MA in-sample returns prediction</a></span><br /><span class="lofToc" >4.49&#x00A0;<a 
href="#x1-6100849">100
day CDE MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.50&#x00A0;<a 
href="#x1-6100950">NAVB MA time series
analysis</a></span><br /><span class="lofToc" >4.51&#x00A0;<a 
href="#x1-6101051">NAVB MA histogram of returns</a></span><br /><span class="lofToc" >4.52&#x00A0;<a 
href="#x1-6101152">NAVB MA in-sample returns
prediction</a></span><br /><span class="lofToc" >4.53&#x00A0;<a 
href="#x1-6101253">100 day NAVB MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.54&#x00A0;<a 
href="#x1-6101354">HRG MA
time series analysis</a></span><br /><span class="lofToc" >4.55&#x00A0;<a 
href="#x1-6101455">HRG MA histogram of returns</a></span><br /><span class="lofToc" >4.56&#x00A0;<a 
href="#x1-6101556">HRG MA in-sample
returns prediction</a></span><br /><span class="lofToc" >4.57&#x00A0;<a 
href="#x1-6101657">100 day HRG MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.58&#x00A0;<a 
href="#x1-6101758">HL MA
time series analysis</a></span><br /><span class="lofToc" >4.59&#x00A0;<a 
href="#x1-6101859">HL MA histogram of returns</a></span><br /><span class="lofToc" >4.60&#x00A0;<a 
href="#x1-6101960">HL MA in-sample
returns prediction</a></span><br /><span class="lofToc" >4.61&#x00A0;<a 
href="#x1-6102061">100 day HL MA in-sample returns forecast</a></span><br /><span class="lofToc" >4.62&#x00A0;<a 
href="#x1-6200162">MSFT
ARMA time series analysis</a></span><br /><span class="lofToc" >4.63&#x00A0;<a 
href="#x1-6200263">MSFT ARMA histogram of returns</a></span><br /><span class="lofToc" >4.64&#x00A0;<a 
href="#x1-6200364">MSFT
ARMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.65&#x00A0;<a 
href="#x1-6200465">100 day MSFT ARMA in-sample
returns forecast</a></span><br /><span class="lofToc" >4.66&#x00A0;<a 
href="#x1-6200566">CDE ARMA time series analysis</a></span><br /><span class="lofToc" >4.67&#x00A0;<a 
href="#x1-6200667">CDE ARMA
histogram of returns</a></span><br /><span class="lofToc" >4.68&#x00A0;<a 
href="#x1-6200768">CDE ARMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.69&#x00A0;<a 
href="#x1-6200869">100
day CDE ARMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.70&#x00A0;<a 
href="#x1-6200970">NAVB ARMA time series
analysis</a></span><br /><span class="lofToc" >4.71&#x00A0;<a 
href="#x1-6201071">NAVB ARMA histogram of returns</a></span><br /><span class="lofToc" >4.72&#x00A0;<a 
href="#x1-6201172">NAVB ARMA in-sample
returns prediction</a></span><br /><span class="lofToc" >4.73&#x00A0;<a 
href="#x1-6201273">100 day NAVB ARMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.74&#x00A0;<a 
href="#x1-6201374">HRG
ARMA time series analysis</a></span><br /><span class="lofToc" >4.75&#x00A0;<a 
href="#x1-6201475">HRG ARMA histogram of returns</a></span><br /><span class="lofToc" >4.76&#x00A0;<a 
href="#x1-6201576">HRG
ARMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.77&#x00A0;<a 
href="#x1-6201677">100 day HRG ARMA in-sample
returns forecast</a></span><br /><span class="lofToc" >4.78&#x00A0;<a 
href="#x1-6201778">HL ARMA time series analysis</a></span><br /><span class="lofToc" >4.79&#x00A0;<a 
href="#x1-6201879">HL ARMA histogram
of returns</a></span><br /><span class="lofToc" >4.80&#x00A0;<a 
href="#x1-6201980">HL ARMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.81&#x00A0;<a 
href="#x1-6202081">100 day HL ARMA
in-sample returns forecast</a></span><br /><span class="lofToc" >4.82&#x00A0;<a 
href="#x1-6300182">MSFT ARIMA time series analysis</a></span><br /><span class="lofToc" >4.83&#x00A0;<a 
href="#x1-6300283">MSFT
                                                                                
                                                                                
ARIMA histogram of returns</a></span><br /><span class="lofToc" >4.84&#x00A0;<a 
href="#x1-6300384">MSFT ARIMA in-sample returns
prediction</a></span><br /><span class="lofToc" >4.85&#x00A0;<a 
href="#x1-6300485">100 day MSFT ARIMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.86&#x00A0;<a 
href="#x1-6300586">CDE
ARIMA time series analysis</a></span><br /><span class="lofToc" >4.87&#x00A0;<a 
href="#x1-6300687">CDE ARIMA histogram of returns</a></span><br /><span class="lofToc" >4.88&#x00A0;<a 
href="#x1-6300788">CDE
ARIMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.89&#x00A0;<a 
href="#x1-6300889">100 day CDE ARIMA in-sample
returns forecast</a></span><br /><span class="lofToc" >4.90&#x00A0;<a 
href="#x1-6300990">NAVB ARIMA time series analysis</a></span><br /><span class="lofToc" >4.91&#x00A0;<a 
href="#x1-6301091">NAVB ARIMA
histogram of returns</a></span><br /><span class="lofToc" >4.92&#x00A0;<a 
href="#x1-6301192">NAVB ARIMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.93&#x00A0;<a 
href="#x1-6301293">100
day NAVB ARIMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.94&#x00A0;<a 
href="#x1-6301394">HRG ARIMA time series
analysis</a></span><br /><span class="lofToc" >4.95&#x00A0;<a 
href="#x1-6301495">HRG ARIMA histogram of returns</a></span><br /><span class="lofToc" >4.96&#x00A0;<a 
href="#x1-6301596">HRG ARIMA in-sample
returns prediction</a></span><br /><span class="lofToc" >4.97&#x00A0;<a 
href="#x1-6301697">100 day HRG ARIMA in-sample returns forecast</a></span><br /><span class="lofToc" >4.98&#x00A0;<a 
href="#x1-6301798">HL
ARIMA time series analysis</a></span><br /><span class="lofToc" >4.99&#x00A0;<a 
href="#x1-6301899">HL ARIMA histogram of returns</a></span><br /><span class="lofToc" >4.100&#x00A0;<a 
href="#x1-63019100">HL
ARIMA in-sample returns prediction</a></span><br /><span class="lofToc" >4.101&#x00A0;<a 
href="#x1-63020101">100 day HL ARIMA in-sample
returns forecast</a></span><br /><span class="lofToc" >4.102&#x00A0;<a 
href="#x1-77001102">MSFT Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.103&#x00A0;<a 
href="#x1-77002103">100 day
MSFT Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.104&#x00A0;<a 
href="#x1-77003104">CDE Decision Trees in-sample
prediction</a></span><br /><span class="lofToc" >4.105&#x00A0;<a 
href="#x1-77004105">100 day CDE Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.106&#x00A0;<a 
href="#x1-77005106">NAVB
Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.107&#x00A0;<a 
href="#x1-77006107">100 day NAVB Decision Trees
out-of-sample forecast</a></span><br /><span class="lofToc" >4.108&#x00A0;<a 
href="#x1-77007108">HRG Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.109&#x00A0;<a 
href="#x1-77008109">100
day HRG Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.110&#x00A0;<a 
href="#x1-77009110">HL Decision Trees in-sample
prediction</a></span><br /><span class="lofToc" >4.111&#x00A0;<a 
href="#x1-77010111">100 day HL Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.112&#x00A0;<a 
href="#x1-78001112">MSFT
Boosted Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.113&#x00A0;<a 
href="#x1-78002113">100 day MSFT Boosted
Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.114&#x00A0;<a 
href="#x1-78003114">CDE Boosted Decision Trees
in-sample prediction</a></span><br /><span class="lofToc" >4.115&#x00A0;<a 
href="#x1-78004115">100 day CDE Boosted Decision Trees out-of-sample
forecast</a></span><br /><span class="lofToc" >4.116&#x00A0;<a 
href="#x1-78005116">NAVB Boosted Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.117&#x00A0;<a 
href="#x1-78006117">100 day
NAVB Boosted Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.118&#x00A0;<a 
href="#x1-78007118">HRG Boosted Decision
Trees in-sample prediction</a></span><br /><span class="lofToc" >4.119&#x00A0;<a 
href="#x1-78008119">100 day HRG Boosted Decision Trees out-of-sample
forecast</a></span><br /><span class="lofToc" >4.120&#x00A0;<a 
href="#x1-78009120">HL Boosted Decision Trees in-sample prediction</a></span><br /><span class="lofToc" >4.121&#x00A0;<a 
href="#x1-78010121">100 day HL
Boosted Decision Trees out-of-sample forecast</a></span><br /><span class="lofToc" >4.122&#x00A0;<a 
href="#x1-79001122">MSFT K-Nearest Neighbour
in-sample prediction</a></span><br /><span class="lofToc" >4.123&#x00A0;<a 
href="#x1-79002123">100 day MSFT K-Nearest Neighbour out-of-sample
forecast</a></span><br /><span class="lofToc" >4.124&#x00A0;<a 
href="#x1-79003124">CDE K-Nearest Neighbour in-sample prediction</a></span><br /><span class="lofToc" >4.125&#x00A0;<a 
href="#x1-79004125">100 day CDE
K-Nearest Neighbour out-of-sample forecast</a></span><br /><span class="lofToc" >4.126&#x00A0;<a 
href="#x1-79005126">NAVB K-Nearest Neighbour
in-sample prediction</a></span><br /><span class="lofToc" >4.127&#x00A0;<a 
href="#x1-79006127">100 day NAVB K-Nearest Neighbour out-of-sample
forecast</a></span><br /><span class="lofToc" >4.128&#x00A0;<a 
href="#x1-79007128">HRG K-Nearest Neighbour in-sample prediction</a></span><br /><span class="lofToc" >4.129&#x00A0;<a 
href="#x1-79008129">100 day
HRG K-Nearest Neighbour out-of-sample forecast</a></span><br /><span class="lofToc" >4.130&#x00A0;<a 
href="#x1-79009130">HL K-Nearest Neighbour
in-sample prediction</a></span><br /><span class="lofToc" >4.131&#x00A0;<a 
href="#x1-79010131">100 day HL K-Nearest Neighbour out-of-sample
forecast</a></span><br /><span class="lofToc" >4.132&#x00A0;<a 
href="#x1-80001132">MSFT Random Forest in-sample prediction</a></span><br /><span class="lofToc" >4.133&#x00A0;<a 
href="#x1-80002133">100 day MSFT
Random Forest out-of-sample forecast</a></span><br /><span class="lofToc" >4.134&#x00A0;<a 
href="#x1-80003134">CDE Random Forest in-sample
prediction</a></span><br /><span class="lofToc" >4.135&#x00A0;<a 
href="#x1-80004135">100 day CDE Random Forest out-of-sample forecast</a></span><br /><span class="lofToc" >4.136&#x00A0;<a 
href="#x1-80005136">NAVB
Random Forest in-sample prediction</a></span><br /><span class="lofToc" >4.137&#x00A0;<a 
href="#x1-80006137">100 day NAVB Random Forest
out-of-sample forecast</a></span><br /><span class="lofToc" >4.138&#x00A0;<a 
href="#x1-80007138">HRG Random Forest in-sample prediction</a></span><br /><span class="lofToc" >4.139&#x00A0;<a 
href="#x1-80008139">100
day HRG Random Forest out-of-sample forecast</a></span><br /><span class="lofToc" >4.140&#x00A0;<a 
href="#x1-80009140">HL Random Forest in-sample
prediction</a></span><br /><span class="lofToc" >4.141&#x00A0;<a 
href="#x1-80010141">100 day HL Random Forest out-of-sample forecast</a></span><br /><span class="lofToc" >4.142&#x00A0;<a 
href="#x1-81001142">MSFT
Linear Regression in-sample prediction</a></span><br /><span class="lofToc" >4.143&#x00A0;<a 
href="#x1-81002143">100 day MSFT Linear Regression
out-of-sample forecast</a></span><br /><span class="lofToc" >4.144&#x00A0;<a 
href="#x1-81003144">CDE Linear Regression in-sample prediction</a></span><br /><span class="lofToc" >4.145&#x00A0;<a 
href="#x1-81004145">100
day CDE Linear Regression out-of-sample forecast</a></span><br /><span class="lofToc" >4.146&#x00A0;<a 
href="#x1-81005146">CDE Linear Regression
in-sample prediction</a></span><br /><span class="lofToc" >4.147&#x00A0;<a 
href="#x1-81006147">100 day CDE Linear Regression out-of-sample
forecast</a></span><br /><span class="lofToc" >4.148&#x00A0;<a 
href="#x1-81007148">HRG Linear Regression in-sample prediction</a></span><br /><span class="lofToc" >4.149&#x00A0;<a 
href="#x1-81008149">100 day HRG
Linear Regression out-of-sample forecast</a></span><br /><span class="lofToc" >4.150&#x00A0;<a 
href="#x1-81009150">HL Linear Regression in-sample
prediction</a></span><br /><span class="lofToc" >4.151&#x00A0;<a 
href="#x1-81010151">100 day HL Linear Regression out-of-sample forecast</a></span><br /><span class="lofToc" >4.152&#x00A0;<a 
href="#x1-82001152">MSFT
Neural Network in-sample prediction</a></span><br /><span class="lofToc" >4.153&#x00A0;<a 
href="#x1-82002153">100 day MSFT Neural Network
out-of-sample forecast</a></span><br /><span class="lofToc" >4.154&#x00A0;<a 
href="#x1-82003154">CDE Neural Network in-sample prediction</a></span><br /><span class="lofToc" >4.155&#x00A0;<a 
href="#x1-82004155">100
day CDE Neural Network out-of-sample forecast</a></span><br /><span class="lofToc" >4.156&#x00A0;<a 
href="#x1-82005156">NAVB Neural Network
                                                                                
                                                                                
in-sample prediction</a></span><br /><span class="lofToc" >4.157&#x00A0;<a 
href="#x1-82006157">100 day NAVB Neural Network out-of-sample
forecast</a></span><br /><span class="lofToc" >4.158&#x00A0;<a 
href="#x1-82007158">HRG Neural Network in-sample prediction</a></span><br /><span class="lofToc" >4.159&#x00A0;<a 
href="#x1-82008159">100 day HRG
Neural Network out-of-sample forecast</a></span><br /><span class="lofToc" >4.160&#x00A0;<a 
href="#x1-82009160">HL Neural Network in-sample
prediction</a></span><br /><span class="lofToc" >4.161&#x00A0;<a 
href="#x1-82010161">100 day HL Neural Network out-of-sample forecast</a></span><br /><span class="lofToc" >4.162&#x00A0;<a 
href="#x1-83001162">MSFT
SGD in-sample prediction</a></span><br /><span class="lofToc" >4.163&#x00A0;<a 
href="#x1-83002163">100 day MSFT SGD out-of-sample
forecast</a></span><br /><span class="lofToc" >4.164&#x00A0;<a 
href="#x1-83003164">CDE SGD in-sample prediction</a></span><br /><span class="lofToc" >4.165&#x00A0;<a 
href="#x1-83004165">100 day CDE SGD
out-of-sample forecast</a></span><br /><span class="lofToc" >4.166&#x00A0;<a 
href="#x1-83005166">NAVB SGD in-sample prediction</a></span><br /><span class="lofToc" >4.167&#x00A0;<a 
href="#x1-83006167">100 day NAVB
SGD out-of-sample forecast</a></span><br /><span class="lofToc" >4.168&#x00A0;<a 
href="#x1-83007168">HRG SGD in-sample prediction</a></span><br /><span class="lofToc" >4.169&#x00A0;<a 
href="#x1-83008169">100 day
HRG SGD out-of-sample forecast</a></span><br /><span class="lofToc" >4.170&#x00A0;<a 
href="#x1-83009170">HL SGD in-sample prediction</a></span><br /><span class="lofToc" >4.171&#x00A0;<a 
href="#x1-83010171">100
day HL SGD out-of-sample forecast</a></span><br /><span class="lofToc" >4.172&#x00A0;<a 
href="#x1-85001172">MSFT Metropolis-Hastimgs
in-sample prediction</a></span><br /><span class="lofToc" >4.173&#x00A0;<a 
href="#x1-85002173">100 day MSFT Metropolis-Hastings out-of-sample
forecast</a></span><br /><span class="lofToc" >4.174&#x00A0;<a 
href="#x1-85003174">CDE Metropolis-Hastimgs in-sample prediction</a></span><br /><span class="lofToc" >4.175&#x00A0;<a 
href="#x1-85004175">100 day CDE
Metropolis-Hastings out-of-sample forecast</a></span><br /><span class="lofToc" >4.176&#x00A0;<a 
href="#x1-85005176">NAVB Metropolis-Hastimgs
in-sample prediction</a></span><br /><span class="lofToc" >4.177&#x00A0;<a 
href="#x1-85006177">100 day NAVB Metropolis-Hastings out-of-sample
forecast</a></span><br /><span class="lofToc" >4.178&#x00A0;<a 
href="#x1-85007178">HRG Metropolis-Hastimgs in-sample prediction</a></span><br /><span class="lofToc" >4.179&#x00A0;<a 
href="#x1-85008179">100 day
HRG Metropolis-Hastings out-of-sample forecast</a></span><br /><span class="lofToc" >4.180&#x00A0;<a 
href="#x1-85009180">HL Metropolis-Hastimgs
in-sample prediction</a></span><br /><span class="lofToc" >4.181&#x00A0;<a 
href="#x1-85010181">100 day HL Metropolis-Hastings out-of-sample
forecast</a></span><br /><span class="lofToc" >4.182&#x00A0;<a 
href="#x1-86001182">MSFT NUTS in-sample prediction</a></span><br /><span class="lofToc" >4.183&#x00A0;<a 
href="#x1-86002183">100 day MSFT NUTS
out-of-sample forecast</a></span><br /><span class="lofToc" >4.184&#x00A0;<a 
href="#x1-86003184">CDE NUTS in-sample prediction</a></span><br /><span class="lofToc" >4.185&#x00A0;<a 
href="#x1-86004185">100 day CDE
NUTS out-of-sample forecast</a></span><br /><span class="lofToc" >4.186&#x00A0;<a 
href="#x1-86005186">NAVB NUTS in-sample prediction</a></span><br /><span class="lofToc" >4.187&#x00A0;<a 
href="#x1-86006187">100
day NAVB NUTS out-of-sample forecast</a></span><br /><span class="lofToc" >4.188&#x00A0;<a 
href="#x1-86007188">HRG NUTS in-sample
prediction</a></span><br /><span class="lofToc" >4.189&#x00A0;<a 
href="#x1-86008189">100 day HRG NUTS out-of-sample forecast</a></span><br /><span class="lofToc" >4.190&#x00A0;<a 
href="#x1-86009190">HL NUTS
in-sample prediction</a></span><br /><span class="lofToc" >4.191&#x00A0;<a 
href="#x1-86010191">100 day HL NUTS out-of-sample forecast</a></span><br /><span class="lofToc" >4.192&#x00A0;<a 
href="#x1-88002192">Machine
Learning Classifier strategy with only upwards forecasts</a></span><br /><span class="lofToc" >4.193&#x00A0;<a 
href="#x1-88004193">Machine Learning
Classifier strategy with upwards and downwards forecasts</a></span><br /><span class="lofToc" >4.194&#x00A0;<a 
href="#x1-89002194">Machine
Learning Regression strategy with only upwards forecasts</a></span><br /><span class="lofToc" >4.195&#x00A0;<a 
href="#x1-89004195">Machine
Learning Regression strategy with upwards and downwards forecasts</a></span><br />
</div>
                                                                                
                                                                                
<a 
 id="x1-2001r5"></a>
<a 
 id="Q1-1-7"></a>
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-30005"></a>List of Tables</h2> <div class="tableofcontents"><span class="lotToc" >4.1&#x00A0;<a 
href="#x1-660011">Decision Tree results</a></span><br /><span class="lotToc" >4.2&#x00A0;<a 
href="#x1-670012">Boosted Decision Tree
results</a></span><br /><span class="lotToc" >4.3&#x00A0;<a 
href="#x1-680013">Support Vector Machine results</a></span><br /><span class="lotToc" >4.4&#x00A0;<a 
href="#x1-690014">Random Forest results</a></span><br /><span class="lotToc" >4.5&#x00A0;<a 
href="#x1-700015">K-Nearest
Neighbour results</a></span><br /><span class="lotToc" >4.6&#x00A0;<a 
href="#x1-710016">Logistic Regression results</a></span><br /><span class="lotToc" >4.7&#x00A0;<a 
href="#x1-720017">Gaussian Naive
Bayes results</a></span><br /><span class="lotToc" >4.8&#x00A0;<a 
href="#x1-730018">Bernoulli Naive Bayes results</a></span><br /><span class="lotToc" >4.9&#x00A0;<a 
href="#x1-740019">Neural Network
results</a></span><br /><span class="lotToc" >4.10&#x00A0;<a 
href="#x1-7500110">Stochastic Gradient Descent results</a></span><br /><span class="lotToc" >4.11&#x00A0;<a 
href="#x1-8800111">Machine Learning
Classifier strategy with only upwards forecasts</a></span><br /><span class="lotToc" >4.12&#x00A0;<a 
href="#x1-8800312">Machine Learning
Classifier strategy with upwarda and downwards forecasts</a></span><br /><span class="lotToc" >4.13&#x00A0;<a 
href="#x1-8900113">Machine
Learning Regression strategy with only upwards forecasts</a></span><br /><span class="lotToc" >4.14&#x00A0;<a 
href="#x1-8900314">Machine
Learning Regression strategy with upwards and downwards forecasts</a></span><br />
</div>
                                                                                
                                                                                
                                                                                
                                                                                
<a 
 id="x1-3001r6"></a>
<a 
 id="Q1-1-9"></a>
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-40006"></a>Abbreviations</h2>
<a 
 id="x1-560011"></a><a 
 id="x1-580012"></a><a 
 id="x1-580023"></a><a 
 id="x1-580034"></a><a 
 id="x1-580045"></a><a 
 id="x1-580056"></a><a 
 id="x1-580067"></a><a 
 id="x1-580078"></a><a 
 id="x1-580089"></a><a 
 id="x1-5800910"></a><a 
 id="x1-5801011"></a><a 
 id="x1-5900112"></a><a 
 id="x1-5900213"></a><a 
 id="x1-5900314"></a><a 
 id="x1-5900415"></a><a 
 id="x1-5900516"></a><a 
 id="x1-5900617"></a><a 
 id="x1-5900718"></a><a 
 id="x1-5900819"></a><a 
 id="x1-5900920"></a><a 
 id="x1-5901021"></a><a 
 id="x1-6000122"></a><a 
 id="x1-6000223"></a><a 
 id="x1-6000324"></a><a 
 id="x1-6000425"></a><a 
 id="x1-6000526"></a><a 
 id="x1-6000627"></a><a 
 id="x1-6000728"></a><a 
 id="x1-6000829"></a><a 
 id="x1-6000930"></a><a 
 id="x1-6001031"></a><a 
 id="x1-6001132"></a><a 
 id="x1-6001233"></a><a 
 id="x1-6001334"></a><a 
 id="x1-6001435"></a><a 
 id="x1-6001536"></a><a 
 id="x1-6001637"></a><a 
 id="x1-6001738"></a><a 
 id="x1-6001839"></a><a 
 id="x1-6001940"></a><a 
 id="x1-6002041"></a><a 
 id="x1-6100142"></a><a 
 id="x1-6100243"></a><a 
 id="x1-6100344"></a><a 
 id="x1-6100445"></a><a 
 id="x1-6100546"></a><a 
 id="x1-6100647"></a><a 
 id="x1-6100748"></a><a 
 id="x1-6100849"></a><a 
 id="x1-6100950"></a><a 
 id="x1-6101051"></a><a 
 id="x1-6101152"></a><a 
 id="x1-6101253"></a><a 
 id="x1-6101354"></a><a 
 id="x1-6101455"></a><a 
 id="x1-6101556"></a><a 
 id="x1-6101657"></a><a 
 id="x1-6101758"></a><a 
 id="x1-6101859"></a><a 
 id="x1-6101960"></a><a 
 id="x1-6102061"></a><a 
 id="x1-6200162"></a><a 
 id="x1-6200263"></a><a 
 id="x1-6200364"></a><a 
 id="x1-6200465"></a><a 
 id="x1-6200566"></a><a 
 id="x1-6200667"></a><a 
 id="x1-6200768"></a><a 
 id="x1-6200869"></a><a 
 id="x1-6200970"></a><a 
 id="x1-6201071"></a><a 
 id="x1-6201172"></a><a 
 id="x1-6201273"></a><a 
 id="x1-6201374"></a><a 
 id="x1-6201475"></a><a 
 id="x1-6201576"></a><a 
 id="x1-6201677"></a><a 
 id="x1-6201778"></a><a 
 id="x1-6201879"></a><a 
 id="x1-6201980"></a><a 
 id="x1-6202081"></a><a 
 id="x1-6300182"></a><a 
 id="x1-6300283"></a><a 
 id="x1-6300384"></a><a 
 id="x1-6300485"></a><a 
 id="x1-6300586"></a><a 
 id="x1-6300687"></a><a 
 id="x1-6300788"></a><a 
 id="x1-6300889"></a><a 
 id="x1-6300990"></a><a 
 id="x1-6301091"></a><a 
 id="x1-6301192"></a><a 
 id="x1-6301293"></a><a 
 id="x1-6301394"></a><a 
 id="x1-6301495"></a><a 
 id="x1-6301596"></a><a 
 id="x1-6301697"></a><a 
 id="x1-6301798"></a><a 
 id="x1-6301899"></a><a 
 id="x1-63019100"></a><a 
 id="x1-63020101"></a><a 
 id="x1-660011"></a><a 
 id="x1-670012"></a><a 
 id="x1-680013"></a><a 
 id="x1-690014"></a><a 
 id="x1-700015"></a><a 
 id="x1-710016"></a><a 
 id="x1-720017"></a><a 
 id="x1-730018"></a><a 
 id="x1-740019"></a><a 
 id="x1-7500110"></a><a 
 id="x1-77001102"></a><a 
 id="x1-77002103"></a><a 
 id="x1-77003104"></a><a 
 id="x1-77004105"></a><a 
 id="x1-77005106"></a><a 
 id="x1-77006107"></a><a 
 id="x1-77007108"></a><a 
 id="x1-77008109"></a><a 
 id="x1-77009110"></a><a 
 id="x1-77010111"></a><a 
 id="x1-78001112"></a><a 
 id="x1-78002113"></a><a 
 id="x1-78003114"></a><a 
 id="x1-78004115"></a><a 
 id="x1-78005116"></a><a 
 id="x1-78006117"></a><a 
 id="x1-78007118"></a><a 
 id="x1-78008119"></a><a 
 id="x1-78009120"></a><a 
 id="x1-78010121"></a><a 
 id="x1-79001122"></a><a 
 id="x1-79002123"></a><a 
 id="x1-79003124"></a><a 
 id="x1-79004125"></a><a 
 id="x1-79005126"></a><a 
 id="x1-79006127"></a><a 
 id="x1-79007128"></a><a 
 id="x1-79008129"></a><a 
 id="x1-79009130"></a><a 
 id="x1-79010131"></a><a 
 id="x1-80001132"></a><a 
 id="x1-80002133"></a><a 
 id="x1-80003134"></a><a 
 id="x1-80004135"></a><a 
 id="x1-80005136"></a><a 
 id="x1-80006137"></a><a 
 id="x1-80007138"></a><a 
 id="x1-80008139"></a><a 
 id="x1-80009140"></a><a 
 id="x1-80010141"></a><a 
 id="x1-81001142"></a><a 
 id="x1-81002143"></a><a 
 id="x1-81003144"></a><a 
 id="x1-81004145"></a><a 
 id="x1-81005146"></a><a 
 id="x1-81006147"></a><a 
 id="x1-81007148"></a><a 
 id="x1-81008149"></a><a 
 id="x1-81009150"></a><a 
 id="x1-81010151"></a><a 
 id="x1-82001152"></a><a 
 id="x1-82002153"></a><a 
 id="x1-82003154"></a><a 
 id="x1-82004155"></a><a 
 id="x1-82005156"></a><a 
 id="x1-82006157"></a><a 
 id="x1-82007158"></a><a 
 id="x1-82008159"></a><a 
 id="x1-82009160"></a><a 
 id="x1-82010161"></a><a 
 id="x1-83001162"></a><a 
 id="x1-83002163"></a><a 
 id="x1-83003164"></a><a 
 id="x1-83004165"></a><a 
 id="x1-83005166"></a><a 
 id="x1-83006167"></a><a 
 id="x1-83007168"></a><a 
 id="x1-83008169"></a><a 
 id="x1-83009170"></a><a 
 id="x1-83010171"></a><a 
 id="x1-85001172"></a><a 
 id="x1-85002173"></a><a 
 id="x1-85003174"></a><a 
 id="x1-85004175"></a><a 
 id="x1-85005176"></a><a 
 id="x1-85006177"></a><a 
 id="x1-85007178"></a><a 
 id="x1-85008179"></a><a 
 id="x1-85009180"></a><a 
 id="x1-85010181"></a><a 
 id="x1-86001182"></a><a 
 id="x1-86002183"></a><a 
 id="x1-86003184"></a><a 
 id="x1-86004185"></a><a 
 id="x1-86005186"></a><a 
 id="x1-86006187"></a><a 
 id="x1-86007188"></a><a 
 id="x1-86008189"></a><a 
 id="x1-86009190"></a><a 
 id="x1-86010191"></a><a 
 id="x1-8800111"></a><a 
 id="x1-88002192"></a><a 
 id="x1-8800312"></a><a 
 id="x1-88004193"></a><a 
 id="x1-8900113"></a><a 
 id="x1-89002194"></a><a 
 id="x1-8900314"></a><a 
 id="x1-89004195"></a>
<a 
 id="x1-4001r1"></a><!--l. 160--><div class="longtable"><table id="TBL-1" class="longtable" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-1-1g"><col 
id="TBL-1-1"><col 
id="TBL-1-2"></colgroup>
<tr  
 style="vertical-align:baseline;" id="TBL-1-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-1-1"  
class="td11"><span 
class="cmbx-10x-x-109">EMH </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-1-2"  
class="td11"><span 
class="cmbx-10x-x-109">E</span>fficient <span 
class="cmbx-10x-x-109">M</span>arket <span 
class="cmbx-10x-x-109">H</span>ypothesis                      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-2-1"  
class="td11"><span 
class="cmbx-10x-x-109">RWH </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-2-2"  
class="td11"><span 
class="cmbx-10x-x-109">R</span>andom <span 
class="cmbx-10x-x-109">W</span>alk <span 
class="cmbx-10x-x-109">H</span>ypothesis                        </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-3-1"  
class="td11"><span 
class="cmbx-10x-x-109">OLS </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-3-2"  
class="td11"><span 
class="cmbx-10x-x-109">O</span>rdinary <span 
class="cmbx-10x-x-109">L</span>east <span 
class="cmbx-10x-x-109">S</span>quares                            </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-4-1"  
class="td11"><span 
class="cmbx-10x-x-109">AR </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-4-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>uto <span 
class="cmbx-10x-x-109">R</span>egressive                                      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-5-1"  
class="td11"><span 
class="cmbx-10x-x-109">MA </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-5-2"  
class="td11"><span 
class="cmbx-10x-x-109">M</span>oving <span 
class="cmbx-10x-x-109">A</span>verage                                      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-6-1"  
class="td11"><span 
class="cmbx-10x-x-109">ARMA </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-6-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>uto <span 
class="cmbx-10x-x-109">R</span>egressive <span 
class="cmbx-10x-x-109">M</span>oving <span 
class="cmbx-10x-x-109">A</span>verage               </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-7-1"  
class="td11"><span 
class="cmbx-10x-x-109">ARIMA</span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-7-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>uto <span 
class="cmbx-10x-x-109">R</span>egressive <span 
class="cmbx-10x-x-109">I</span>ntegrated <span 
class="cmbx-10x-x-109">M</span>oving <span 
class="cmbx-10x-x-109">A</span>verage</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-8-1"  
class="td11"><span 
class="cmbx-10x-x-109">ADF </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-8-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>ugmented <span 
class="cmbx-10x-x-109">D</span>ickey <span 
class="cmbx-10x-x-109">F</span>uller                         </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-9-1"  
class="td11"><span 
class="cmbx-10x-x-109">SVM </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-9-2"  
class="td11"><span 
class="cmbx-10x-x-109">S</span>upport <span 
class="cmbx-10x-x-109">V</span>ector <span 
class="cmbx-10x-x-109">M</span>achine                          </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-10-1"  
class="td11"><span 
class="cmbx-10x-x-109">SGD </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-10-2"  
class="td11"><span 
class="cmbx-10x-x-109">S</span>tochastic <span 
class="cmbx-10x-x-109">G</span>radient <span 
class="cmbx-10x-x-109">D</span>escent                      </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-11-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-11-1"  
class="td11"><span 
class="cmbx-10x-x-109">ADF </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-11-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>ugmented <span 
class="cmbx-10x-x-109">D</span>ickey <span 
class="cmbx-10x-x-109">F</span>uller                         </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-12-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-12-1"  
class="td11"><span 
class="cmbx-10x-x-109">SMA </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-12-2"  
class="td11"><span 
class="cmbx-10x-x-109">S</span>imple <span 
class="cmbx-10x-x-109">M</span>oving <span 
class="cmbx-10x-x-109">A</span>verage                            </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-13-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-13-1"  
class="td11"><span 
class="cmbx-10x-x-109">AIC </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-13-2"  
class="td11"><span 
class="cmbx-10x-x-109">A</span>lkaline <span 
class="cmbx-10x-x-109">I</span>nformation <span 
class="cmbx-10x-x-109">C</span>riterion                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-14-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-14-1"  
class="td11"><span 
class="cmbx-10x-x-109">IC </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-14-2"  
class="td11"><span 
class="cmbx-10x-x-109">I</span>nformation <span 
class="cmbx-10x-x-109">C</span>riterion                               </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-15-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-15-1"  
class="td11"><span 
class="cmbx-10x-x-109">NUTS </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-15-2"  
class="td11"><span 
class="cmbx-10x-x-109">N</span>o-<span 
class="cmbx-10x-x-109">U</span>-<span 
class="cmbx-10x-x-109">T</span>urn <span 
class="cmbx-10x-x-109">S</span>ampler                                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-16-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-16-1"  
class="td11"><span 
class="cmbx-10x-x-109">QQ </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-16-2"  
class="td11"><span 
class="cmbx-10x-x-109">Q</span>uantile-<span 
class="cmbx-10x-x-109">Q</span>uantile                                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-17-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-17-1"  
class="td11"> </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-18-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-18-1"  
class="td11"> </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-19-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-19-1"  
class="td11"> </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-20-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-20-1"  
class="td11"></td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-20-2"  
class="td11">
</td></tr>
</table></div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 170--><p class="noindent" >
<!--l. 170--><p class="noindent" ><span 
class="cmsl-12x-x-120">To my parents, Keith &amp; Christine Gauci Maistre. Without them,</span>
<span 
class="cmsl-12x-x-120">and their unconditional love and support, none of this would have</span>
<span 
class="cmsl-12x-x-120">been possible.</span></div>
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;1</span><br /><a 
 id="x1-50001"></a>Introduction</h2>
<!--l. 3--><p class="noindent" >&#8221;Investing in stocks is just like gambling&#8221; - is a phrase commonly heard coming
from people describing stock investing. But is it really just like gambling? Were all
those investors who made billions from the stock market just lucky? To understand
this, we must first review what it means to buy stocks. In the stock market, buying
stocks means owning a share of the company. It entitles the stock holder to a claim
on assets as well as a fraction of the profits which the company generates. It is
unfortunately very common for investors to misunderstand this concept, often thinking of
shares as simply a trading vehicle and forget that stock represents the ownership of a
company.
<!--l. 5--><p class="noindent" >So how are stocks valued? The value of a companys stock depends on a number of
factors, and assessing the value is not an easy practice. Investors are constantly trying
to assess the profit that will be left over to shareholders. This is why stock prices
fluctuate. The outlook for business conditions is always changing, and so are the future
earnings of a company. Since many people fail to understand this concept, it is far too
common to believe that the short-term price movements of a company are random.
However, it is the long-term price movements of a company which reflect the value of a
company as it is supposed to be worth the present value of the profits it will make. A
company can survive without profits in the short term, as long as expectations of future
earnings exist. A company may try to fool investors in the beginning, however a
companys stock price will eventually be expected to show the true value of the
firm.
<!--l. 7--><p class="noindent" >So how is this all different to gambling? Gambling is a zero-sum game. It merely takes money
from a loser and gives it to a winner. No value is ever created. By investing, we increase the
overall wealth of an economy. As companies compete, they increase productivity and develop
products that can make our lives better. This does not mean that stock investing cannot be
a gamble, as it is extremely common for many people to skip the due diligence
before spending a huge chunk of their life savings on stock, often losing it all in the
process.
<!--l. 9--><p class="noindent" >This report aims to disprove three hypotheses, the Efficient Market Hypotheses (EMH), the
Zero-Sum Game theory, and the Random Walk Hypothesis.
<h3 class="sectionHead"><span class="titlemark">1.1 </span> <a 
 id="x1-60001.1"></a>Efficient Market Hypothesis - EMH</h3>
<!--l. 13--><p class="noindent" >In financial economics, the efficient market hypothesis (EMH) is an investment theory which
states it is impossible for an investor to &#8221;beat the market&#8221; because stock market
                                                                                
                                                                                
efficiency causes existing share prices to always incorporate and reflect all relevant
information. In accordance to the EMH, stocks always trade at their fair value on stock
exchanges and only react to new information or charges in discount rates, making it
impossible for investors to make a profit by either purchasing undervalued stocks or
selling stocks for inflated prices. This means that as such, it should be impossible to
outperform the overall market through expert stock selection or market timing, and the
only way an investor can possibly obtain higher returns is by purchasing riskier
investments.
<!--l. 15--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">1.1.1 </span> <a 
 id="x1-70001.1.1"></a>Breaking down EMH</h4>
<!--l. 16--><p class="noindent" >Although it is a cornerstone of modern financial theory, the EMH is highly controversial
and often disputed. Believers argue it is pointless to search for undervalued stocks
or to try to predict trends in the market through either fundamental or technical
analysis.
<!--l. 18--><p class="noindent" >While academics point to a large body of evidence in support of EMH, an equal amount of
dissension also exists. For example, investors such as Warren Buffett have consistently beaten
the market over long periods of time, which in itself is impossible by definition according to
the EMH. Detractors of the EMH also point to events such as the 1987 stock market
crash, when the Dow Jones Industrial Average (DJIA) fell by over 20% in a single
day, which was clear evidence that stock prices can seriously deviate from their fair
values.
<!--l. 20--><p class="noindent" >Proponents of the EMH conclude that, because of the randomness of the market, investors
would be better off by investing in a low-cost, passive portfolio such as one comprising of
various low risk index funds. Data compiled by Morningstar Inc. through its June 2015
Active/Passive Barometer study supports the conclusion. Morningstar compared active
managers returns in all categories against a composite made of related index funds
and exchange-traded funds (ETFs). The study found that year-over-year, only two
groups of active managers successfully outperformed passive funds more than 50% of
the time. These were U.S. small growth funds and diversified emerging markets
funds.
<!--l. 22--><p class="noindent" >In all of the other categories, including U.S. large blend, U.S. large value, and U.S. large
growth, among others, investors would have fared better by investing in low-cost index funds
or ETFs. While a percentage of active managers do outperform passive funds at some point,
                                                                                
                                                                                
the challenge for investors is being able to identify which ones will do so. Less than 25% of the
top-performing active managers are able to consistently outperform their passive manager
counterparts.
<!--l. 24--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">1.2 </span> <a 
 id="x1-80001.2"></a>Zero-Sum Game</h3>
<!--l. 25--><p class="noindent" >Zero-sum, not to be confused with Empty sum, or Zero game, is a mathematical
representation of a situation found in game theory and economic theory, in which one persons
gain is equivalent to anothers loss, so the net change in wealth or benefit is zero. A zero-sum
game may have as few as two players, or millions of participants.
<!--l. 27--><p class="noindent" >Zero-sum games are found in game theory, but are less common than non-zero sum games.
Poker and gambling are popular examples of zero-sum games since the sum of the
amounts won by some players equals the combined losses of the others. Games such
as chess and tennis, in which there is one winner and one loser, are also zero-sum
games.
<!--l. 29--><p class="noindent" >It is important to note that the stock market overall is often considered a zero-sum game,
which is a misconception, along with other popular misunderstandings. Historically and in
contemporary culture the stock market is often equated with gambling, which is definitely a
zero-sum game. When an investor buys a stock, it is a share of ownership of a company that
entitles that investor to a fraction of the company&#8217;s profits. The value of a stock can go up or
down depending on the economy and a host of other factors, but ultimately, ownership of that
stock will eventually result in a profit or a loss that is not based on chance or the guarantee of
someone else&#8217;s loss. In contrast, gambling means that somebody wins the money of another
who loses it.
<!--l. 31--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">1.3 </span> <a 
 id="x1-90001.3"></a>Random Walk Hypothesis</h3>
<!--l. 32--><p class="noindent" >The random walk theory suggests that stock price changes have the same distribution and are
independent of each other, so the past movement or trend of a stock price or market cannot be
used to predict its future movement. In short, this is the idea that stocks take a random and
unpredictable path.
                                                                                
                                                                                
<!--l. 34--><p class="noindent" >A follower of the random walk theory believes it&#8217;s impossible to outperform the
market without assuming additional risk. Critics of the theory, however, contend
that stocks do maintain price trends over time  in other words, that it is possible
to outperform the market by carefully selecting entry and exit points for equity
investments.
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;2</span><br /><a 
 id="x1-100002"></a>Background Theory</h2>
<!--l. 3--><p class="noindent" >Computational finance is a branch of applied computer science that deals with problems of
practical interest in finance. Some slightly different definitions are the study of data and
algorithms currently used in finance and the mathematics of computer programs that realize
financial models or systems. Using computational finance in order to allocate assets in
a portfolio is not at all unheard of and was first documented 1952.[<a 
href="#XMarkowitz:1952aa">1</a>] Markowitz
first introduced the concept of portfolio selection as an exercise in mean-variance
optimisation. This required more computer power than was available at the time, so he
worked on useful algorithms for approximate solutions. He theorised that risk-averse
investors could construct portfolios to optimise or maximise expected return based on
a given level of market risk, emphasising that risk is an inherent part of higher
reward. According to his theory, it&#8217;s possible to construct an &#8221;efficient frontier&#8221; of
optimal portfolios offering the maximum possible expected return for a given level of
risk.
<h3 class="sectionHead"><span class="titlemark">2.1 </span> <a 
 id="x1-110002.1"></a>Time Series Analysis</h3>
<!--l. 7--><p class="noindent" >A time series is a series of data points which may be indexed, listed, or graphed, in a time
order. Most commonly, a time series is a sequence taken at successive equally spaced points in
time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean
tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.
Brockwell et al provide a formal description of time series as having a set of observations xt,
each one being recorded at a specific time t. A discrete-time time series (the type
to which this book is primarily devoted) is one in which the set T0 of times at
which observations are made is a discrete set, as is the case, for example, when
observations are made at fixed time intervals. Continuous time series are obtained when
observations are recorded continuously over some time interval, e.g., when T0 = [0,
1].[<a 
href="#XPeter-J.-Brockwell:2016aa">2</a> ]
<!--l. 9--><p class="noindent" >Significant &#8221;time series momentum&#8221; has also been documented in equity index, currency,
commodity, and bond futures for each of the 58 liquid instruments they consider.[<a 
href="#XMoskowitz:2011aa">3</a>] They find
persistence in returns for 1 to 12 months that partially reverses over longer horizons,
consistent with sentiment theories of initial under-reaction and delayed over-reaction. A
diversified portfolio of time series momentum strategies across all asset classes delivers
substantial abnormal returns with little exposure to standard asset pricing factors and
performs best during extreme markets. Examining the trading activities of speculators and
hedgers, they find that speculators profit from time series momentum at the expense of
hedgers.
                                                                                
                                                                                
<!--l. 11--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.1.1 </span> <a 
 id="x1-120002.1.1"></a>Auto Regressive Moving Average (ARMA)</h4>
<!--l. 13--><p class="noindent" >In the statistical analysis of time series, autoregressivemoving-average (ARMA) models
provide a parsimonious description of a weakly stationary stochastic process in terms of two
polynomials, one for the autoregression and the second for the moving average. The notation
ARMA(p, q) refers to the model with p autoregressive terms and q moving-average terms.
This model contains the AR(p) and MA(q) models,
<!--l. 15--><p class="noindent" ><img 
src="Thesis0x.png" alt="&#x2211;q
xt=c+&#x03B5;t+i=1&#x03C6;iXt- i&#x03B8;i&#x03B5;t- i  "  class="math" >
<!--l. 17--><p class="noindent" >Forecasting interest rates is of great concern for financial researchers, economists, and players
in the fixed income markets. A study was carried out to develop an appropriate model for
forecasting the short-term interest rates, implicit yield on 91 day treasury bill, overnight
MIBOR rate, and call money rate.[<a 
href="#XRadha:2015aa">4</a>] The short-term interest rates are forecasted
using univariate models such as the Random Walk, ARIMA, ARMA-GARCH, and
ARMA-EGARCH. The appropriate model for forecasting is determined considering a six-year
period from 1999. The results show that interest rates time series have volatility clustering
effect and hence GARCH based models are more appropriate to forecast than the other
models. Radha et al found that for commercial paper rate ARIMA-EGARCH model is the
most appropriate model, while for implicit yield 91 day Treasury bill, overnight MIBOR rate,
and call money rate, the ARIMA-GARCH model is the most appropriate model for
forecasting.
<!--l. 19--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.1.2 </span> <a 
 id="x1-130002.1.2"></a>Auto Regressive Integrated Moving Average (ARIMA)</h4>
<!--l. 21--><p class="noindent" >In time series analysis, an autoregressive integrated moving average (ARIMA) model
is a generalization of an ARMA model. Given a time series of data <span 
class="cmmi-10x-x-109">X</span><sub><span 
class="cmmi-8">t</span></sub> where t
is an integer index and the <span 
class="cmmi-10x-x-109">X</span><sub><span 
class="cmmi-8">t</span></sub> are real numbers, an ARMA(p, q) model is given
by
<!--l. 23--><p class="noindent" ><img 
src="Thesis1x.png" alt="x-&#x03B1;X-...&#x03B1; &#x2032;X   &#x2032; = &#x03B5; + &#x03B8; &#x03F5;   + &#x22C5;&#x22C5;&#x22C5;+ &#x03B8; &#x03F5;
t1t-1   p  t-p    t    1t-1         qt-q  "  class="math" >
<!--l. 25--><p class="noindent" >The aforementioned model is fitted to time series data either to better understand the data or
to predict future points in the series (forecasting). ARIMA models are applied in some cases
                                                                                
                                                                                
where data show evidence of non-stationarity, where an initial differencing step
(corresponding to the &#8221;integrated&#8221; part of the model) can be applied to reduce the
non-stationarity.
<!--l. 27--><p class="noindent" >The existence of weak-form efficiency in the Russian stock market is examined for
the period 1st September 1995 to 1st May 2001 using daily, weekly and monthly
Russian Trading System index time series.[<a 
href="#XAbrosimova:2002aa">5</a>] Several different approaches are used to
assess the predictability of the RTS index time series. Unit root, autocorrelation and
variance ratio tests are conducted for the null hypothesis of a random walk model. The
results support the null hypothesis for the monthly data only. Further analysis is
performed for the daily and weekly data. Linear and non-linear modelling of the serial
dependence is conducted using ARIMA and GARCH models estimated on the in-sample
period 1st September 1995 to 1st January 2001. Forecasts based on the best fitting
models are performed for the out-of-sample period 2nd January 2001 to 1st May
2001. Comparisons of the forecasts reveal that none of the models outperforms the
others, and the most accurate forecasts are obtained for just the first out-of-sample
observation. Whilst our research results provide some limited evidence of short-term
market predictability on the RTS, there is insufficient evidence to suggest that it
would lead to a profitable trading rule, once transaction costs and risk are taken into
account.
<!--l. 29--><p class="noindent" >The main intention of this paper is to investigate, with new daily data, whether prices in the
two Chinese stock exchanges (Shanghai and Shenzhen) follow a random-walk process
as required by market efficiency.[<a 
href="#XDarrat:2001aa">6</a>] We use two different approaches, the standard
variance-ratio test of Lo and MacKinlay (1988) and a model-comparison test that
compares the ex post forecasts from a NAIVE model with those obtained from several
alternative models (ARIMA, GARCH and Artificial Neural Network-ANN). To
evaluate ex post forecasts, we utilize several procedures including RMSE, MAE,
Theil&#8217;s U, and encompassing tests. In contrast to the variance-ratio test, results from
the model-comparison approach are quite decisive in rejecting the random-walk
hypothesis in both Chinese stock markets. Moreover, our results provide strong support
for the ANN as a potentially useful device for predicting stock prices in emerging
markets.
<!--l. 31--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">2.2 </span> <a 
 id="x1-140002.2"></a>Statistical Machine Learning</h3>
                                                                                
                                                                                
<!--l. 33--><p class="noindent" >A machine learning algorithm is an algorithm that is able to learn through examples from
data. To understand what an algorithm is, Cormen et al informally describe algorithms as
&#8221;any well-defined computational procedures which takes some value, or set of values, as input
and produce some value, or set of values, as output. An algorithm is thus a sequence of
computational steps that transform the input into the output.&#8221;[<a 
href="#XCormen:2009aa">7</a>] In simple terms, it
is possible to say that an algorithm is a sequence of steps which allow to solve a
certain task. Similarly to a normal algorithm, a machine learning algorithm as defined
formally by Tom M. Mitchell, states that &#8221;A computer program is said to learn
from experience E with respect to some class of tasks T and performance measure
P, if its performance at tasks in T, as measured by P, improves with experience
E.&#8221;[<a 
href="#XMitchell:1997aa">8</a> ]
<!--l. 35--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.1 </span> <a 
 id="x1-150002.2.1"></a>Classification</h4>
<!--l. 37--><p class="noindent" >Classifiers are a class of machine learning algorithms which identify in which category a new
observation belongs to, on the basis of a training set of data containing observations
whose category membership is known. A model based on discriminant analysis was
sought out to categorise a stock as manipulated or non-manipulated based on certain
key variables that capture the characteristics of the stock.[<a 
href="#XMurugesan:2012aa">9</a>] The model in which
Murugesan et al chose, helps them identify stocks witnessing activities that are
indicative of potential manipulation irrespective of the type of manipulation, such as
action-based, information-based, or trade-based. The model which they proposed, helps
investigators to arrive at a shortlist of securities that are potentially manipulated
and which could be subject to further detailed investigation to detect the type and
nature of the manipulation, if any. In a market like India, where there are about
5000 plus securities listed on its major exchanges, it becomes extremely difficult to
monitor all securities for potential market abuse. Academics who have earlier used
discriminant analysis have used the Linear Classification Function without validating the
assumption that governs the model. Through their research, they have tested the
assumption on data from the Indian capital market and found that the data does
not comply with the assumptions that govern the use of the linear classification
function. This therefore resulted in them using the Quadratic Classification Function,
which is the appropriate technique for instances where the data does not meet the
sated assumptions, to categorise stocks into two categories, namely manipulated and
non-manipulated.
                                                                                
                                                                                
<!--l. 39--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.2 </span> <a 
 id="x1-160002.2.2"></a>Support Vector Machines</h4>
<!--l. 41--><p class="noindent" >Support vector machines (SVM), are a class of machine learning algorithms that have become
incredibly popular in the past few years. SVMs are very similar to classifiers in the sense that
they also classify data by drawing a line, called a decision boundary, to separate
them. However, SVMs go a step further by calculating a vector from the data point
with the smallest margin to the decision boundary. This is called a support vector.
There exists vast research articles which predict the stock market as well pricing of
stock index financial instruments but most of the proposed models focus on the
accurate forecasting of the levels of the underlying stock index. There is a lack of
studies examining the predictability of the direction of stock index movement. Given
the notion that a prediction with little forecast error does not necessarily translate
into capital gain, the authors of this research attempt to predict the direction of
the S&amp;P CNX NIFTY Market Index of the National Stock Exchange, one of the
fastest growing financial exchanges in developing Asian countries.[<a 
href="#XKumar:2016aa">10</a>] Random forest
and Support Vector Machines (SVM) are very specific type of machine learning
method, and are promising tools for the prediction of financial time series. The tested
classification models, which predict direction, include linear discriminant analysis,
logit, artificial neural network, random forest, and SVM. Empirical experimentation
suggests that the SVM outperforms the other classification methods in terms of
predicting the direction of the stock market movement and random forest method
outperforms neural network, discriminant analysis and logit model used in their
study.
<!--l. 43--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.3 </span> <a 
 id="x1-170002.2.3"></a>Regression</h4>
<!--l. 45--><p class="noindent" >Regression analysis is widely used for prediction and forecasting to understand which among
the independent variables are related to the dependent variable while also exploring the forms
of these relationships. In restricted circumstances, regression analysis can be used to infer
causal relationships between the independent and dependent variables. Regression analysis
helps one understand how the typical value of the dependent variable changes when any one of
the independent variables is varied, while the other independent variables are held fixed.
                                                                                
                                                                                
Kakushadze et al provide a systematic quantitative framework in what is intended to
be a pedagogical fashion for discussing mean-reversion and optimisation.[<a 
href="#XKakushadze:2015aa">11</a>] In
their paper, they start off their research with pair trading and add complexity by
following the sequence mean-reversion via demeaning, regression, weighted regression,
(constrained) optimization, factor models. They discuss in further detail how to conduct
mean-reversion based on this approach, including common pitfalls encountered in
practical applications, such as the difference between maximising the Sharpe ratio and
minimising an objective function when trading costs are included. Kakushadze et al
also discuss explicit algorithms for optimization with linear costs, constraints and
bounds, and also illustrate their discussion on an explicit intraday mean-reversion
alpha.
<!--l. 47--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.2.4 </span> <a 
 id="x1-180002.2.4"></a>Decision Trees</h4>
<!--l. 49--><p class="noindent" >Decision tree learning uses a decision tree as a predictive model which maps observations
about an item, represented in the branches, to conclusions about the item&#8217;s target
value represented in the leaves. Tree models where the target variable can take
a finite set of values are called classification trees; in these tree structures, leaves
represent class labels and branches represent conjunctions of features that lead to those
class labels. Decision trees where the target variable can take continuous values,
typically real numbers, are called regression trees. Creamer et al propose a multi-stock
automated trading system which relies on a layered structure consisting of a machine
learning algorithm, an online learning utility, and a risk management overlay.[<a 
href="#XCreamer:2010aa">12</a>] An
alternating decision tree (ADT), which is implemented with Logitboost, was chosen
as their underlying algorithm. One of the strengths of their approach is that the
algorithm is able to select the best combination of rules derived from well-known
technical analysis indicators and is also able to select the best parameters of the
technical indicators. Additionally, their online learning layer combines the output of
several ADTs and suggests a short or long position. Finally, the risk management
layer in which they implemented, can validate the trading signal when it exceeds a
specified non-zero threshold and limit the application of their trading strategy when
it is not profitable. They tested the expert weighting algorithm with data of 100
randomly selected companies of the S&amp;P 500 index during the period 20032005. They
found that their algorithm generates abnormal returns during the test period. Their
experiments show that the boosting approach was able to improve the predictive capacity
when indicators were combined and aggregated as a single predictor. Even more, the
                                                                                
                                                                                
combination of indicators of different stocks demonstrated to be adequate in order to
reduce the use of computational resources, and still maintain an adequate predictive
capacity.
<!--l. 51--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">2.3 </span> <a 
 id="x1-190002.3"></a>Bayesian Statistics</h3>
<!--l. 53--><p class="noindent" >Bayesian Statistics, a form of probabilistic programming, describes probabilistic models and
then performs inference in those models. Probabilistic reasoning is a foundational technology
of machine learning and has been used by companies such as Google, Amazon, and Microsoft.
Probabilistic reasoning has been used for predicting stock prices, recommending
movies, diagnosing computers, detecting cyber intrusions, and image detection.
Gelman et al defines bayesian inference as the process of fitting a probability model
to a set of data and summarising the result by a probability distribution on the
parameters of the model and on unobserved quantities such as predictions for new
observations.[<a 
href="#XGelman:2014aa">13</a>]
<!--l. 55--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">2.3.1 </span> <a 
 id="x1-200002.3.1"></a>Markov Chain Monte Carlo (MCMC)</h4>
<!--l. 57--><p class="noindent" >In statistics, (MCMC) methods are a class of algorithms for sampling from a probability
distribution based on constructing a Markov chain that has the desired distribution of its
equilibrium distribution. The state of the chain after a number of steps is then used as a
sample of the desired distribution. The quality of the sample improves as a function of the
number of steps.
<!--l. 59--><p class="noindent" >A 1993 paper was prepared for the purpose of presenting the methodology and uses of the
Markov Chain Monte Carlo simulation technique as applied in the evaluation of investment
projects to analyse and assess risk.[<a 
href="#XSavvides:1994aa">14</a>] In the first part of his paper, he highlights the
importance of risk analysis in investment appraisal. The author follows by presenting the
various stages in the application of the risk analysis process and examines the interpretation of
the results generated by a risk analysis application including investment decision criteria and
various measures of risk based on the expected value concept. In the final part of his paper, he
draws some conclusions regarding the usefulness and limitations of risk analysis in investment
appraisal.
                                                                                
                                                                                
<!--l. 61--><p class="noindent" >Stochastic volatility models are increasingly important in practical derivatives pricing
applications, yet relatively little work has been undertaken in the development of practical
Monte Carlo simulation methods for this class of models. This paper considers several new
algorithms for time-discretization and Monte Carlo simulation of Heston-type stochastic
volatility models.[<a 
href="#XAndersen:2007aa">15</a>] The algorithms are based on a careful analysis of the properties of affine
stochastic volatility diffusions, and are straightforward and quick to implement and execute.
Tests on realistic model parameterizations reveal that the computational efficiency and
robustness of the simulation schemes proposed in the paper compare very favourably to
existing methods.
<!--l. 63--><p class="noindent" >Bond yields today are well below and stock market valuations are well above their historical
average.[<a 
href="#XBlanchett:2013aa">16</a> ] There are no historical periods in the United States where comparable low bond
yields and high equity valuations have occurred simultaneously. Both current bond
yields and stock values have been shown to predict near-term returns. Portfolio
returns in the first decade of retirement have an outsize impact on retirement income
strategies. Traditional Monte Carlo simulation approaches generally do not incorporate
market valuations into their analysis. In order to simulate how retirees will fare in
a low return environment for both stocks and bonds, Blanchett et al incorporate
the predictive ability of current valuations to simulate its impact on retirement
portfolios. Blanchett et al estimate bond returns through an autoregressive model that
uses an initial bond yield value where yields drift in the future. Blanchett et al use
the cyclically adjusted price-to-earnings (CAPE) ratio as an estimate of market
valuation to predict short-run stock performance. Our simulations indicate that the
safety of a given withdrawal strategy is significantly affected by the initial bond yield
and CAPE value at retirement, and that the relative impact varies based on the
portfolio equity allocation. Using valuation measures current as of April 15, 2013, which
is a bond yield of 2.0% and a CAPE of 22, Blanchett et al find the probability
of success for a 40% equity allocation with a 4% initial withdrawal rate over a 30
year period is approximately 48%. This success rate is materially lower than past
studies and has sobering implications on the likelihood of success for retirees today, as
well as how much those near retirement may need to save to ensure a successful
retirement.
<!--l. 65--><p class="noindent" >This paper provides a numerical approach based on a Monte Carlo simulation for valuing
dynamic capital budgeting problems with many embedded real options dependent on
numerous state variables.[<a 
href="#XGamba:2003aa">17</a>] Schwartz et al propose a way of decomposing a complex capital
budgeting problem with many options into a set of simple options, suitably accounting for
interaction and interdependence among them. The decomposition approach is numerically
implemented using an extension of the Least Squares Monte Carlo algorithm, presented by
Longstaff and Schwartz (2001) applied to our multi-option setting. Schwartz et al
                                                                                
                                                                                
also provide a number of applications of our approach to well-known real options
models and real life capital budgeting problems. Moreover, Schwartz et al present a
set of numerical experiments to provide evidence for the accuracy of the proposed
methodology.
<!--l. 67--><p class="noindent" >Hoffman et al introduce the &#8217;No-U-Turn-Sampler&#8217;, an extension to the Hamiltonian Monte
Carlo (HMC), which is an MCMC algorithm that avoids the random walk behavioir and
sensitivity to correlated paramaters that plague many MCMC methods by taking a
series of steps informed by first-order gradient information.[<a 
href="#XMatthew-D.-Hoffman:2014aa">18</a>] In their paper, they
claim HMCs performance to be highly sensitive to two user-specified parameters: a
step size, and a desired number of steps. NUTS is an improvement from HMC as
it eliminates the need to set a number of steps. Their algorithm uses a recursive
algorithm to build a set of likely candidate points that spans a wide swath of the target
distribution, stopping automatically when it starts to double back and retrace its
steps.
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;3</span><br /><a 
 id="x1-210003"></a>Experminetal Setup</h2>
<h3 class="sectionHead"><span class="titlemark">3.1 </span> <a 
 id="x1-220003.1"></a>Data Tidying</h3>
<!--l. 4--><p class="noindent" >A data set containing end of day stock prices, dividends, and splits for 3,000 US companies,
curated by the Quandl community, and released into the public domain, was used. The date
column in the CSV file was loaded into memory, and said column was converted to a date data
type. The DataFrame was then sorted using the date column, starting from the oldest date,
ending with the latest. The date column was also set to the index. The DataFrame was split
into two, the training data set consisting of 80%, and the test data set consisting of 20% of the
original DataFrame.
<!--l. 6--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.2 </span> <a 
 id="x1-230003.2"></a>Stock Selection</h3>
<!--l. 7--><p class="noindent" >The pairwise correlation of all the columns in the DataFrame was computing and stored in a
new DataFrame. A list of stock pairs with low correlation were extracted.
<!--l. 9--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.3 </span> <a 
 id="x1-240003.3"></a>Time Series Analysis</h3>
<!--l. 10--><p class="noindent" >The selected stocks was extracted from the data set and stored in a DataFrame. The log
returns of the stocks were calculated by calculating the logarithm of the stock&#8217;s adjusted close
price divided by the following day&#8217;s adjusred close price. The resulting values from the
calculation were then stored in a new column in the DataFrame and all infinite values were
dropped from the series.
<!--l. 12--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.1 </span> <a 
 id="x1-250003.3.1"></a>Random Walk</h4>
<!--l. 13--><p class="noindent" >The first difference of the stocks were calculated abd stored in a new column in the Dataframe
and all infinite valued were dropped.
                                                                                
                                                                                
<!--l. 15--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.2 </span> <a 
 id="x1-260003.3.2"></a>Ordinary Least Squares (OLS)</h4>
<!--l. 16--><p class="noindent" >The series was fitted to an OLS model. The 15 day SMA was used as a nobs x k array where
nobs is the number of observations and k is the number of regressors, to train the model, while
the adjusted close price of the stock was used as the dependent variable for the model to
predict. The mean absolute error, mean squared error, median absolute error, and r2 score
were used as metrics in order to rank the performance of the model&#8217;s prediction capabilities in
in-sample testing.
<!--l. 18--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.3 </span> <a 
 id="x1-270003.3.3"></a>Auto Regressive (AR)</h4>
<!--l. 19--><p class="noindent" >The series was fitted to an AR(p) model with a maximum lag value of 30. The IC was used
to fit the AR model in order to select the optimal lag length. No constants were
passed when fitting the AR model to the series. The optimal lag for the fit of the AR
model was then calculated using the same paramaeters passed when fitting the AR
model.
<!--l. 21--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.4 </span> <a 
 id="x1-280003.3.4"></a>Moving Average (MA)</h4>
<!--l. 22--><p class="noindent" >The series was fitted to an MA(p, q) model with an order selected based on the lowest AIC. A
maximum lag of 30 was once again passed to the MA model with no constant. The
exact loglikelihood for the fit of the MA model was maximized via the Kalman
Filter.
<!--l. 24--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.5 </span> <a 
 id="x1-290003.3.5"></a>Auto Regressive Moving Average (ARMA)</h4>
                                                                                
                                                                                
<!--l. 25--><p class="noindent" >The series was fitted to an ARMA(p, q, r) model with an order selected based on the lowest
AIC. No constants were passed to the ARMA model. The exact loglikelihood for the fit of the
ARMA model was maximized via the Kalman Filter.
<!--l. 27--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.3.6 </span> <a 
 id="x1-300003.3.6"></a>Auto Regressive Integrated Moving Average (ARIMA)</h4>
<!--l. 28--><p class="noindent" >The series was fitted to an ARIMA(p, q, r) model with an order selected based on the lowest
AIC. No constants were passed to the ARIMA model. The exact loglikelihood for the fit of the
ARIMA model was maximized via the Kalman Filter.
<!--l. 30--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.4 </span> <a 
 id="x1-310003.4"></a>Machine Learning</h3>
<!--l. 31--><p class="noindent" >The data set was split for training and testing purposes when fitting and predicting data. The
training data set consisted of 80% of the whole data set, while the test data set consisted of
20%.
<!--l. 33--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.4.1 </span> <a 
 id="x1-320003.4.1"></a>Classification</h4>
<!--l. 34--><p class="noindent" >2, 3, 4, 5, and 6 day SMAs were used as the features to train the models, while a binary value
used to determine whether the current day&#8217;s adjusted close has risen or not from the previous
day was used as the target valued for the model to predict. In-sample testing was carried out
using the models predict function, passing the test data set&#8217;s features in order to predict the
output. The classification report and confusion matrix were used as metrics in order to rank
the performance of the model&#8217;s prediction capabilities in in-sample testing. An algorithm was
developed for out-of-sample testing. The algorithm iterates for a number of n steps, increasing
the index by 1 each step, forecasting the next day&#8217;s outcome, and calculating the
SMAs.
                                                                                
                                                                                
<!--l. 36--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.1 </span> <a 
 id="x1-330003.4.1.1"></a>Decision Tree</h5>
<!--l. 37--><p class="noindent" >The decision tree classifier was fit using the Gini impurity criterion to measure the quality of
the split. The model was given the liberty to select the best strategy in order to split the
tree at each node. The maximum allowed depth of the tree was left unrestricted,
which was the same as for the maximum leaf nodes. A threshold of 1e-7 was used
to terminate the tree growth to determine if a node is a leaf, if the impurity of a
node is below the threshold, the node is a leaf. The minimum number of samples
required to be at a leaf node was set to 1, while the minimum number of samples
required to split an internal node was set to 2. The data fitted to the model was not
pre-sorted, and the random number generator used by the model was that of Numpy&#8217;s
RandomState.
<!--l. 39--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.2 </span> <a 
 id="x1-340003.4.1.2"></a>Boosted Decision Tree</h5>
<!--l. 40--><p class="noindent" >The boosted decision tree was fit using the &#8217;SAME.R&#8217; real algorithm which converges at a
faster rate, achieving a lower test error with fewer boosting iterations. The maximum number
of estimators at which boosting is terminated was set to 50; in case of perfect fit, the learning
procedure is stopped early. The learning rate which is the contribution of each classifier of the
model was shrunk by 1. The best estimator used to fit the data to the model was a Decision
Tree Classifier, and the random number generator used by the model was that of Numpy&#8217;s
RandomState.
<!--l. 42--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.3 </span> <a 
 id="x1-350003.4.1.3"></a>Support Vector Machine (SVM)</h5>
<!--l. 43--><p class="noindent" >The C-Support Vector Classification implementation was used for the SVM model with a 0
penalty parameter of the error term. A kernal type of &#8217;rbf&#8217; was used when fitted the model to
the data, along with a polynomial kernel function degree of 3. The gama &#8217;rbf&#8217; Kernel
coefficient was calculated by dividing the number of features by 1, while no probability
estimates were used when fitting. A shrinking heuristic was used and a tolerance of 1e-3 was
used for stopping criterion. A cache size of 200MB was used for the kernel when
fitting the model, and all classes were assigned a weight of one. Verbose output
                                                                                
                                                                                
was not used, and the random number generator used by the model was that of
Numpy&#8217;s RandomState. No limits were set on the iterations within the solver, and a
one-vs-rest decision function of shape (number of samples, number of classes) was
returned.
<!--l. 45--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.4 </span> <a 
 id="x1-360003.4.1.4"></a>Random Forest</h5>
<!--l. 46--><p class="noindent" >The random forest was fit with bootstrap samples when building the trees, while all the
weights associated were set to 1. The &#8217;gini&#8217; function to measure the quality of a split, and no
maximum depth of the tree was set, allowing the nodes to expand until all leaves are pure or
until all leaves contain less than the minimum split samples. The number of features to
consider when looking for the best split was the square root of the number of passed,
and no limit on the maximum leaf nodes for growing trees was set. A threshold
of 1e-7 was used to terminate the tree growth to determine if a node is a leaf, if
the impurity of a node is below the threshold, the node is a leaf. The minimum
number of samples required to be at a leaf node was set to 1, and the minimum
number of samples required to split an internal node was set to 2. The minimum
weighted fraction of the sum total of weights (of all the input samples) required to be
at a leaf node was set to 0, and the number of trees in the forest was set to 10.
The number of jobs to use for the computation was set to 1, making use of only 1
CPU core, and out-of-bag samples to estimate the generalization accuracy were
not used. The verbosity of the tree building process was not controlled, and the
random number generator used by the model was that of Numpy&#8217;s RandomState.
The model was built using a cold start by not making use of the previous call to
fit and add more estimators to the ensemble, meaning a whole new forest was fit
instead.
<!--l. 48--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.5 </span> <a 
 id="x1-370003.4.1.5"></a>K-Nearest Neighbour</h5>
<!--l. 49--><p class="noindent" >The k-nearest neighbour was fit with a number of 5 neighbors to use for k-neighbours queries,
with a uniform weight making all points in each neighbourhood weighted equally when
carrying out predictions. The model was left at liberty to select the most appropriate
algorithm based on the values passed when fitting the model to the data. The lead size of the
                                                                                
                                                                                
model was set to 30, and a minkowski distance metric with p equal to 2 which equivalent to
the standard Euclidean metric. The number of jobs to use for the computation was set to 1,
making use of only 1 CPU core.
<!--l. 51--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.6 </span> <a 
 id="x1-380003.4.1.6"></a>Logistic Regression</h5>
<!--l. 52--><p class="noindent" >The logistic regression was fit with an inverse of regularization of strength 0 specifying
stronger regularisation, while all the weights associated were set to 1. Dual formulation was
used in conjunction with the l2 penalty with a liblinear solver. A constant (bias or intercept)
was added to the decision function, and the intercept scalar was set to 1. The maximum
number of iterations taken for the solvers to converge were set to 100, while the multiclass
option used was ovr, thus fitting binary problem for each label. The number of jobs to use for
the computation was set to 1, making use of only 1 CPU core, and the random number
generator used by the model was that of Numpy&#8217;s RandomState. The tolerance for stopping
criteria was set to 0.0001, and verbosity was used with a value of 0 for the liblinear solver.
The model was built using a cold start by not making use of the previous call to
fit and add more estimators to the ensemble, meaning a whole new forest was fit
instead.
<!--l. 54--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.7 </span> <a 
 id="x1-390003.4.1.7"></a>Bernoulli Naive Bayes</h5>
<!--l. 55--><p class="noindent" >The model was fit with an additive (Laplace/Lidstone) smoothing parameter of 0 for no
smoothing, and the threshold for binarising (mapping to booleans) of sample features was set
to 0, and was presumed to already consist of binary vectors. The model was set to learn class
prior probabilities, which means that the priors were adjusted according to the
data.
<!--l. 57--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.8 </span> <a 
 id="x1-400003.4.1.8"></a>Gaussian Naive Bayes</h5>
                                                                                
                                                                                
<!--l. 58--><p class="noindent" >The model was fit to the data with no previous prior probabilities of the classes, thus the
priors were not adjusted according to the data.
<!--l. 60--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.9 </span> <a 
 id="x1-410003.4.1.9"></a>Neural Network</h5>
<!--l. 61--><p class="noindent" >The model was fit to the data with 3 hidden layer of 100 neurons each using the rectified
linear unit activation function for the hidden layer. The L2 penalty parameter used, which is
the regularisation term, was set to 0.0001, and the size of minibatches for stochastic
optimisers was set to the minimum of the two arguments being the number 200 and the
number of samples. The solver used for weight optimisation was the &#8217;adam&#8217; solver,
which refers to a stochastic gradient-based optimiser. Since the &#8217;adam&#8217; solver was
used, the exponential decay rate for estimates of first moment vector was set to 0.9,
while the second moment vector was set to 0.999. Early stopping was not used to
terminate training when validation score is not improving whilst fitting the data to
the model, and the value for numerical stability in the &#8217;adam&#8217; solver was set to
1e-08. A constant learning rate was used for weight updates in conjunction with the
initial learning rate, which controls the step-size in updating the weights, and was
set to 0.001. The maximum number of iterations which the solver iterates until
convergence was set to 200, and the momentum for gradient descent update was set
to 0.9. Nesterov&#8217;s momentum was used, and the exponent for the inverse scaling
learning rate, which is used in updating the effective learning rate, was set to 0.5.
A random number generator was not used, and the samples were shuffled in each
iteration. The tolerance for optimisation was set to 0.0001 when the loss or score is not
improving by at least the tolerance score set for two consecutive iterations in which
convergence is considered to be reached and training is stopped. The validation
fraction, which is the portion of training data to set aside as validation set for early
stopping, was set to 0.1. The model was built using a cold start by not making
use of the previous call to fit as initialisation, meaning the previous solution was
erased.
<!--l. 63--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.1.10 </span> <a 
 id="x1-420003.4.1.10"></a>Stochastic Gradient Descent</h5>
<!--l. 64--><p class="noindent" >The model was fit to the data using the ordinary least squares fit squared epsilon insensitive
                                                                                
                                                                                
loss function, which ignores errors less than epsilon and is linear past that; this is the loss
function used in SVR. The penalty used, also referred to regularisation term, was &#8217;l2&#8217; which is
the standard regularizer for linear SVM models, and the constant used to multiply the
regularisation term was 0.0001. The elastic net mixing paramater was set to 0.15, while the fit
intercept was estimated, meaning the data was not assumed to be already centered. A
total of 5 passes over the training data, also known as epochs, were used, and the
training data was shuffled after each epoch. No seed from the pseudo random number
generated was used while shuffling the data, and a level 0 verbosity was used. The
epsilon threshold in the epsilon-insensitive loss functions was set to 0.1, and any
differences between the current prediction and the correct label were ignored if they were
less than the threshold. The learning rate schedule used was &#8217;invscaling&#8217;, and the
initial learning rate was set to 0.01. The exponent for inverse scaling learning rate
was set to 0.25, and the model was built using a cold start by not making use of
the previous call to fit as initialisation. The SGD weights of the model were not
averaged.
<!--l. 66--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.4.2 </span> <a 
 id="x1-430003.4.2"></a>Regression</h4>
<!--l. 67--><p class="noindent" >15 and 50 day SMAs were used as the features to train the models, while the adjusted close
price of the stock was used as the target valued for the model to predict. In-sample testing was
carried out using the models predict function, passing the test data set&#8217;s features in order to
predict the output. The mean absolute error, mean squared error, median absolute error, and
<span 
class="cmmi-10x-x-109">R</span><sup><span 
class="cmr-8">2</span> </sup> (coefficient of determination) score were used as metrics in order to rank the performance
of the model&#8217;s prediction capabilities in in-sample testing. An algorithm was developed for
out-of-sample testing. The algorithm iterates for a number of n steps, increasing
the index by 1 each step, forecasting the next day&#8217;s outcome, and calculating the
SMAs.
<!--l. 69--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.1 </span> <a 
 id="x1-440003.4.2.1"></a>Decision Tree</h5>
<!--l. 70--><p class="noindent" >The decision tree model was fit with a mean squared error, which is equal to variance
reduction as feature selection criterion, and mae for the mean absolute error. The
maximum allowed depth of the tree was left unrestricted, which was the same as for the
                                                                                
                                                                                
maximum leaf nodes. A threshold of 1e-7 was used to terminate the tree growth to
determine if a node is a leaf, if the impurity of a node is below the threshold, the
node is a leaf. The minimum number of samples required to be at a leaf node was
set to 1, while the minimum number of samples required to split an internal node
was set to 2. The data fitted to the model was not pre-sorted, and the random
number generator used by the model was that of Numpy&#8217;s RandomState. The model
was given the liberty to select the best strategy in order to split the tree at each
node.
<!--l. 72--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.2 </span> <a 
 id="x1-450003.4.2.2"></a>Boosted Decision Tree</h5>
<!--l. 73--><p class="noindent" >The boosted decision tree was fit with with a decision tree regressor as the base estimator
from which the boosted ensemble is built. The maximum number of estimators at which the
boosting is terminated was set to 50, and the learning rate which shrinks the contribution of
each regressor was set to 1.0. The loss function used when updating the weights after each
boosting iteration was set to linear, and the random number generator used by the model was
that of Numpy&#8217;s RandomState.
<!--l. 75--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.3 </span> <a 
 id="x1-460003.4.2.3"></a>Random Forest</h5>
<!--l. 76--><p class="noindent" >The random forest was fit with a mean absolute error, and bootstrap samples were used when
building trees. The maximum features to consider when looking for the best split were left to
the model to select the best number, and the number of jobs to run in parallel for both the
fitting of the model and its predictions. The model was built using a cold start by not making
use of the previous call to fit and add more estimators to the ensemble, meaning a whole new
forest was fit instead. A threshold of 1e-7 was used to terminate the tree growth to determine
if a node is a leaf, if the impurity of a ode is below the threshold, the node is a
leaf. The minimum number of samples required to be at a leaf node was set to 1,
while the minimum number of samples required to split an internal node was set to
2.
                                                                                
                                                                                
<!--l. 78--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.4 </span> <a 
 id="x1-470003.4.2.4"></a>Linear Regression</h5>
<!--l. 79--><p class="noindent" >The linear regression was fit with no normalised regressors, note that this makes the
hyperparameters learnt more robust and almost independent of the number of samples. The
number of jobs to use for the computation was set to 1, making use of only 1 CPU core.
The intercept of the model was calculated which centres the data being fit to the
model.
<!--l. 81--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.5 </span> <a 
 id="x1-480003.4.2.5"></a>Neural Network</h5>
<!--l. 82--><p class="noindent" >The model was fit to the data with the same paramaters as the Neural Network
classifier.
<!--l. 84--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">3.4.2.6 </span> <a 
 id="x1-490003.4.2.6"></a>Stochastic Gradient Descent</h5>
<!--l. 85--><p class="noindent" >The model was fit to the data with the same paramaters as the SGD classifier.
<!--l. 87--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.5 </span> <a 
 id="x1-500003.5"></a>Bayesian Statistics</h3>
<!--l. 88--><p class="noindent" >The last 500 rows of the selected stocks were extracted from the data set and stored into a
DataFrame. The log returns were calculated by dividing each day&#8217;s adjusted close with the
adjusted close of the following day, in logarithmic form. The resulting values from
the said calculation were then stored in a new column in the DataFrame and all
infinite values were dropped from the series. The sharpe ratio of the original and
predicted price returns was calculated to serve as an accuracy score in the in-sample
tests.
                                                                                
                                                                                
<!--l. 90--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.5.1 </span> <a 
 id="x1-510003.5.1"></a>No-U-Turn Sampler (NUTS)</h4>
<!--l. 91--><p class="noindent" >The model returns were modeled with a Student-t distribution with an unknown degrees of
freedom paramater, and a scale paramater determined by a latent process.
<!--l. 93--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.5.2 </span> <a 
 id="x1-520003.5.2"></a>Metropolis-Hastings</h4>
<!--l. 94--><p class="noindent" >This model was fit to the data with the same paramaters as the NUTS algorithm except that
the model was optimised using the L-BFGS-B algorithm.
<!--l. 96--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3.6 </span> <a 
 id="x1-530003.6"></a>Strategy</h3>
<!--l. 97--><p class="noindent" >The automated algorithmic strategies were run over the period of 01/05/2014. An ordered
dictionary was used to store a series of data frames container the price data of the stocks
selected. The data was tidied where the columns &#8217;open&#8217;, &#8217;high&#8217;, &#8217;low&#8217;, &#8217;close&#8217;, &#8217;ex-dividend&#8217;, and
&#8217;split_ratio&#8217; were dropped from each data frame and the columns &#8217;ticker&#8217;, &#8217;adj_open&#8217;,
&#8217;adj_high&#8217;, &#8217;adj_low&#8217;, and &#8217;adj_close&#8217; were renamed to &#8217;sid&#8217;, &#8217;open&#8217;, &#8217;high&#8217;, &#8217;low&#8217;, and &#8217;close&#8217;
respectively. The ordered dictionary was converted into a panel and passed to the trading
algorithm. An ordered dictionary was used to store a boolean value for each ticker to
determine whether the stock has already been invested in or not. All boolean values were set
to false.
<!--l. 99--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.6.1 </span> <a 
 id="x1-540003.6.1"></a>Classification</h4>
<!--l. 100--><p class="noindent" >An ordered dictionary was used to store the day&#8217;s open and close price for each stock as the
backtester simulated each trading day. The algorithm was only allowed to run once there was
enough price data, the amount chosen was 6. The 2, 3, 4, 5, and 6 day SMAs were calculated
and each stored in an array. The six arrays were converted into an array of tuples, where the
                                                                                
                                                                                
i-th tuple contains the i-th element from each of the argument sequences or iterables. An
array was created to store a 1 if the cloase price increased from the open price,
while a 0 if it decreased. The independent variables, the SMAs, and the dependant
variables, the binary outcome for the particular stock, were passed to the machine
learning algorithm to predict the next day&#8217;s close price. An order of 10% of the
current portfolio value was placed on a stock if the algorithm predicted an increase in
the following day&#8217;s price. The boolean value to determine whether the stock has
already been invested in was set to true. The stock was shorted 10% of the current
portfolio value if the algorithm predicted a decrease in the following day&#8217;s price. The
boolean value to determine whether the stock has already been invested in was set to
false.
<!--l. 102--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">3.6.2 </span> <a 
 id="x1-550003.6.2"></a>Regression</h4>
<!--l. 103--><p class="noindent" >An ordered dictionary was used to store the day&#8217;s close price for each stock as the
backtester simulated each trading day. The algorithm was only allowed to run once
there was enough price data, the amount chosen was 50. The 15 and 50 day SMAs
were calculated and each stored in an array. The two arrays were converted into
an array of tuples, where the i-th tuple contains the i-th element from each of the
argument sequences or iterables. The independent variables, the SMAs, and the
dependant variables, the close prices for the particular stock, were passed to the machine
learning algorithm to predict the next day&#8217;s close price. The next day&#8217;s 15 and 50
day SMAs were calculated using the predicted price and the following day&#8217;s price
predicted again. This process was iterated over 100 times in order to predict the close
price of the following 100 days. An order was placed on a stock if the difference
of the last predicted price and the current day&#8217;s predicted price was higher than
10. The amount of the stock was determined based on the difference, the higher
the difference, the larger the percentage of the allocation. The boolean value to
determine whether the stock has already been invested in was set to true. The stock was
shorted if the difference of the last predicted price and the current day&#8217;s predicted
price was higher than -10. The amount of the stock was determined based on the
difference, the higher the difference, the larger the percentage of the allocation. The
boolean value to determine whether the stock has already been invested in was set to
false.
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;4</span><br /><a 
 id="x1-560004"></a>Research Findings</h2> For the purpose of bencharking the performance of the
algorithms, a total of five stocks from the basket of uncorrelated stocks were selected. These
were MSFT, CDE, NAVB, HRG, and HL.
<div class="center" 
>
<!--l. 4--><p class="noindent" >

<!--l. 5--><p class="noindent" ><img 
src="Figures/stocks.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.1: </span><span  
class="content">Basket of stocks</span></div><!--tex4ht:label?: x1-560011 -->
</div>
<h3 class="sectionHead"><span class="titlemark">4.1 </span> <a 
 id="x1-570004.1"></a>Time Series Analysis</h3>
<!--l. 12--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.1 </span> <a 
 id="x1-580004.1.1"></a>Random Walk</h4>
<div class="center" 
>
<!--l. 14--><p class="noindent" >

<!--l. 15--><p class="noindent" ><img 
src="Figures/MSFT-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.2: </span><span  
class="content">MSFT time series analysis</span></div><!--tex4ht:label?: x1-580012 -->
</div>
<div class="center" 
>
<!--l. 20--><p class="noindent" >

<!--l. 21--><p class="noindent" ><img 
src="Figures/MSFT-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.3: </span><span  
class="content">MSFT histogram of returns</span></div><!--tex4ht:label?: x1-580023 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 26--><p class="noindent" >

<!--l. 27--><p class="noindent" ><img 
src="Figures/CDE-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.4: </span><span  
class="content">CDE time series analysis</span></div><!--tex4ht:label?: x1-580034 -->
</div>
<div class="center" 
>
<!--l. 32--><p class="noindent" >

<!--l. 33--><p class="noindent" ><img 
src="Figures/CDE-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.5: </span><span  
class="content">CDE histogram of returns</span></div><!--tex4ht:label?: x1-580045 -->
</div>
<div class="center" 
>
<!--l. 38--><p class="noindent" >

<!--l. 40--><p class="noindent" ><img 
src="Figures/NAVB-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.6: </span><span  
class="content">NAVB time series analysis</span></div><!--tex4ht:label?: x1-580056 -->
</div>
<div class="center" 
>
<!--l. 45--><p class="noindent" >

<!--l. 47--><p class="noindent" ><img 
src="Figures/NAVB-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.7: </span><span  
class="content">NAVB histogram of returns</span></div><!--tex4ht:label?: x1-580067 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 52--><p class="noindent" >

<!--l. 53--><p class="noindent" ><img 
src="Figures/HRG-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.8: </span><span  
class="content">HRG time series analysis</span></div><!--tex4ht:label?: x1-580078 -->
</div>
<div class="center" 
>
<!--l. 58--><p class="noindent" >

<!--l. 59--><p class="noindent" ><img 
src="Figures/HRG-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.9: </span><span  
class="content">HRG histogram of returns</span></div><!--tex4ht:label?: x1-580089 -->
</div>
<div class="center" 
>
<!--l. 64--><p class="noindent" >

<!--l. 65--><p class="noindent" ><img 
src="Figures/HL-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.10: </span><span  
class="content">HL time series analysis</span></div><!--tex4ht:label?: x1-5800910 -->
</div>
<div class="center" 
>
<!--l. 70--><p class="noindent" >

<!--l. 71--><p class="noindent" ><img 
src="Figures/HL-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.11: </span><span  
class="content">HL histogram of returns</span></div><!--tex4ht:label?: x1-5801011 -->
</div>
                                                                                
                                                                                
<!--l. 76--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.2 </span> <a 
 id="x1-590004.1.2"></a>Ordinary Least Squares (OLS)</h4>
<!--l. 77--><p class="noindent" >MSFT scored a mean absolute error regression loss of 0.810, and a coefficient of determination
of 0.991.
<div class="center" 
>
<!--l. 79--><p class="noindent" >

<!--l. 80--><p class="noindent" ><img 
src="Figures/MSFT-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.12: </span><span  
class="content">MSFT OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5900112 -->
</div>
<div class="center" 
>
<!--l. 85--><p class="noindent" >

<!--l. 86--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.13: </span><span  
class="content">100 day MSFT OLS in-sample forecast</span></div><!--tex4ht:label?: x1-5900213 -->
</div>
<!--l. 91--><p class="noindent" >CDE scored a mean absolute error regression loss of 0.985, and a coefficient of determination
of 0.973.
<div class="center" 
>
<!--l. 93--><p class="noindent" >

<!--l. 94--><p class="noindent" ><img 
src="Figures/CDE-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.14: </span><span  
class="content">CDE OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5900314 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 99--><p class="noindent" >

<!--l. 100--><p class="noindent" ><img 
src="Figures/100-Day-CDE-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.15: </span><span  
class="content">100 Day CDE OLS out of sample forecast</span></div><!--tex4ht:label?: x1-5900415 -->
</div>
<!--l. 105--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.124, and a coefficient of determination
of 0.966.
<div class="center" 
>
<!--l. 107--><p class="noindent" >

<!--l. 108--><p class="noindent" ><img 
src="Figures/NAVB-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.16: </span><span  
class="content">NAVB OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5900516 -->
</div>
<div class="center" 
>
<!--l. 113--><p class="noindent" >

<!--l. 115--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.17: </span><span  
class="content">100 day NAVB OLS in-sample forecast</span></div><!--tex4ht:label?: x1-5900617 -->
</div>
<!--l. 120--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.291, and a coefficient of determination
of 0.986.
<div class="center" 
>
<!--l. 122--><p class="noindent" >

                                                                                
                                                                                
<!--l. 123--><p class="noindent" ><img 
src="Figures/HRG-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.18: </span><span  
class="content">HRG OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5900718 -->
</div>
<div class="center" 
>
<!--l. 128--><p class="noindent" >

<!--l. 129--><p class="noindent" ><img 
src="Figures/100-Day-HRG-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.19: </span><span  
class="content">100 day HRG OLS in-sample forecast</span></div><!--tex4ht:label?: x1-5900819 -->
</div>
<!--l. 134--><p class="noindent" >HL scored a mean absolute error regression loss of 0.336, and a coefficient of determination of
0.963.
<div class="center" 
>
<!--l. 136--><p class="noindent" >

<!--l. 137--><p class="noindent" ><img 
src="Figures/HL-OLS-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.20: </span><span  
class="content">HL OLS in-sample prediction</span></div><!--tex4ht:label?: x1-5900920 -->
</div>
<div class="center" 
>
<!--l. 142--><p class="noindent" >

<!--l. 143--><p class="noindent" ><img 
src="Figures/100-Day-HL-OLS-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.21: </span><span  
class="content">100 day HL OLS in-sample forecast</span></div><!--tex4ht:label?: x1-5901021 -->
</div>
                                                                                
                                                                                
<!--l. 148--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.3 </span> <a 
 id="x1-600004.1.3"></a>Auto Regressive (AR)</h4>
<!--l. 150--><p class="noindent" >MSFT scored sharpe ratios of 1.152 for the original returns, and -34.641 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 152--><p class="noindent" >

<!--l. 153--><p class="noindent" ><img 
src="Figures/MSFT-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.22: </span><span  
class="content">MSFT AR time series analysis</span></div><!--tex4ht:label?: x1-6000122 -->
</div>
<div class="center" 
>
<!--l. 158--><p class="noindent" >

<!--l. 159--><p class="noindent" ><img 
src="Figures/MSFT-AR-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.23: </span><span  
class="content">MSFT AR histogram of returns</span></div><!--tex4ht:label?: x1-6000223 -->
</div>
<div class="center" 
>
<!--l. 164--><p class="noindent" >

<!--l. 165--><p class="noindent" ><img 
src="Figures/MSFT-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.24: </span><span  
class="content">MSFT AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-6000324 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 170--><p class="noindent" >

<!--l. 171--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.25: </span><span  
class="content">100 day MSFT AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-6000425 -->
</div>
<!--l. 176--><p class="noindent" >CDE scored sharpe ratios of -1.050 for the original returns, and -0.239 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 178--><p class="noindent" >

<!--l. 179--><p class="noindent" ><img 
src="Figures/CDE-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.26: </span><span  
class="content">CDE AR time series analysis</span></div><!--tex4ht:label?: x1-6000526 -->
</div>
<div class="center" 
>
<!--l. 184--><p class="noindent" >

<!--l. 185--><p class="noindent" ><img 
src="Figures/CDE-AR-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.27: </span><span  
class="content">CDE AR histogram of returns</span></div><!--tex4ht:label?: x1-6000627 -->
</div>
<div class="center" 
>
<!--l. 190--><p class="noindent" >

<!--l. 191--><p class="noindent" ><img 
src="Figures/CDE-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.28: </span><span  
class="content">CDE AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-6000728 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 196--><p class="noindent" >

<!--l. 197--><p class="noindent" ><img 
src="Figures/100-Day-CDE-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.29: </span><span  
class="content">100 day CDE AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-6000829 -->
</div>
<!--l. 202--><p class="noindent" >NAVB scored sharpe ratios of -0.410 for the original returns, and 0.124 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 204--><p class="noindent" >

<!--l. 205--><p class="noindent" ><img 
src="Figures/NAVB-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.30: </span><span  
class="content">NAVB AR time series analysis</span></div><!--tex4ht:label?: x1-6000930 -->
</div>
<div class="center" 
>
<!--l. 210--><p class="noindent" >

<!--l. 211--><p class="noindent" ><img 
src="Figures/NAVB-AR-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.31: </span><span  
class="content">NAVB AR histogram of returns</span></div><!--tex4ht:label?: x1-6001031 -->
</div>
<div class="center" 
>
<!--l. 216--><p class="noindent" >

                                                                                
                                                                                
<!--l. 217--><p class="noindent" ><img 
src="Figures/NAVB-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.32: </span><span  
class="content">NAVB AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-6001132 -->
</div>
<div class="center" 
>
<!--l. 222--><p class="noindent" >

<!--l. 223--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.33: </span><span  
class="content">100 day NAVB AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-6001233 -->
</div>
<!--l. 228--><p class="noindent" >HRG scored sharpe ratios of 0.627 for the original returns, and -2.603 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 230--><p class="noindent" >

<!--l. 231--><p class="noindent" ><img 
src="Figures/HRG-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.34: </span><span  
class="content">HRG AR time series analysis</span></div><!--tex4ht:label?: x1-6001334 -->
</div>
<div class="center" 
>
<!--l. 236--><p class="noindent" >

<!--l. 238--><p class="noindent" ><img 
src="Figures/HRG-AR-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.35: </span><span  
class="content">HRG AR histogram of returns</span></div><!--tex4ht:label?: x1-6001435 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 243--><p class="noindent" >

<!--l. 244--><p class="noindent" ><img 
src="Figures/HRG-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.36: </span><span  
class="content">HL AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-6001536 -->
</div>
<div class="center" 
>
<!--l. 249--><p class="noindent" >

<!--l. 250--><p class="noindent" ><img 
src="Figures/100-Day-HRG-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.37: </span><span  
class="content">100 day HRG AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-6001637 -->
</div>
<!--l. 255--><p class="noindent" >HL scored sharpe ratios of 0.695 for the original returns, and -1.880 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 257--><p class="noindent" >

<!--l. 258--><p class="noindent" ><img 
src="Figures/HL-AR-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.38: </span><span  
class="content">HL AR time series analysis</span></div><!--tex4ht:label?: x1-6001738 -->
</div>
<div class="center" 
>
<!--l. 263--><p class="noindent" >

<!--l. 264--><p class="noindent" ><img 
src="Figures/HL-AR-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.39: </span><span  
class="content">HL AR histogram of returns</span></div><!--tex4ht:label?: x1-6001839 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 269--><p class="noindent" >

<!--l. 271--><p class="noindent" ><img 
src="Figures/HL-AR-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.40: </span><span  
class="content">HL AR in-sample returns prediction</span></div><!--tex4ht:label?: x1-6001940 -->
</div>
<div class="center" 
>
<!--l. 276--><p class="noindent" >

<!--l. 278--><p class="noindent" ><img 
src="Figures/100-Day-HL-AR-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.41: </span><span  
class="content">100 day HL AR in-sample returns forecast</span></div><!--tex4ht:label?: x1-6002041 -->
</div>
<!--l. 283--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.4 </span> <a 
 id="x1-610004.1.4"></a>Moving Average (MA)</h4>
<!--l. 285--><p class="noindent" >MSFT scored sharpe ratios of 0.358 for the original returns, and -5.610 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 287--><p class="noindent" >

<!--l. 288--><p class="noindent" ><img 
src="Figures/MSFT-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.42: </span><span  
class="content">MSFT MA time series analysis</span></div><!--tex4ht:label?: x1-6100142 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 293--><p class="noindent" >

<!--l. 294--><p class="noindent" ><img 
src="Figures/MSFT-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.43: </span><span  
class="content">MSFT MA histogram of returns</span></div><!--tex4ht:label?: x1-6100243 -->
</div>
<div class="center" 
>
<!--l. 299--><p class="noindent" >

<!--l. 300--><p class="noindent" ><img 
src="Figures/MSFT-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.44: </span><span  
class="content">MSFT MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6100344 -->
</div>
<div class="center" 
>
<!--l. 305--><p class="noindent" >

<!--l. 306--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.45: </span><span  
class="content">100 day MSFT MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6100445 -->
</div>
<!--l. 311--><p class="noindent" >CDE scored sharpe ratios of -0.199 for the original returns, and -0.760 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 313--><p class="noindent" >

<!--l. 314--><p class="noindent" ><img 
src="Figures/CDE-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.46: </span><span  
class="content">CDE MA time series analysis</span></div><!--tex4ht:label?: x1-6100546 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 319--><p class="noindent" >

<!--l. 320--><p class="noindent" ><img 
src="Figures/CDE-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.47: </span><span  
class="content">CDE MA histogram of returns</span></div><!--tex4ht:label?: x1-6100647 -->
</div>
<div class="center" 
>
<!--l. 325--><p class="noindent" >

<!--l. 326--><p class="noindent" ><img 
src="Figures/CDE-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.48: </span><span  
class="content">CDE MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6100748 -->
</div>
<div class="center" 
>
<!--l. 331--><p class="noindent" >

<!--l. 332--><p class="noindent" ><img 
src="Figures/100-Day-CDE-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.49: </span><span  
class="content">100 day CDE MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6100849 -->
</div>
<!--l. 337--><p class="noindent" >NAVB scored sharpe ratios of -0.067 for the original returns, and 0.590 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 339--><p class="noindent" >

                                                                                
                                                                                
<!--l. 340--><p class="noindent" ><img 
src="Figures/NAVB-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.50: </span><span  
class="content">NAVB MA time series analysis</span></div><!--tex4ht:label?: x1-6100950 -->
</div>
<div class="center" 
>
<!--l. 345--><p class="noindent" >

<!--l. 346--><p class="noindent" ><img 
src="Figures/NAVB-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.51: </span><span  
class="content">NAVB MA histogram of returns</span></div><!--tex4ht:label?: x1-6101051 -->
</div>
<div class="center" 
>
<!--l. 351--><p class="noindent" >

<!--l. 352--><p class="noindent" ><img 
src="Figures/NAVB-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.52: </span><span  
class="content">NAVB MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6101152 -->
</div>
<div class="center" 
>
<!--l. 357--><p class="noindent" >

<!--l. 358--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.53: </span><span  
class="content">100 day NAVB MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6101253 -->
</div>
<!--l. 363--><p class="noindent" >HRG scored sharpe ratios of 0.084 for the original returns, and -1.020 for the predicted returns
in the in-sample test.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 365--><p class="noindent" >

<!--l. 366--><p class="noindent" ><img 
src="Figures/HRG-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.54: </span><span  
class="content">HRG MA time series analysis</span></div><!--tex4ht:label?: x1-6101354 -->
</div>
<div class="center" 
>
<!--l. 371--><p class="noindent" >

<!--l. 372--><p class="noindent" ><img 
src="Figures/HRG-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.55: </span><span  
class="content">HRG MA histogram of returns</span></div><!--tex4ht:label?: x1-6101455 -->
</div>
<div class="center" 
>
<!--l. 377--><p class="noindent" >

<!--l. 378--><p class="noindent" ><img 
src="Figures/HRG-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.56: </span><span  
class="content">HRG MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6101556 -->
</div>
<div class="center" 
>
<!--l. 383--><p class="noindent" >

<!--l. 384--><p class="noindent" ><img 
src="Figures/100-Day-HRG-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.57: </span><span  
class="content">100 day HRG MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6101657 -->
</div>
                                                                                
                                                                                
<!--l. 389--><p class="noindent" >HL scored sharpe ratios of -0.128 for the original returns, and -0.977 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 391--><p class="noindent" >

<!--l. 392--><p class="noindent" ><img 
src="Figures/HL-MA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.58: </span><span  
class="content">HL MA time series analysis</span></div><!--tex4ht:label?: x1-6101758 -->
</div>
<div class="center" 
>
<!--l. 397--><p class="noindent" >

<!--l. 398--><p class="noindent" ><img 
src="Figures/HL-MA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.59: </span><span  
class="content">HL MA histogram of returns</span></div><!--tex4ht:label?: x1-6101859 -->
</div>
<div class="center" 
>
<!--l. 403--><p class="noindent" >

<!--l. 404--><p class="noindent" ><img 
src="Figures/HL-MA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.60: </span><span  
class="content">HL MA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6101960 -->
</div>
<div class="center" 
>
<!--l. 409--><p class="noindent" >

                                                                                
                                                                                
<!--l. 410--><p class="noindent" ><img 
src="Figures/100-Day-HL-MA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.61: </span><span  
class="content">100 day HL MA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6102061 -->
</div>
<!--l. 415--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.5 </span> <a 
 id="x1-620004.1.5"></a>Auto Regressive Moving Average (ARMA)</h4>
<!--l. 417--><p class="noindent" >HL scored sharpe ratios of 0.537 for the original returns, and -6.485 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 419--><p class="noindent" >

<!--l. 420--><p class="noindent" ><img 
src="Figures/MSFT-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.62: </span><span  
class="content">MSFT ARMA time series analysis</span></div><!--tex4ht:label?: x1-6200162 -->
</div>
<div class="center" 
>
<!--l. 425--><p class="noindent" >

<!--l. 426--><p class="noindent" ><img 
src="Figures/MSFT-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.63: </span><span  
class="content">MSFT ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6200263 -->
</div>
<div class="center" 
>
<!--l. 431--><p class="noindent" >

<!--l. 432--><p class="noindent" ><img 
src="Figures/MSFT-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
                                                                                
                                                                                
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.64: </span><span  
class="content">MSFT ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6200364 -->
</div>
<div class="center" 
>
<!--l. 437--><p class="noindent" >

<!--l. 438--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.65: </span><span  
class="content">100 day MSFT ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6200465 -->
</div>
<!--l. 443--><p class="noindent" >MSFT scored sharpe ratios of -0.128 for the original returns, and -0.977 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 445--><p class="noindent" >

<!--l. 446--><p class="noindent" ><img 
src="Figures/CDE-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.66: </span><span  
class="content">CDE ARMA time series analysis</span></div><!--tex4ht:label?: x1-6200566 -->
</div>
<div class="center" 
>
<!--l. 451--><p class="noindent" >

<!--l. 452--><p class="noindent" ><img 
src="Figures/CDE-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.67: </span><span  
class="content">CDE ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6200667 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 457--><p class="noindent" >

<!--l. 458--><p class="noindent" ><img 
src="Figures/CDE-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.68: </span><span  
class="content">CDE ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6200768 -->
</div>
<div class="center" 
>
<!--l. 463--><p class="noindent" >

<!--l. 464--><p class="noindent" ><img 
src="Figures/100-Day-CDE-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.69: </span><span  
class="content">100 day CDE ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6200869 -->
</div>
<!--l. 469--><p class="noindent" >CDE scored sharpe ratios of -0.245 for the original returns, and -0.614 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 471--><p class="noindent" >

<!--l. 472--><p class="noindent" ><img 
src="Figures/NAVB-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.70: </span><span  
class="content">NAVB ARMA time series analysis</span></div><!--tex4ht:label?: x1-6200970 -->
</div>
<div class="center" 
>
<!--l. 477--><p class="noindent" >

<!--l. 478--><p class="noindent" ><img 
src="Figures/NAVB-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.71: </span><span  
class="content">NAVB ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6201071 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 483--><p class="noindent" >

<!--l. 484--><p class="noindent" ><img 
src="Figures/NAVB-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.72: </span><span  
class="content">NAVB ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6201172 -->
</div>
<div class="center" 
>
<!--l. 489--><p class="noindent" >

<!--l. 490--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.73: </span><span  
class="content">100 day NAVB ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6201273 -->
</div>
<!--l. 495--><p class="noindent" >NAVB scored sharpe ratios of -0.032 for the original returns, and -0.641 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 497--><p class="noindent" >

<!--l. 498--><p class="noindent" ><img 
src="Figures/HRG-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.74: </span><span  
class="content">HRG ARMA time series analysis</span></div><!--tex4ht:label?: x1-6201374 -->
</div>
<div class="center" 
>
<!--l. 503--><p class="noindent" >

                                                                                
                                                                                
<!--l. 504--><p class="noindent" ><img 
src="Figures/HRG-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.75: </span><span  
class="content">HRG ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6201475 -->
</div>
<div class="center" 
>
<!--l. 509--><p class="noindent" >

<!--l. 510--><p class="noindent" ><img 
src="Figures/HRG-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.76: </span><span  
class="content">HRG ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6201576 -->
</div>
<div class="center" 
>
<!--l. 515--><p class="noindent" >

<!--l. 516--><p class="noindent" ><img 
src="Figures/100-Day-HRG-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.77: </span><span  
class="content">100 day HRG ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6201677 -->
</div>
<!--l. 521--><p class="noindent" >HL scored sharpe ratios of 0.111 for the original returns, and -1.023 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 523--><p class="noindent" >

<!--l. 525--><p class="noindent" ><img 
src="Figures/HL-ARMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.78: </span><span  
class="content">HL ARMA time series analysis</span></div><!--tex4ht:label?: x1-6201778 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 530--><p class="noindent" >

<!--l. 531--><p class="noindent" ><img 
src="Figures/HL-ARMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.79: </span><span  
class="content">HL ARMA histogram of returns</span></div><!--tex4ht:label?: x1-6201879 -->
</div>
<div class="center" 
>
<!--l. 536--><p class="noindent" >

<!--l. 537--><p class="noindent" ><img 
src="Figures/HL-ARMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.80: </span><span  
class="content">HL ARMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6201980 -->
</div>
<div class="center" 
>
<!--l. 542--><p class="noindent" >

<!--l. 543--><p class="noindent" ><img 
src="Figures/100-Day-HL-ARMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.81: </span><span  
class="content">100 day HL ARMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6202081 -->
</div>
<!--l. 548--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.1.6 </span> <a 
 id="x1-630004.1.6"></a>Auto Regressive Integrated Moving Average (ARIMA)</h4>
<!--l. 550--><p class="noindent" >MSFT scored sharpe ratios of 0.123 for the original returns, and -4.904 for the predicted
returns in the in-sample test.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 552--><p class="noindent" >

<!--l. 553--><p class="noindent" ><img 
src="Figures/MSFT-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.82: </span><span  
class="content">MSFT ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6300182 -->
</div>
<div class="center" 
>
<!--l. 558--><p class="noindent" >

<!--l. 559--><p class="noindent" ><img 
src="Figures/MSFT-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.83: </span><span  
class="content">MSFT ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6300283 -->
</div>
<div class="center" 
>
<!--l. 564--><p class="noindent" >

<!--l. 565--><p class="noindent" ><img 
src="Figures/MSFT-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.84: </span><span  
class="content">MSFT ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6300384 -->
</div>
<div class="center" 
>
<!--l. 570--><p class="noindent" >

<!--l. 571--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.85: </span><span  
class="content">100 day MSFT ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6300485 -->
</div>
                                                                                
                                                                                
<!--l. 576--><p class="noindent" >CDE scored sharpe ratios of -0.133 for the original returns, and -0.698 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 578--><p class="noindent" >

<!--l. 579--><p class="noindent" ><img 
src="Figures/CDE-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.86: </span><span  
class="content">CDE ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6300586 -->
</div>
<div class="center" 
>
<!--l. 584--><p class="noindent" >

<!--l. 585--><p class="noindent" ><img 
src="Figures/CDE-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.87: </span><span  
class="content">CDE ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6300687 -->
</div>
<div class="center" 
>
<!--l. 590--><p class="noindent" >

<!--l. 591--><p class="noindent" ><img 
src="Figures/CDE-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.88: </span><span  
class="content">CDE ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6300788 -->
</div>
<div class="center" 
>
<!--l. 596--><p class="noindent" >

                                                                                
                                                                                
<!--l. 597--><p class="noindent" ><img 
src="Figures/100-Day-CDE-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.89: </span><span  
class="content">100 day CDE ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6300889 -->
</div>
<!--l. 602--><p class="noindent" >NAVB scored sharpe ratios of 0.015 for the original returns, and -0.708 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 604--><p class="noindent" >

<!--l. 605--><p class="noindent" ><img 
src="Figures/NAVB-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.90: </span><span  
class="content">NAVB ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6300990 -->
</div>
<div class="center" 
>
<!--l. 610--><p class="noindent" >

<!--l. 611--><p class="noindent" ><img 
src="Figures/NAVB-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.91: </span><span  
class="content">NAVB ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6301091 -->
</div>
<div class="center" 
>
<!--l. 616--><p class="noindent" >

<!--l. 617--><p class="noindent" ><img 
src="Figures/NAVB-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.92: </span><span  
class="content">NAVB ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6301192 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 622--><p class="noindent" >

<!--l. 623--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.93: </span><span  
class="content">100 day NAVB ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6301293 -->
</div>
<!--l. 628--><p class="noindent" >HRG scored sharpe ratios of 0.401 for the original returns, and -1.354 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 630--><p class="noindent" >

<!--l. 631--><p class="noindent" ><img 
src="Figures/HRG-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.94: </span><span  
class="content">HRG ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6301394 -->
</div>
<div class="center" 
>
<!--l. 636--><p class="noindent" >

<!--l. 637--><p class="noindent" ><img 
src="Figures/HRG-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.95: </span><span  
class="content">HRG ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6301495 -->
</div>
<div class="center" 
>
<!--l. 642--><p class="noindent" >

<!--l. 643--><p class="noindent" ><img 
src="Figures/HRG-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.96: </span><span  
class="content">HRG ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-6301596 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 648--><p class="noindent" >

<!--l. 649--><p class="noindent" ><img 
src="Figures/100-Day-HRG-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.97: </span><span  
class="content">100 day HRG ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-6301697 -->
</div>
<!--l. 654--><p class="noindent" >HL scored sharpe ratios of -0.127 for the original returns, and -0.974 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 656--><p class="noindent" >

<!--l. 657--><p class="noindent" ><img 
src="Figures/HL-ARIMA-time-series.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.98: </span><span  
class="content">HL ARIMA time series analysis</span></div><!--tex4ht:label?: x1-6301798 -->
</div>
<div class="center" 
>
<!--l. 662--><p class="noindent" >

<!--l. 663--><p class="noindent" ><img 
src="Figures/HL-ARIMA-histogram.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.99: </span><span  
class="content">HL ARIMA histogram of returns</span></div><!--tex4ht:label?: x1-6301899 -->
</div>
<div class="center" 
>
<!--l. 668--><p class="noindent" >

                                                                                
                                                                                
<!--l. 669--><p class="noindent" ><img 
src="Figures/HL-ARIMA-In-Sample-Return-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.100: </span><span  
class="content">HL ARIMA in-sample returns prediction</span></div><!--tex4ht:label?: x1-63019100 -->
</div>
<div class="center" 
>
<!--l. 674--><p class="noindent" >

<!--l. 675--><p class="noindent" ><img 
src="Figures/100-Day-HL-ARIMA-Out-of-Sample-Return-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.101: </span><span  
class="content">100 day HL ARIMA in-sample returns forecast</span></div><!--tex4ht:label?: x1-63020101 -->
</div>
<!--l. 680--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">4.2 </span> <a 
 id="x1-640004.2"></a>Machine Learning</h3>
<!--l. 682--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.2.1 </span> <a 
 id="x1-650004.2.1"></a>Classification</h4>
<!--l. 684--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.1 </span> <a 
 id="x1-660004.2.1.1"></a>Decision Tree</h5>
<div class="center" 
>
<!--l. 686--><p class="noindent" >
                                                                                
                                                                                
<div class="tabular"><table id="TBL-2" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"></colgroup><colgroup id="TBL-2-2g"><col 
id="TBL-2-2"></colgroup><colgroup id="TBL-2-3g"><col 
id="TBL-2-3"></colgroup><colgroup id="TBL-2-4g"><col 
id="TBL-2-4"></colgroup><colgroup id="TBL-2-5g"><col 
id="TBL-2-5"></colgroup><colgroup id="TBL-2-6g"><col 
id="TBL-2-6"></colgroup><colgroup id="TBL-2-7g"><col 
id="TBL-2-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-4"  
class="td11">131                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-5"  
class="td11">547              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-3"  
class="td11">565 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-4"  
class="td11">144 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-5"  
class="td11">504 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-3-6"  
class="td11">133</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-3"  
class="td11">592               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-4"  
class="td11">165                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-5"  
class="td11">331              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-4-6"  
class="td11">126               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-3"  
class="td11">553               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-4"  
class="td11">210                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-5"  
class="td11">462              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-5-6"  
class="td11">135               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-2"  
class="td11">0.79       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-3"  
class="td11">603               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-4"  
class="td11">135                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-5"  
class="td11">475              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-6-6"  
class="td11">147               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.1: </span><span  
class="content">Decision Tree results</span></div><!--tex4ht:label?: x1-660011 -->
</div>
<!--l. 701--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.2 </span> <a 
 id="x1-670004.2.1.2"></a>Boosted Decision Tree</h5>
<div class="center" 
>
<!--l. 703--><p class="noindent" >
<div class="tabular"><table id="TBL-3" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-3-1g"><col 
id="TBL-3-1"></colgroup><colgroup id="TBL-3-2g"><col 
id="TBL-3-2"></colgroup><colgroup id="TBL-3-3g"><col 
id="TBL-3-3"></colgroup><colgroup id="TBL-3-4g"><col 
id="TBL-3-4"></colgroup><colgroup id="TBL-3-5g"><col 
id="TBL-3-5"></colgroup><colgroup id="TBL-3-6g"><col 
id="TBL-3-6"></colgroup><colgroup id="TBL-3-7g"><col 
id="TBL-3-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-3"  
class="td11">564 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-4"  
class="td11">143 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-5"  
class="td11">505 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-6"  
class="td11">134</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-3"  
class="td11">588               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-4"  
class="td11">164                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-5"  
class="td11">332              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-6"  
class="td11">130               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-3"  
class="td11">542               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-4"  
class="td11">191                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-5"  
class="td11">481              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-6"  
class="td11">146               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-2"  
class="td11">0.79       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-3"  
class="td11">602               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-4"  
class="td11">132                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-5"  
class="td11">478              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-6"  
class="td11">149               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.2: </span><span  
class="content">Boosted Decision Tree results</span></div><!--tex4ht:label?: x1-670012 -->
</div>
<!--l. 718--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.3 </span> <a 
 id="x1-680004.2.1.3"></a>Support Vector Machine (SVM)</h5>
<div class="center" 
>
<!--l. 720--><p class="noindent" >
<div class="tabular"><table id="TBL-4" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-4-1g"><col 
id="TBL-4-1"></colgroup><colgroup id="TBL-4-2g"><col 
id="TBL-4-2"></colgroup><colgroup id="TBL-4-3g"><col 
id="TBL-4-3"></colgroup><colgroup id="TBL-4-4g"><col 
id="TBL-4-4"></colgroup><colgroup id="TBL-4-5g"><col 
id="TBL-4-5"></colgroup><colgroup id="TBL-4-6g"><col 
id="TBL-4-6"></colgroup><colgroup id="TBL-4-7g"><col 
id="TBL-4-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-3"  
class="td11">564 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-4"  
class="td11">143 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-5"  
class="td11">505 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-6"  
class="td11">134</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-3"  
class="td11">593               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-4"  
class="td11">169                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-5"  
class="td11">327              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-6"  
class="td11">125               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-3"  
class="td11">542               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-4"  
class="td11">191                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-5"  
class="td11">481              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-6"  
class="td11">146               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-2"  
class="td11">0.81       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-3"  
class="td11">579               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-4"  
class="td11">98                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-5"  
class="td11">512              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-6"  
class="td11">171               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-7-1"  
class="td11"> </td>
</tr></table></div>
                                                                                
                                                                                
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.3: </span><span  
class="content">Support Vector Machine results</span></div><!--tex4ht:label?: x1-680013 -->
</div>
<!--l. 735--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.4 </span> <a 
 id="x1-690004.2.1.4"></a>Random Forest</h5>
<div class="center" 
>
<!--l. 737--><p class="noindent" >
<div class="tabular"><table id="TBL-5" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-5-1g"><col 
id="TBL-5-1"></colgroup><colgroup id="TBL-5-2g"><col 
id="TBL-5-2"></colgroup><colgroup id="TBL-5-3g"><col 
id="TBL-5-3"></colgroup><colgroup id="TBL-5-4g"><col 
id="TBL-5-4"></colgroup><colgroup id="TBL-5-5g"><col 
id="TBL-5-5"></colgroup><colgroup id="TBL-5-6g"><col 
id="TBL-5-6"></colgroup><colgroup id="TBL-5-7g"><col 
id="TBL-5-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-4"  
class="td11">131                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-5"  
class="td11">547              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-3"  
class="td11">566 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-4"  
class="td11">145 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-5"  
class="td11">503 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-3-6"  
class="td11">132</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-3"  
class="td11">590               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-4"  
class="td11">164                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-5"  
class="td11">332              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-4-6"  
class="td11">128               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-3"  
class="td11">554               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-4"  
class="td11">210                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-5"  
class="td11">462              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-5-6"  
class="td11">134               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-2"  
class="td11">0.81       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-3"  
class="td11">581               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-4"  
class="td11">101                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-5"  
class="td11">509              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-5-6-6"  
class="td11">169               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-5-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.4: </span><span  
class="content">Random Forest results</span></div><!--tex4ht:label?: x1-690014 -->
</div>
<!--l. 752--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.5 </span> <a 
 id="x1-700004.2.1.5"></a>K-Nearest Neighbour</h5>
<div class="center" 
>
<!--l. 754--><p class="noindent" >
<div class="tabular"><table id="TBL-6" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-6-1g"><col 
id="TBL-6-1"></colgroup><colgroup id="TBL-6-2g"><col 
id="TBL-6-2"></colgroup><colgroup id="TBL-6-3g"><col 
id="TBL-6-3"></colgroup><colgroup id="TBL-6-4g"><col 
id="TBL-6-4"></colgroup><colgroup id="TBL-6-5g"><col 
id="TBL-6-5"></colgroup><colgroup id="TBL-6-6g"><col 
id="TBL-6-6"></colgroup><colgroup id="TBL-6-7g"><col 
id="TBL-6-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-3"  
class="td11">489               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-2-6"  
class="td11">194               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-2"  
class="td11">0.80 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-3"  
class="td11">574 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-4"  
class="td11">147 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-5"  
class="td11">501 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-3-6"  
class="td11">126</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-3"  
class="td11">573               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-4"  
class="td11">161                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-5"  
class="td11">335              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-4-6"  
class="td11">145               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-2"  
class="td11">0.74       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-3"  
class="td11">560               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-4"  
class="td11">233                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-5"  
class="td11">439              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-5-6"  
class="td11">128               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-2"  
class="td11">0.81       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-3"  
class="td11">581               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-4"  
class="td11">101                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-5"  
class="td11">509              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-6-6-6"  
class="td11">169               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-6-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.5: </span><span  
class="content">K-Nearest Neighbour results</span></div><!--tex4ht:label?: x1-700015 -->
</div>
                                                                                
                                                                                
<!--l. 769--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.6 </span> <a 
 id="x1-710004.2.1.6"></a>Logistic Regression</h5>
<div class="center" 
>
<!--l. 771--><p class="noindent" >
<div class="tabular"><table id="TBL-7" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-7-1g"><col 
id="TBL-7-1"></colgroup><colgroup id="TBL-7-2g"><col 
id="TBL-7-2"></colgroup><colgroup id="TBL-7-3g"><col 
id="TBL-7-3"></colgroup><colgroup id="TBL-7-4g"><col 
id="TBL-7-4"></colgroup><colgroup id="TBL-7-5g"><col 
id="TBL-7-5"></colgroup><colgroup id="TBL-7-6g"><col 
id="TBL-7-6"></colgroup><colgroup id="TBL-7-7g"><col 
id="TBL-7-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-3"  
class="td11">564 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-4"  
class="td11">143 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-5"  
class="td11">505 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-3-6"  
class="td11">134</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-3"  
class="td11">588               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-4"  
class="td11">164                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-5"  
class="td11">332              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-4-6"  
class="td11">130               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-2"  
class="td11">0.75       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-3"  
class="td11">542               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-4"  
class="td11">191                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-5"  
class="td11">481              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-5-6"  
class="td11">146               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-2"  
class="td11">0.79       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-3"  
class="td11">601               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-4"  
class="td11">132                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-5"  
class="td11">478              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-7-6-6"  
class="td11">149               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-7-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-7-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.6: </span><span  
class="content">Logistic Regression results</span></div><!--tex4ht:label?: x1-710016 -->
</div>
<!--l. 786--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.7 </span> <a 
 id="x1-720004.2.1.7"></a>Gaussian Naive Bayes</h5>
<div class="center" 
>
<!--l. 788--><p class="noindent" >
<div class="tabular"><table id="TBL-8" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-8-1g"><col 
id="TBL-8-1"></colgroup><colgroup id="TBL-8-2g"><col 
id="TBL-8-2"></colgroup><colgroup id="TBL-8-3g"><col 
id="TBL-8-3"></colgroup><colgroup id="TBL-8-4g"><col 
id="TBL-8-4"></colgroup><colgroup id="TBL-8-5g"><col 
id="TBL-8-5"></colgroup><colgroup id="TBL-8-6g"><col 
id="TBL-8-6"></colgroup><colgroup id="TBL-8-7g"><col 
id="TBL-8-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-2"  
class="td11">0.73       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-3"  
class="td11">478               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-4"  
class="td11">168                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-5"  
class="td11">510              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-2-6"  
class="td11">205               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-2"  
class="td11">0.76 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-3"  
class="td11">555 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-4"  
class="td11">187 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-5"  
class="td11">461 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-3-6"  
class="td11">143</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-2"  
class="td11">0.74       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-3"  
class="td11">553               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-4"  
class="td11">153                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-5"  
class="td11">343              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-4-6"  
class="td11">165               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-2"  
class="td11">0.73       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-3"  
class="td11">491               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-4"  
class="td11">170                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-5"  
class="td11">502              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-5-6"  
class="td11">197               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-3"  
class="td11">590               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-4"  
class="td11">157                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-5"  
class="td11">453              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-8-6-6"  
class="td11">160               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-8-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-8-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.7: </span><span  
class="content">Gaussian Naive Bayes results</span></div><!--tex4ht:label?: x1-720017 -->
</div>
                                                                                
                                                                                
<!--l. 803--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.8 </span> <a 
 id="x1-730004.2.1.8"></a>Bernoulli Naive Bayes</h5>
<div class="center" 
>
<!--l. 805--><p class="noindent" >
<div class="tabular"><table id="TBL-9" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-9-1g"><col 
id="TBL-9-1"></colgroup><colgroup id="TBL-9-2g"><col 
id="TBL-9-2"></colgroup><colgroup id="TBL-9-3g"><col 
id="TBL-9-3"></colgroup><colgroup id="TBL-9-4g"><col 
id="TBL-9-4"></colgroup><colgroup id="TBL-9-5g"><col 
id="TBL-9-5"></colgroup><colgroup id="TBL-9-6g"><col 
id="TBL-9-6"></colgroup><colgroup id="TBL-9-7g"><col 
id="TBL-9-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-2"  
class="td11">0.73       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-3"  
class="td11">479               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-4"  
class="td11">169                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-5"  
class="td11">509              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-2-6"  
class="td11">204               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-2"  
class="td11">0.76 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-3"  
class="td11">558 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-4"  
class="td11">187 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-5"  
class="td11">461 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-3-6"  
class="td11">140</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-2"  
class="td11">0.74       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-3"  
class="td11">553               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-4"  
class="td11">153                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-5"  
class="td11">343              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-4-6"  
class="td11">165               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-2"  
class="td11">0.73       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-3"  
class="td11">492               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-4"  
class="td11">170                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-5"  
class="td11">502              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-5-6"  
class="td11">196               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-3"  
class="td11">590               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-4"  
class="td11">158                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-5"  
class="td11">452              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-9-6-6"  
class="td11">160               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-9-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-9-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.8: </span><span  
class="content">Bernoulli Naive Bayes results</span></div><!--tex4ht:label?: x1-730018 -->
</div>
<!--l. 820--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.9 </span> <a 
 id="x1-740004.2.1.9"></a>Neural Network</h5>
<div class="center" 
>
<!--l. 822--><p class="noindent" >
<div class="tabular"><table id="TBL-10" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-10-1g"><col 
id="TBL-10-1"></colgroup><colgroup id="TBL-10-2g"><col 
id="TBL-10-2"></colgroup><colgroup id="TBL-10-3g"><col 
id="TBL-10-3"></colgroup><colgroup id="TBL-10-4g"><col 
id="TBL-10-4"></colgroup><colgroup id="TBL-10-5g"><col 
id="TBL-10-5"></colgroup><colgroup id="TBL-10-6g"><col 
id="TBL-10-6"></colgroup><colgroup id="TBL-10-7g"><col 
id="TBL-10-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-3"  
class="td11">685               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-4"  
class="td11">181                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-5"  
class="td11">787              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-2-6"  
class="td11">258               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-2"  
class="td11">0.79 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-3"  
class="td11">616 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-4"  
class="td11">156 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-5"  
class="td11">542 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-3-6"  
class="td11">152</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-2"  
class="td11">0.76       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-3"  
class="td11">691               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-4"  
class="td11">183                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-5"  
class="td11">403              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-4-6"  
class="td11">153               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-2"  
class="td11">0.74       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-3"  
class="td11">690               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-4"  
class="td11">269                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-5"  
class="td11">542              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-5-6"  
class="td11">164               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-3"  
class="td11">1059              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-4"  
class="td11">270                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-5"  
class="td11">848              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-10-6-6"  
class="td11">164               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-10-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.9: </span><span  
class="content">Neural Network results</span></div><!--tex4ht:label?: x1-740019 -->
</div>
                                                                                
                                                                                
<!--l. 837--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1.10 </span> <a 
 id="x1-750004.2.1.10"></a>Stochastic Gradient Descent</h5>
<div class="center" 
>
<!--l. 839--><p class="noindent" >
<div class="tabular"><table id="TBL-11" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-11-1g"><col 
id="TBL-11-1"></colgroup><colgroup id="TBL-11-2g"><col 
id="TBL-11-2"></colgroup><colgroup id="TBL-11-3g"><col 
id="TBL-11-3"></colgroup><colgroup id="TBL-11-4g"><col 
id="TBL-11-4"></colgroup><colgroup id="TBL-11-5g"><col 
id="TBL-11-5"></colgroup><colgroup id="TBL-11-6g"><col 
id="TBL-11-6"></colgroup><colgroup id="TBL-11-7g"><col 
id="TBL-11-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-1"  
class="td11">Ticker</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-2"  
class="td11">Precision</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-3"  
class="td11">True Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-4"  
class="td11">False Negatives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-5"  
class="td11">True Positives</td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-1-6"  
class="td11">False Positives</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-1"  
class="td11">MSFT </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-2"  
class="td11">0.77       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-3"  
class="td11">493               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-4"  
class="td11">130                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-5"  
class="td11">548              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-2-6"  
class="td11">190               </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-1"  
class="td11">CDE </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-2"  
class="td11">0.76 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-3"  
class="td11">462 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-4"  
class="td11">106 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-5"  
class="td11">542 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-3-6"  
class="td11">236</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-1"  
class="td11">NAVB </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-2"  
class="td11">0.59       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-3"  
class="td11">1032              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-4"  
class="td11">707                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-5"  
class="td11">118              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-4-6"  
class="td11">80                </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-1"  
class="td11">HRG </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-2"  
class="td11">0.72       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-3"  
class="td11">761               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-4"  
class="td11">225                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-5"  
class="td11">1059             </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-5-6"  
class="td11">518               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-1"  
class="td11">HL </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-2"  
class="td11">0.80       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-3"  
class="td11">535               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-4"  
class="td11">83                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-5"  
class="td11">527              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-11-6-6"  
class="td11">215               </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-11-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-11-7-1"  
class="td11"> </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.10: </span><span  
class="content">Stochastic Gradient Descent results</span></div><!--tex4ht:label?: x1-7500110 -->
</div>
<!--l. 854--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.2.2 </span> <a 
 id="x1-760004.2.2"></a>Regression</h4>
<!--l. 856--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.1 </span> <a 
 id="x1-770004.2.2.1"></a>Decision Tree</h5>
<!--l. 857--><p class="noindent" >MSFT scored a mean absolute error regression loss of 5.606, and a coefficient of determination
of 0.458.
<div class="center" 
>
<!--l. 859--><p class="noindent" >

<!--l. 860--><p class="noindent" ><img 
src="Figures/MSFT-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.102: </span><span  
class="content">MSFT Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77001102 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 865--><p class="noindent" >

<!--l. 866--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.103: </span><span  
class="content">100 day MSFT Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77002103 -->
</div>
<!--l. 871--><p class="noindent" >CDE scored a mean absolute error regression loss of 2.906, and a coefficient of determination
of 0.816.
<div class="center" 
>
<!--l. 873--><p class="noindent" >

<!--l. 874--><p class="noindent" ><img 
src="Figures/CDE-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.104: </span><span  
class="content">CDE Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77003104 -->
</div>
<div class="center" 
>
<!--l. 879--><p class="noindent" >

<!--l. 880--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.105: </span><span  
class="content">100 day CDE Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77004105 -->
</div>
<!--l. 885--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.422, and a coefficient of determination
of 0.693.
<div class="center" 
>
                                                                                
                                                                                
<!--l. 887--><p class="noindent" >

<!--l. 888--><p class="noindent" ><img 
src="Figures/NAVB-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.106: </span><span  
class="content">NAVB Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77005106 -->
</div>
<div class="center" 
>
<!--l. 893--><p class="noindent" >

<!--l. 894--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.107: </span><span  
class="content">100 day NAVB Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77006107 -->
</div>
<!--l. 899--><p class="noindent" >HRG scored a mean absolute error regression loss of 1.415, and a coefficient of determination
of 0.464.
<div class="center" 
>
<!--l. 901--><p class="noindent" >

<!--l. 902--><p class="noindent" ><img 
src="Figures/HRG-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.108: </span><span  
class="content">HRG Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77007108 -->
</div>
<div class="center" 
>
<!--l. 907--><p class="noindent" >

<!--l. 908--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.109: </span><span  
class="content">100 day HRG Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77008109 -->
</div>
                                                                                
                                                                                
<!--l. 913--><p class="noindent" >HL scored a mean absolute error regression loss of 0.506, and a coefficient of determination of
0.923.
<div class="center" 
>
<!--l. 915--><p class="noindent" >

<!--l. 916--><p class="noindent" ><img 
src="Figures/HL-Decision-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.110: </span><span  
class="content">HL Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-77009110 -->
</div>
<div class="center" 
>
<!--l. 921--><p class="noindent" >

<!--l. 922--><p class="noindent" ><img 
src="Figures/100-Day-HL-Decision-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.111: </span><span  
class="content">100 day HL Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-77010111 -->
</div>
<!--l. 927--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.2 </span> <a 
 id="x1-780004.2.2.2"></a>Boosted Decision Tree</h5>
<!--l. 928--><p class="noindent" >MSFT scored a mean absolute error regression loss of 4.426, and a coefficient of determination
of 0.580.
<div class="center" 
>
<!--l. 930--><p class="noindent" >

<!--l. 931--><p class="noindent" ><img 
src="Figures/MSFT-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.112: </span><span  
class="content">MSFT Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-78001112 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 936--><p class="noindent" >

<!--l. 937--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.113: </span><span  
class="content">100 day MSFT Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-78002113 -->
</div>
<!--l. 942--><p class="noindent" >CDE scored a mean absolute error regression loss of 5.452, and a coefficient of determination
of 0.581.
<div class="center" 
>
<!--l. 944--><p class="noindent" >

<!--l. 945--><p class="noindent" ><img 
src="Figures/CDE-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.114: </span><span  
class="content">CDE Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-78003114 -->
</div>
<div class="center" 
>
<!--l. 950--><p class="noindent" >

<!--l. 951--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.115: </span><span  
class="content">100 day CDE Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-78004115 -->
</div>
<!--l. 956--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.579, and a coefficient of determination
of 0.509.
<div class="center" 
>
                                                                                
                                                                                
<!--l. 958--><p class="noindent" >

<!--l. 959--><p class="noindent" ><img 
src="Figures/NAVB-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.116: </span><span  
class="content">NAVB Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-78005116 -->
</div>
<div class="center" 
>
<!--l. 964--><p class="noindent" >

<!--l. 965--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.117: </span><span  
class="content">100 day NAVB Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-78006117 -->
</div>
<!--l. 970--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.980, and a coefficient of determination
of 0.855.
<div class="center" 
>
<!--l. 972--><p class="noindent" >

<!--l. 973--><p class="noindent" ><img 
src="Figures/HRG-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.118: </span><span  
class="content">HRG Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-78007118 -->
</div>
<div class="center" 
>
<!--l. 978--><p class="noindent" >

<!--l. 979--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.119: </span><span  
class="content">100 day HRG Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-78008119 -->
</div>
                                                                                
                                                                                
<!--l. 984--><p class="noindent" >HL scored a mean absolute error regression loss of 0.410, and a coefficient of determination of
0.952.
<div class="center" 
>
<!--l. 986--><p class="noindent" >

<!--l. 987--><p class="noindent" ><img 
src="Figures/HL-Boosted-Trees-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.120: </span><span  
class="content">HL Boosted Decision Trees in-sample prediction</span></div><!--tex4ht:label?: x1-78009120 -->
</div>
<div class="center" 
>
<!--l. 992--><p class="noindent" >

<!--l. 993--><p class="noindent" ><img 
src="Figures/100-Day-HL-Boosted-Trees-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.121: </span><span  
class="content">100 day HL Boosted Decision Trees out-of-sample forecast</span></div><!--tex4ht:label?: x1-78010121 -->
</div>
<!--l. 998--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.3 </span> <a 
 id="x1-790004.2.2.3"></a>K-Nearest Neighbour</h5>
<!--l. 999--><p class="noindent" >MSFT scored a mean absolute error regression loss of 4.426, and a coefficient of determination
of 0.580.
<div class="center" 
>
<!--l. 1001--><p class="noindent" >

<!--l. 1002--><p class="noindent" ><img 
src="Figures/MSFT-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.122: </span><span  
class="content">MSFT K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-79001122 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1007--><p class="noindent" >

<!--l. 1008--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.123: </span><span  
class="content">100 day MSFT K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-79002123 -->
</div>
<!--l. 1013--><p class="noindent" >CDE scored a mean absolute error regression loss of 5.452, and a coefficient of determination
of 0.581.
<div class="center" 
>
<!--l. 1015--><p class="noindent" >

<!--l. 1016--><p class="noindent" ><img 
src="Figures/CDE-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.124: </span><span  
class="content">CDE K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-79003124 -->
</div>
<div class="center" 
>
<!--l. 1021--><p class="noindent" >

<!--l. 1022--><p class="noindent" ><img 
src="Figures/100-Day-CDE-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.125: </span><span  
class="content">100 day CDE K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-79004125 -->
</div>
<!--l. 1027--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.579, and a coefficient of determination
of 0.509.
<div class="center" 
>
                                                                                
                                                                                
<!--l. 1029--><p class="noindent" >

<!--l. 1030--><p class="noindent" ><img 
src="Figures/NAVB-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.126: </span><span  
class="content">NAVB K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-79005126 -->
</div>
<div class="center" 
>
<!--l. 1035--><p class="noindent" >

<!--l. 1036--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.127: </span><span  
class="content">100 day NAVB K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-79006127 -->
</div>
<!--l. 1041--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.980, and a coefficient of determination
of 0.855.
<div class="center" 
>
<!--l. 1043--><p class="noindent" >

<!--l. 1044--><p class="noindent" ><img 
src="Figures/HRG-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.128: </span><span  
class="content">HRG K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-79007128 -->
</div>
<div class="center" 
>
<!--l. 1049--><p class="noindent" >

<!--l. 1050--><p class="noindent" ><img 
src="Figures/100-Day-HRG-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.129: </span><span  
class="content">100 day HRG K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-79008129 -->
</div>
                                                                                
                                                                                
<!--l. 1055--><p class="noindent" >HL scored a mean absolute error regression loss of 0.410, and a coefficient of determination of
0.952.
<div class="center" 
>
<!--l. 1057--><p class="noindent" >

<!--l. 1058--><p class="noindent" ><img 
src="Figures/HL-K-Nearest-Neighbour-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.130: </span><span  
class="content">HL K-Nearest Neighbour in-sample prediction</span></div><!--tex4ht:label?: x1-79009130 -->
</div>
<div class="center" 
>
<!--l. 1063--><p class="noindent" >

<!--l. 1064--><p class="noindent" ><img 
src="Figures/100-Day-HL-K-Nearest-Neighbour-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.131: </span><span  
class="content">100 day HL K-Nearest Neighbour out-of-sample forecast</span></div><!--tex4ht:label?: x1-79010131 -->
</div>
<!--l. 1069--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.4 </span> <a 
 id="x1-800004.2.2.4"></a>Random Forest</h5>
<!--l. 1070--><p class="noindent" >MSFT scored a mean absolute error regression loss of 5.199, and a coefficient of determination
of 0.483.
<div class="center" 
>
<!--l. 1072--><p class="noindent" >

<!--l. 1073--><p class="noindent" ><img 
src="Figures/MSFT-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.132: </span><span  
class="content">MSFT Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-80001132 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1078--><p class="noindent" >

<!--l. 1079--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.133: </span><span  
class="content">100 day MSFT Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-80002133 -->
</div>
<!--l. 1084--><p class="noindent" >CDE scored a mean absolute error regression loss of 2.505, and a coefficient of determination
of 0.861.
<div class="center" 
>
<!--l. 1086--><p class="noindent" >

<!--l. 1087--><p class="noindent" ><img 
src="Figures/CDE-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.134: </span><span  
class="content">CDE Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-80003134 -->
</div>
<div class="center" 
>
<!--l. 1092--><p class="noindent" >

<!--l. 1093--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.135: </span><span  
class="content">100 day CDE Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-80004135 -->
</div>
<!--l. 1098--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.293, and a coefficient of determination
of 0.856.
<div class="center" 
>
                                                                                
                                                                                
<!--l. 1100--><p class="noindent" >

<!--l. 1101--><p class="noindent" ><img 
src="Figures/NAVB-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.136: </span><span  
class="content">NAVB Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-80005136 -->
</div>
<div class="center" 
>
<!--l. 1106--><p class="noindent" >

<!--l. 1107--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.137: </span><span  
class="content">100 day NAVB Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-80006137 -->
</div>
<!--l. 1112--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.675, and a coefficient of determination
of 0.907.
<div class="center" 
>
<!--l. 1114--><p class="noindent" >

<!--l. 1115--><p class="noindent" ><img 
src="Figures/HRG-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.138: </span><span  
class="content">HRG Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-80007138 -->
</div>
<div class="center" 
>
<!--l. 1120--><p class="noindent" >

<!--l. 1121--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.139: </span><span  
class="content">100 day HRG Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-80008139 -->
</div>
                                                                                
                                                                                
<!--l. 1126--><p class="noindent" >HL scored a mean absolute error regression loss of 0.434, and a coefficient of determination of
0.922.
<div class="center" 
>
<!--l. 1128--><p class="noindent" >

<!--l. 1129--><p class="noindent" ><img 
src="Figures/HL-Random-Forest-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.140: </span><span  
class="content">HL Random Forest in-sample prediction</span></div><!--tex4ht:label?: x1-80009140 -->
</div>
<div class="center" 
>
<!--l. 1134--><p class="noindent" >

<!--l. 1135--><p class="noindent" ><img 
src="Figures/100-Day-HL-Random-Forest-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.141: </span><span  
class="content">100 day HL Random Forest out-of-sample forecast</span></div><!--tex4ht:label?: x1-80010141 -->
</div>
<!--l. 1140--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.5 </span> <a 
 id="x1-810004.2.2.5"></a>Linear Regression</h5>
<!--l. 1141--><p class="noindent" >MSFT scored a mean absolute error regression loss of 0.844, and a coefficient of determination
of 0.990.
<div class="center" 
>
<!--l. 1143--><p class="noindent" >

<!--l. 1144--><p class="noindent" ><img 
src="Figures/MSFT-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.142: </span><span  
class="content">MSFT Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-81001142 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1149--><p class="noindent" >

<!--l. 1150--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.143: </span><span  
class="content">100 day MSFT Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-81002143 -->
</div>
<!--l. 1155--><p class="noindent" >CDE scored a mean absolute error regression loss of 0.990, and a coefficient of determination
of 0.972.
<div class="center" 
>
<!--l. 1157--><p class="noindent" >

<!--l. 1158--><p class="noindent" ><img 
src="Figures/CDE-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.144: </span><span  
class="content">CDE Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-81003144 -->
</div>
<div class="center" 
>
<!--l. 1163--><p class="noindent" >

<!--l. 1164--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.145: </span><span  
class="content">100 day CDE Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-81004145 -->
</div>
<!--l. 1169--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.128, and a coefficient of determination
of 0.962.
<!--l. 1171--><p class="noindent" ><img 
src="Figures/NAVB-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<!--l. 1173--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1175--><p class="noindent" >

<!--l. 1176--><p class="noindent" ><img 
src="Figures/CDE-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.146: </span><span  
class="content">CDE Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-81005146 -->
</div>
<div class="center" 
>
<!--l. 1181--><p class="noindent" >

<!--l. 1182--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.147: </span><span  
class="content">100 day CDE Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-81006147 -->
</div>
<!--l. 1187--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.324, and a coefficient of determination
of 0.984.
<div class="center" 
>
<!--l. 1189--><p class="noindent" >

<!--l. 1190--><p class="noindent" ><img 
src="Figures/HRG-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.148: </span><span  
class="content">HRG Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-81007148 -->
</div>
<div class="center" 
>
<!--l. 1195--><p class="noindent" >

<!--l. 1196--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.149: </span><span  
class="content">100 day HRG Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-81008149 -->
</div>
                                                                                
                                                                                
<!--l. 1201--><p class="noindent" >HL scored a mean absolute error regression loss of 0.243, and a coefficient of determination of
0.964.
<div class="center" 
>
<!--l. 1203--><p class="noindent" >

<!--l. 1204--><p class="noindent" ><img 
src="Figures/HL-Linear-Regression-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.150: </span><span  
class="content">HL Linear Regression in-sample prediction</span></div><!--tex4ht:label?: x1-81009150 -->
</div>
<div class="center" 
>
<!--l. 1209--><p class="noindent" >

<!--l. 1210--><p class="noindent" ><img 
src="Figures/100-Day-HL-Linear-Regression-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.151: </span><span  
class="content">100 day HL Linear Regression out-of-sample forecast</span></div><!--tex4ht:label?: x1-81010151 -->
</div>
<!--l. 1215--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.6 </span> <a 
 id="x1-820004.2.2.6"></a>Neural Network</h5>
<!--l. 1216--><p class="noindent" >MSFT scored a mean absolute error regression loss of 1.073, and a coefficient of determination
of 0.992.
<div class="center" 
>
<!--l. 1218--><p class="noindent" >

<!--l. 1219--><p class="noindent" ><img 
src="Figures/MSFT-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.152: </span><span  
class="content">MSFT Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-82001152 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1224--><p class="noindent" >

<!--l. 1225--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.153: </span><span  
class="content">100 day MSFT Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-82002153 -->
</div>
<!--l. 1230--><p class="noindent" >CDE scored a mean absolute error regression loss of 2.906, and a coefficient of determination
of 0.816.
<!--l. 1232--><p class="noindent" ><img 
src="Figures/CDE-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<!--l. 1234--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<div class="center" 
>
<!--l. 1236--><p class="noindent" >

<!--l. 1237--><p class="noindent" ><img 
src="Figures/CDE-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.154: </span><span  
class="content">CDE Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-82003154 -->
</div>
<div class="center" 
>
<!--l. 1242--><p class="noindent" >

<!--l. 1243--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.155: </span><span  
class="content">100 day CDE Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-82004155 -->
</div>
<!--l. 1248--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.422, and a coefficient of determination
of 0.693.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1250--><p class="noindent" >

<!--l. 1251--><p class="noindent" ><img 
src="Figures/NAVB-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.156: </span><span  
class="content">NAVB Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-82005156 -->
</div>
<div class="center" 
>
<!--l. 1256--><p class="noindent" >

<!--l. 1257--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.157: </span><span  
class="content">100 day NAVB Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-82006157 -->
</div>
<!--l. 1262--><p class="noindent" >HRG scored a mean absolute error regression loss of 1.415, and a coefficient of determination
of 0.464.
<div class="center" 
>
<!--l. 1264--><p class="noindent" >

<!--l. 1265--><p class="noindent" ><img 
src="Figures/HRG-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.158: </span><span  
class="content">HRG Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-82007158 -->
</div>
<div class="center" 
>
<!--l. 1270--><p class="noindent" >

<!--l. 1271--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.159: </span><span  
class="content">100 day HRG Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-82008159 -->
</div>
                                                                                
                                                                                
<!--l. 1276--><p class="noindent" >HL scored a mean absolute error regression loss of 0.506, and a coefficient of determination of
0.923.
<div class="center" 
>
<!--l. 1278--><p class="noindent" >

<!--l. 1279--><p class="noindent" ><img 
src="Figures/HL-Neural-Network-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.160: </span><span  
class="content">HL Neural Network in-sample prediction</span></div><!--tex4ht:label?: x1-82009160 -->
</div>
<div class="center" 
>
<!--l. 1284--><p class="noindent" >

<!--l. 1285--><p class="noindent" ><img 
src="Figures/100-Day-HL-Neural-Network-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.161: </span><span  
class="content">100 day HL Neural Network out-of-sample forecast</span></div><!--tex4ht:label?: x1-82010161 -->
</div>
<!--l. 1290--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2.7 </span> <a 
 id="x1-830004.2.2.7"></a>Stochastic Gradient Descent</h5>
<!--l. 1291--><p class="noindent" >MSFT scored a mean absolute error regression loss of 0.829, and a coefficient of determination
of 0.990.
<div class="center" 
>
<!--l. 1293--><p class="noindent" >

<!--l. 1294--><p class="noindent" ><img 
src="Figures/MSFT-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.162: </span><span  
class="content">MSFT SGD in-sample prediction</span></div><!--tex4ht:label?: x1-83001162 -->
</div>
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1299--><p class="noindent" >

<!--l. 1300--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.163: </span><span  
class="content">100 day MSFT SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-83002163 -->
</div>
<!--l. 1305--><p class="noindent" >CDE scored a mean absolute error regression loss of 4.724, and a coefficient of determination
of 0.788.
<div class="center" 
>
<!--l. 1307--><p class="noindent" >

<!--l. 1308--><p class="noindent" ><img 
src="Figures/CDE-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.164: </span><span  
class="content">CDE SGD in-sample prediction</span></div><!--tex4ht:label?: x1-83003164 -->
</div>
<div class="center" 
>
<!--l. 1313--><p class="noindent" >

<!--l. 1314--><p class="noindent" ><img 
src="Figures/100-Day-CDE-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.165: </span><span  
class="content">100 day CDE SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-83004165 -->
</div>
<!--l. 1319--><p class="noindent" >NAVB scored a mean absolute error regression loss of 0.142, and a coefficient of determination
of 0.955.
<div class="center" 
>
                                                                                
                                                                                
<!--l. 1321--><p class="noindent" >

<!--l. 1322--><p class="noindent" ><img 
src="Figures/NAVB-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.166: </span><span  
class="content">NAVB SGD in-sample prediction</span></div><!--tex4ht:label?: x1-83005166 -->
</div>
<div class="center" 
>
<!--l. 1327--><p class="noindent" >

<!--l. 1328--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.167: </span><span  
class="content">100 day NAVB SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-83006167 -->
</div>
<!--l. 1333--><p class="noindent" >HRG scored a mean absolute error regression loss of 0.292, and a coefficient of determination
of 0.987.
<div class="center" 
>
<!--l. 1335--><p class="noindent" >

<!--l. 1336--><p class="noindent" ><img 
src="Figures/HRG-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.168: </span><span  
class="content">HRG SGD in-sample prediction</span></div><!--tex4ht:label?: x1-83007168 -->
</div>
<div class="center" 
>
<!--l. 1341--><p class="noindent" >

<!--l. 1342--><p class="noindent" ><img 
src="Figures/100-Day-HRG-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.169: </span><span  
class="content">100 day HRG SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-83008169 -->
</div>
                                                                                
                                                                                
<!--l. 1347--><p class="noindent" >HL scored a mean absolute error regression loss of 0.275, and a coefficient of determination of
0.962.
<div class="center" 
>
<!--l. 1349--><p class="noindent" >

<!--l. 1350--><p class="noindent" ><img 
src="Figures/HL-SGD-In-Sample-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.170: </span><span  
class="content">HL SGD in-sample prediction</span></div><!--tex4ht:label?: x1-83009170 -->
</div>
<div class="center" 
>
<!--l. 1355--><p class="noindent" >

<!--l. 1356--><p class="noindent" ><img 
src="Figures/100-Day-HL-SGD-Out-of-Sample-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.171: </span><span  
class="content">100 day HL SGD out-of-sample forecast</span></div><!--tex4ht:label?: x1-83010171 -->
</div>
<!--l. 1361--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">4.3 </span> <a 
 id="x1-840004.3"></a>Bayesian Statistics</h3>
<!--l. 1363--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.3.1 </span> <a 
 id="x1-850004.3.1"></a>Metropolis-Hastings</h4>
<!--l. 1364--><p class="noindent" >MSFT scored sharpe ratios of 0.439 for the original returns, and -4.796 for the predicted
returns in the in-sample test.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1366--><p class="noindent" >

<!--l. 1367--><p class="noindent" ><img 
src="Figures/MSFT-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.172: </span><span  
class="content">MSFT Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-85001172 -->
</div>
<div class="center" 
>
<!--l. 1372--><p class="noindent" >

<!--l. 1373--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.173: </span><span  
class="content">100 day MSFT Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-85002173 -->
</div>
<!--l. 1378--><p class="noindent" >CDE scored sharpe ratios of 2.028 for the original returns, and 4.075 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 1380--><p class="noindent" >

<!--l. 1381--><p class="noindent" ><img 
src="Figures/CDE-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.174: </span><span  
class="content">CDE Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-85003174 -->
</div>
<div class="center" 
>
<!--l. 1386--><p class="noindent" >

<!--l. 1387--><p class="noindent" ><img 
src="Figures/100-Day-CDE-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.175: </span><span  
class="content">100 day CDE Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-85004175 -->
</div>
                                                                                
                                                                                
<!--l. 1392--><p class="noindent" >NAVB scored sharpe ratios of -2.800 for the original returns, and 2.998 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 1394--><p class="noindent" >

<!--l. 1395--><p class="noindent" ><img 
src="Figures/NAVB-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.176: </span><span  
class="content">NAVB Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-85005176 -->
</div>
<div class="center" 
>
<!--l. 1400--><p class="noindent" >

<!--l. 1401--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.177: </span><span  
class="content">100 day NAVB Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-85006177 -->
</div>
<!--l. 1406--><p class="noindent" >HRG scored sharpe ratios of 1.362 for the original returns, and -2.173 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 1408--><p class="noindent" >

<!--l. 1409--><p class="noindent" ><img 
src="Figures/HRG-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.178: </span><span  
class="content">HRG Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-85007178 -->
</div>
<div class="center" 
>
                                                                                
                                                                                
<!--l. 1414--><p class="noindent" >

<!--l. 1415--><p class="noindent" ><img 
src="Figures/100-Day-HRG-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.179: </span><span  
class="content">100 day HRG Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-85008179 -->
</div>
<!--l. 1420--><p class="noindent" >HL scored sharpe ratios of 0.439 for the original returns, and -2.283 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 1422--><p class="noindent" >

<!--l. 1423--><p class="noindent" ><img 
src="Figures/HL-Metropolis-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.180: </span><span  
class="content">HL Metropolis-Hastimgs in-sample prediction</span></div><!--tex4ht:label?: x1-85009180 -->
</div>
<div class="center" 
>
<!--l. 1428--><p class="noindent" >

<!--l. 1429--><p class="noindent" ><img 
src="Figures/100-Day-HL-Metropolis-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.181: </span><span  
class="content">100 day HL Metropolis-Hastings out-of-sample forecast</span></div><!--tex4ht:label?: x1-85010181 -->
</div>
<!--l. 1434--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.3.2 </span> <a 
 id="x1-860004.3.2"></a>No-U-Turn Sampler (NUTS)</h4>
<!--l. 1435--><p class="noindent" >MSFT scored sharpe ratios of 2.499 for the original returns, and -1.355 for the predicted
returns in the in-sample test.
                                                                                
                                                                                
<div class="center" 
>
<!--l. 1437--><p class="noindent" >

<!--l. 1438--><p class="noindent" ><img 
src="Figures/MSFT-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.182: </span><span  
class="content">MSFT NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-86001182 -->
</div>
<div class="center" 
>
<!--l. 1443--><p class="noindent" >

<!--l. 1444--><p class="noindent" ><img 
src="Figures/100-Day-MSFT-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.183: </span><span  
class="content">100 day MSFT NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-86002183 -->
</div>
<!--l. 1449--><p class="noindent" >CDE scored sharpe ratios of 2.028 for the original returns, and 3.066 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 1451--><p class="noindent" >

<!--l. 1452--><p class="noindent" ><img 
src="Figures/CDE-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.184: </span><span  
class="content">CDE NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-86003184 -->
</div>
<div class="center" 
>
<!--l. 1457--><p class="noindent" >

<!--l. 1458--><p class="noindent" ><img 
src="Figures/100-Day-CDE-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.185: </span><span  
class="content">100 day CDE NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-86004185 -->
</div>
                                                                                
                                                                                
<!--l. 1463--><p class="noindent" >NAVB scored sharpe ratios of -2.800 for the original returns, and -2.914 for the predicted
returns in the in-sample test.
<div class="center" 
>
<!--l. 1465--><p class="noindent" >

<!--l. 1466--><p class="noindent" ><img 
src="Figures/NAVB-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.186: </span><span  
class="content">NAVB NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-86005186 -->
</div>
<div class="center" 
>
<!--l. 1471--><p class="noindent" >

<!--l. 1472--><p class="noindent" ><img 
src="Figures/100-Day-NAVB-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.187: </span><span  
class="content">100 day NAVB NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-86006187 -->
</div>
<!--l. 1477--><p class="noindent" >HRG scored sharpe ratios of 1.362 for the original returns, and -1.812 for the predicted returns
in the in-sample test.
<div class="center" 
>
<!--l. 1479--><p class="noindent" >

<!--l. 1480--><p class="noindent" ><img 
src="Figures/HRG-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.188: </span><span  
class="content">HRG NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-86007188 -->
</div>
<div class="center" 
>
                                                                                
                                                                                
<!--l. 1485--><p class="noindent" >

<!--l. 1486--><p class="noindent" ><img 
src="Figures/100-Day-HRG-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.189: </span><span  
class="content">100 day HRG NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-86008189 -->
</div>
<!--l. 1491--><p class="noindent" >HL scored sharpe ratios of 0.506 for the original returns, and 0.923 for the predicted returns in
the in-sample test.
<div class="center" 
>
<!--l. 1493--><p class="noindent" >

<!--l. 1494--><p class="noindent" ><img 
src="Figures/HL-NUTS-In-Sample-Returns-Prediction.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.190: </span><span  
class="content">HL NUTS in-sample prediction</span></div><!--tex4ht:label?: x1-86009190 -->
</div>
<div class="center" 
>
<!--l. 1499--><p class="noindent" >

<!--l. 1500--><p class="noindent" ><img 
src="Figures/100-Day-HL-NUTS-Out-of-Sample-Returns-Forecast.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.191: </span><span  
class="content">100 day HL NUTS out-of-sample forecast</span></div><!--tex4ht:label?: x1-86010191 -->
</div>
<!--l. 1505--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">4.4 </span> <a 
 id="x1-870004.4"></a>Strategy</h3>
                                                                                
                                                                                
<!--l. 1507--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.4.1 </span> <a 
 id="x1-880004.4.1"></a>Classification</h4>
<div class="center" 
>
<!--l. 1509--><p class="noindent" >
<div class="tabular"><table id="TBL-12" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-12-1g"><col 
id="TBL-12-1"></colgroup><colgroup id="TBL-12-2g"><col 
id="TBL-12-2"></colgroup><colgroup id="TBL-12-3g"><col 
id="TBL-12-3"></colgroup><colgroup id="TBL-12-4g"><col 
id="TBL-12-4"></colgroup><colgroup id="TBL-12-5g"><col 
id="TBL-12-5"></colgroup><colgroup id="TBL-12-6g"><col 
id="TBL-12-6"></colgroup><colgroup id="TBL-12-7g"><col 
id="TBL-12-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-1-1"  
class="td11">Metric                         </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-1-2"  
class="td11">Value        </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-2-1"  
class="td11">Starting Capital </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-2-2"  
class="td11">100,000</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-3-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-3-2"  
class="td11">103,110.65 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-4-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-4-2"  
class="td11">0.346        </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-5-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-5-2"  
class="td11">149,126.306</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-6-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-6-2"  
class="td11">0.491        </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-7-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-7-2"  
class="td11">1.008        </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-8-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-8-2"  
class="td11">0.272        </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-9-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-12-9-2"  
class="td11">0.156        </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-12-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-12-10-1"  
class="td11">                      </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.11: </span><span  
class="content">Machine Learning Classifier strategy with only upwards forecasts</span></div><!--tex4ht:label?: x1-8800111 -->
</div>
<div class="center" 
>
<!--l. 1527--><p class="noindent" >

<!--l. 1528--><p class="noindent" ><img 
src="Figures/MLC-Portfolio-Benchmark-Up.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.192: </span><span  
class="content">Machine Learning Classifier strategy with only upwards forecasts</span></div><!--tex4ht:label?: x1-88002192 -->
</div>
<div class="center" 
>
<!--l. 1533--><p class="noindent" >
<div class="tabular"><table id="TBL-13" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-13-1g"><col 
id="TBL-13-1"></colgroup><colgroup id="TBL-13-2g"><col 
id="TBL-13-2"></colgroup><colgroup id="TBL-13-3g"><col 
id="TBL-13-3"></colgroup><colgroup id="TBL-13-4g"><col 
id="TBL-13-4"></colgroup><colgroup id="TBL-13-5g"><col 
id="TBL-13-5"></colgroup><colgroup id="TBL-13-6g"><col 
id="TBL-13-6"></colgroup><colgroup id="TBL-13-7g"><col 
id="TBL-13-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-1-1"  
class="td11">Metric                         </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-1-2"  
class="td11">Value      </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-2-1"  
class="td11">Starting Capital </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-2-2"  
class="td11">100,000</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-3-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-3-2"  
class="td11">76,853.53 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-4-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-4-2"  
class="td11">-0.548      </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-5-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-5-2"  
class="td11">20,134.519</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-6-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-6-2"  
class="td11">-0.799      </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-7-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-7-2"  
class="td11">1.008       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-8-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-8-2"  
class="td11">0.323       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-9-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-13-9-2"  
class="td11">0.156       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-13-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-13-10-1"  
class="td11">                      </td>
</tr></table></div>
                                                                                
                                                                                
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.12: </span><span  
class="content">Machine Learning Classifier strategy with upwarda and downwards forecasts</span></div><!--tex4ht:label?: x1-8800312 -->
</div>
<div class="center" 
>
<!--l. 1551--><p class="noindent" >

<!--l. 1552--><p class="noindent" ><img 
src="Figures/MLC-Portfolio-Benchmark-Up-and-Down.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.193:  </span><span  
class="content">Machine  Learning  Classifier  strategy  with  upwards  and  downwards
forecasts</span></div><!--tex4ht:label?: x1-88004193 -->
</div>
<!--l. 1557--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">4.4.2 </span> <a 
 id="x1-890004.4.2"></a>Regression</h4>
<div class="center" 
>
<!--l. 1559--><p class="noindent" >
<div class="tabular"><table id="TBL-14" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-14-1g"><col 
id="TBL-14-1"></colgroup><colgroup id="TBL-14-2g"><col 
id="TBL-14-2"></colgroup><colgroup id="TBL-14-3g"><col 
id="TBL-14-3"></colgroup><colgroup id="TBL-14-4g"><col 
id="TBL-14-4"></colgroup><colgroup id="TBL-14-5g"><col 
id="TBL-14-5"></colgroup><colgroup id="TBL-14-6g"><col 
id="TBL-14-6"></colgroup><colgroup id="TBL-14-7g"><col 
id="TBL-14-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-1-1"  
class="td11">Metric                         </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-1-2"  
class="td11">Value      </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-2-1"  
class="td11">Starting Capital </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-2-2"  
class="td11">100,000</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-3-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-3-2"  
class="td11">104,329.92</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-4-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-4-2"  
class="td11">0.323       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-5-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-5-2"  
class="td11">142,962.49</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-6-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-6-2"  
class="td11">0.430       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-7-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-7-2"  
class="td11">1.008       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-8-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-8-2"  
class="td11">0.280       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-9-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-14-9-2"  
class="td11">0.156       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-14-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-14-10-1"  
class="td11">                      </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.13: </span><span  
class="content">Machine Learning Regression strategy with only upwards forecasts</span></div><!--tex4ht:label?: x1-8900113 -->
</div>
<div class="center" 
>
<!--l. 1577--><p class="noindent" >

                                                                                
                                                                                
<!--l. 1578--><p class="noindent" ><img 
src="Figures/MLR-Portfolio-Benchmark-Up.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.194: </span><span  
class="content">Machine Learning Regression strategy with only upwards forecasts</span></div><!--tex4ht:label?: x1-89002194 -->
</div>
<div class="center" 
>
<!--l. 1583--><p class="noindent" >
<div class="tabular"><table id="TBL-15" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-15-1g"><col 
id="TBL-15-1"></colgroup><colgroup id="TBL-15-2g"><col 
id="TBL-15-2"></colgroup><colgroup id="TBL-15-3g"><col 
id="TBL-15-3"></colgroup><colgroup id="TBL-15-4g"><col 
id="TBL-15-4"></colgroup><colgroup id="TBL-15-5g"><col 
id="TBL-15-5"></colgroup><colgroup id="TBL-15-6g"><col 
id="TBL-15-6"></colgroup><colgroup id="TBL-15-7g"><col 
id="TBL-15-7"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-1-1"  
class="td11">Metric                         </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-1-2"  
class="td11">Value     </td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-2-1"  
class="td11">Starting Capital </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-2-2"  
class="td11">100,000</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-3-1"  
class="td11">Total Capital Used          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-3-2"  
class="td11">80,121.51</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-4-1"  
class="td11">Sharpe Ratio                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-4-2"  
class="td11">-0.258    </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-5-1"  
class="td11">Portfolio Value               </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-5-2"  
class="td11">19,628.34</td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-6-1"  
class="td11">Algorithm Period Return </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-6-2"  
class="td11">-0.804    </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-7-1"  
class="td11">Benchmark Period Return</td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-7-2"  
class="td11">1.008     </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-8-1"  
class="td11">Algorithm Volatility        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-8-2"  
class="td11">0.493     </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-9-1"  
class="td11">Benchmark Volatility       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-15-9-2"  
class="td11">0.156     </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-15-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-15-10-1"  
class="td11">                      </td>
</tr></table></div>
<br /><div class="caption" 
><span class="id">Table&#x00A0;4.14: </span><span  
class="content">Machine  Learning  Regression  strategy  with  upwards  and  downwards
forecasts</span></div><!--tex4ht:label?: x1-8900314 -->
</div>
<div class="center" 
>
<!--l. 1601--><p class="noindent" >

<!--l. 1602--><p class="noindent" ><img 
src="Figures/MLR-Portfolio-Benchmark-Up-and-Down.png" alt="PIC"  
>
<br /><div class="caption" 
><span class="id">Figure&#x00A0;4.195:  </span><span  
class="content">Machine  Learning  Regression  strategy  with  upwards  and  downwards
forecasts</span></div><!--tex4ht:label?: x1-89004195 -->
</div>
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;5</span><br /><a 
 id="x1-900005"></a>Discussion and Suggestions for Future Research</h2>
<h3 class="sectionHead"><span class="titlemark">5.1 </span> <a 
 id="x1-910005.1"></a>Time Series Analysis</h3>
<!--l. 5--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.1 </span> <a 
 id="x1-920005.1.1"></a>Random Walk</h4>
<!--l. 6--><p class="noindent" >The random walk theory suggests that stock price changes have the same distribution and are
independent of each other, so the past movement or trend of a stock price or market cannot be
used to predict its future movement. In short, this is the idea that stocks take a random and
unpredictable path.
<!--l. 8--><p class="noindent" >As can be seen in figures 4.4, 4.6, 4.8, and 4.10, the correlation plots show this theory to be
false and that past movement is related to future movement.
<!--l. 10--><p class="noindent" >The histogram of returns show that the stocks follow a normal distribution, where some stocks
showed higher returns than others as can be seen from a wider x-axis.
<!--l. 12--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.2 </span> <a 
 id="x1-930005.1.2"></a>Ordinary Least Squares (OLS)</h4>
<!--l. 13--><p class="noindent" >This algorithm achieved a good fit in the in-sample tests, having a prediction rate above 96%
for all the stocks tested.
<!--l. 15--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.3 </span> <a 
 id="x1-940005.1.3"></a>Auto Regressive (AR)</h4>
<!--l. 16--><p class="noindent" >This algorithm failed to achieve a good fit in the in-sample tests, having a huge difference
in sharpe ratios based on the original price returns and in-sample predicted price
returns. The algorithm failed completely in forecasting price returns in out-of-sample
testing.
                                                                                
                                                                                
<!--l. 18--><p class="noindent" >The histogram of returns further showed that the algorithm did not achieve a good fit, as the
histogram itself was not completely symmetrical.
<!--l. 20--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.4 </span> <a 
 id="x1-950005.1.4"></a>Moving Average (MA)</h4>
<!--l. 21--><p class="noindent" >This algorithm faired better than AR having a lower difference in sharpe ratios based on the
original price returns and in-sample predicted price returns, however still failed to achieve a
good fit in the in-sample tests.
<!--l. 23--><p class="noindent" >Although achieving a better fit, the histogram of returns still showed flaws as they were not
perfectly symmetrical.
<!--l. 25--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.5 </span> <a 
 id="x1-960005.1.5"></a>Auto Regressive Moving Average (ARMA)</h4>
<!--l. 26--><p class="noindent" >This algorithm once again faired better than the previous algorithm, MA, having an even
lower difference in sharpe ratios based on the original price returns and in-sample predicted
price returns, however still failed to achieve a good fit in the in-sample tests. As can be seen in
the time series analysis plots, ARMA showed to have very heavy tails in the QQ and
probability plots.
<!--l. 28--><p class="noindent" >As can be seen from the histogram of returns, a better fit was achieved as the histogram was
more symmetrical than those of the previous algorithms.
<!--l. 30--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.1.6 </span> <a 
 id="x1-970005.1.6"></a>Auto Regressive Integrated Moving Average (ARIMA)</h4>
<!--l. 31--><p class="noindent" >This algorithm faired worse than the previous algorithm, ARMA, having a higher difference
in sharpe ratios based on the original price returns and in-sample predicted price
returns.
<!--l. 33--><p class="noindent" >This can also be seen from the histogram of returns, in which its distribution was less normal
than that of the previous algorithm.
                                                                                
                                                                                
<!--l. 35--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">5.2 </span> <a 
 id="x1-980005.2"></a>Machine Learning</h3>
<!--l. 37--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.2.1 </span> <a 
 id="x1-990005.2.1"></a>Classification</h4>
<!--l. 39--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.1 </span> <a 
 id="x1-1000005.2.1.1"></a>Decision Tree</h5>
<!--l. 40--><p class="noindent" >This algorithm faired well in predicting stock price movements in the in-sample tests, having
an accuracy score above 75% for all the stocks prices that were forecast.
<!--l. 42--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.2 </span> <a 
 id="x1-1010005.2.1.2"></a>Boosted Decision Tree</h5>
<!--l. 43--><p class="noindent" >This algorithm yielded slightly better results when compared to the previous algorithm in
predicting stock price movements in the in-sample tests.
<!--l. 45--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.3 </span> <a 
 id="x1-1020005.2.1.3"></a>Support Vector Machine (SVM)</h5>
<!--l. 46--><p class="noindent" >This algorithm once again yielded slightly better results when compared to the previous
algorithm in predicting stock price movements in the in-sample tests.
                                                                                
                                                                                
<!--l. 48--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.4 </span> <a 
 id="x1-1030005.2.1.4"></a>Random Forest</h5>
<!--l. 49--><p class="noindent" >This algorithm scored the same accuracy as the previous algorithm.
<!--l. 51--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.5 </span> <a 
 id="x1-1040005.2.1.5"></a>K-Nearest Neighbour</h5>
<!--l. 52--><p class="noindent" >This algorithm&#8217;s accuracy was not consistent as it scored mixed results when tested against all
the stocks, where the accuracy is some was much lower than others.
<!--l. 54--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.6 </span> <a 
 id="x1-1050005.2.1.6"></a>Logistic Regression</h5>
<!--l. 55--><p class="noindent" >This algorithm faired worse than the others having not achieved an accuracy score of above
80% when tested against all the stocks.
<!--l. 57--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.7 </span> <a 
 id="x1-1060005.2.1.7"></a>Bernoulli Naive Bayes</h5>
<!--l. 58--><p class="noindent" >This algorithm faired the worst from the alogrithms tested so far, scoring accuracy scores in
the lower 70th percentile.
<!--l. 60--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.8 </span> <a 
 id="x1-1070005.2.1.8"></a>Gaussian Naive Bayes</h5>
<!--l. 61--><p class="noindent" >This algorithm also scored a lower accuracy score, showing that Naive Bayes isn&#8217;t the best
form of forecasting stock prices.
                                                                                
                                                                                
<!--l. 63--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.9 </span> <a 
 id="x1-1080005.2.1.9"></a>Neural Network</h5>
<!--l. 64--><p class="noindent" >This algorithm, although did not score the best accuracy score from the other algorithms, still
faired well in correctly predicting the price movments of the stocks.
<!--l. 66--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.1.10 </span> <a 
 id="x1-1090005.2.1.10"></a>Stochastic Gradient Descent</h5>
<!--l. 67--><p class="noindent" >This algorithm faired the worst from the lot, varying heavily in accuracy scores, in which
one stock had an accuracy score of 59%. Even though it faired miuch better in
correctly predicting the price movements of other stocks, this algorithm proved to be
unreliable.
<!--l. 69--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.2.2 </span> <a 
 id="x1-1100005.2.2"></a>Regression</h4>
<!--l. 71--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.1 </span> <a 
 id="x1-1110005.2.2.1"></a>Decision Tree</h5>
<!--l. 72--><p class="noindent" >As can be seen in figure 4.102, the algorithm not only failed to achieve a good fit in the
in-sample test, but also failed completely in predicting any values at all. Figure 4.103 also
shoes that the algorithm predicted a sharp fall at the end of the out-of-sample test,
which could be related to the incorrect fit achieved in the in-sample test. This was
reflected in the metric score, having a high number of data losses, and low accuracy
scores.
<!--l. 74--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.2 </span> <a 
 id="x1-1120005.2.2.2"></a>Boosted Decision Tree</h5>
                                                                                
                                                                                
<!--l. 75--><p class="noindent" >This algorithm also did not do very well, as can be seen in figures 4.112, 4.114, 4.116, and
4.118, the algorithm not only failed to achieve a good fit in the in-sample test, but also failed
completely in predicting any values at all. The algorithm also predicted some sharp inclines at
the end of the test, as can be seen in figure 4.115. The bad fit achieved is also reflected in the
metric scores, in which there was a high number of data losses, and low accuracy
scores.
<!--l. 77--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.3 </span> <a 
 id="x1-1130005.2.2.3"></a>K-Nearest Neighbour</h5>
<!--l. 78--><p class="noindent" >This algorithm also didn&#8217;t fair too well, showing bad fits in figures 4.122, 4.124, 4.126, 4.128,
and 4.130. Similarly to the previous algorithm, the alforithm failed completely in predicting
any values at all in some parts of the in-sample test.
<!--l. 80--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.4 </span> <a 
 id="x1-1140005.2.2.4"></a>Random Forest</h5>
<!--l. 81--><p class="noindent" >Similarly to previous algorithms, this algorithm failed to achieve a good fit as can be seen in
figures 4.132, 4.134, 4.136, 4,138, and 4.140. The algorithm also failed to predict any values at
all in the of the in-sample tests, and also predicted sharp inclines and declines at the end some
of the out-of-sample tests.
<!--l. 83--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.5 </span> <a 
 id="x1-1150005.2.2.5"></a>Linear Regression</h5>
<!--l. 84--><p class="noindent" >This algorithm performed very well in the in-sample tests, achieving a good fit with high
accuracy scores above 95% with a low number of data losses.
<!--l. 86--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.6 </span> <a 
 id="x1-1160005.2.2.6"></a>Neural Network</h5>
                                                                                
                                                                                
<!--l. 87--><p class="noindent" >This algorithm, although scored very well in some of the in-sample tests, was not very reliable
as accuracy scores varied between each test, performing extremely well in some, however not
too well in others.
<!--l. 89--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.2.2.7 </span> <a 
 id="x1-1170005.2.2.7"></a>Stochastic Gradient Descent</h5>
<!--l. 90--><p class="noindent" >This algorithm performed fairly well in most in-sample tests, having high accuracy scores and
low data losses escept for CDE as can be seen in 4.164.
<!--l. 92--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">5.3 </span> <a 
 id="x1-1180005.3"></a>Bayesian Statistics</h3>
<!--l. 94--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.3.0.1 </span> <a 
 id="x1-1190005.3.0.1"></a>Metropolis-Hastings</h5>
<!--l. 95--><p class="noindent" >This algorithm failed to achieve a good fit in the in-sample tests, having a huge difference
in sharpe ratios based on the original price returns and in-sample predicted price
returns.
<!--l. 97--><p class="noindent" >
<h5 class="subsubsectionHead"><span class="titlemark">5.3.0.2 </span> <a 
 id="x1-1200005.3.0.2"></a>No-U-Turn-Sampler (NUTS)</h5>
<!--l. 98--><p class="noindent" >The algorithm achieved a good fit in the in-sample tests, having very similar sharpe ratios
based on the original price returns and in-sample predicted price returns.
                                                                                
                                                                                
<!--l. 100--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">5.4 </span> <a 
 id="x1-1210005.4"></a>Strategy</h3>
<!--l. 102--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.4.1 </span> <a 
 id="x1-1220005.4.1"></a>Classification</h4>
<!--l. 103--><p class="noindent" >When tasked with predicting rises in stock prices, the algorithm did fairly well, marking a
total return of 49.1%, this however did not beat the benchmark&#8217;s return of 100.8%. The
agorithm underperformed immensely when tasked at also predicting stock price falls, marking
a negative total return of -80%.
<!--l. 105--><p class="noindent" >
<h4 class="subsectionHead"><span class="titlemark">5.4.2 </span> <a 
 id="x1-1230005.4.2"></a>Regression</h4>
<!--l. 106--><p class="noindent" >The performance of this algorithm was extremely similar to that of the previous one, this goes
to show that both classifiers and regressors can be used for stock price prediction. When
tasked with predicting rises in stock prices, the algorithm did fairly well, marking a
slightly lower total return of 43%. The agorithm also underperformed immensely
when tasked at also predicting stock price falls, marking a negative total return of
-80.4%.
                                                                                
                                                                                
<h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;6</span><br /><a 
 id="x1-1240006"></a>Conclusion</h2>
<!--l. 3--><p class="noindent" >Three financial forecasting methods were presented in this report, two of which
showed little to no potential of ever producing any statistically significant result
when the correct methodology was applied. The third method, machine learning,
showed some potential in the tests carried out, which is why this method was built
into an automated algorithmic strategy to trade with. The algorithm proved to
be successful in forecasting future prices, using both classification and regression
methods. However, the backesting proved this method to fail in forecasting price
falls. Once this factor was removed from the equation, the algorithms were very
successful and reported a profit by the end of the test. This is however not always
ideal as stocks which could fall in price could be catastrophic to the strategy. A
stop loss would be ideal in insuring that no positions are held in downward falling
stocks.
<!--l. 5--><p class="noindent" >If there is anything that this report shows, it is that profitable stock market prediction is an
extremely tough problem. Even through the strategy reported a profit by the end of the
backtest, it still did not beat the market. Whether it is possible at all ultimately remains an
open question.
<!--l. 7--><p class="noindent" >After completing the project, it is the firm belief of the author that the only viable trading
strategy for a casual investor is a passive buy and hold strategy in index funds and
ETFs.
                                                                                
                                                                                
<a 
 id="x1-124001r339"></a>
<h2 class="appendixHead"><span class="titlemark">Appendix&#x00A0;A</span><br /><a 
 id="x1-125000A"></a>An Appendix</h2>
<!--l. 3--><p class="noindent" >Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus at pulvinar nisi. Phasellus
hendrerit, diam placerat interdum iaculis, mauris justo cursus risus, in viverra purus eros at
ligula. Ut metus justo, consequat a tristique posuere, laoreet nec nibh. Etiam et
scelerisque mauris. Phasellus vel massa magna. Ut non neque id tortor pharetra
bibendum vitae sit amet nisi. Duis nec quam quam, sed euismod justo. Pellentesque eu
tellus vitae ante tempus malesuada. Nunc accumsan, quam in congue consequat,
lectus lectus dapibus erat, id aliquet urna neque at massa. Nulla facilisi. Morbi
ullamcorper eleifend posuere. Donec libero leo, faucibus nec bibendum at, mattis et urna.
Proin consectetur, nunc ut imperdiet lobortis, magna neque tincidunt lectus, id
iaculis nisi justo id nibh. Pellentesque vel sem in erat vulputate faucibus molestie ut
lorem.
<!--l. 5--><p class="noindent" >Quisque tristique urna in lorem laoreet at laoreet quam congue. Donec dolor turpis, blandit
non imperdiet aliquet, blandit et felis. In lorem nisi, pretium sit amet vestibulum sed, tempus
et sem. Proin non ante turpis. Nulla imperdiet fringilla convallis. Vivamus vel bibendum nisl.
Pellentesque justo lectus, molestie vel luctus sed, lobortis in libero. Nulla facilisi. Aliquam erat
volutpat. Suspendisse vitae nunc nunc. Sed aliquet est suscipit sapien rhoncus non
adipiscing nibh consequat. Aliquam metus urna, faucibus eu vulputate non, luctus eu
justo.
<!--l. 7--><p class="noindent" >Donec urna leo, vulputate vitae porta eu, vehicula blandit libero. Phasellus eget massa et leo
condimentum mollis. Nullam molestie, justo at pellentesque vulputate, sapien velit ornare
diam, nec gravida lacus augue non diam. Integer mattis lacus id libero ultrices sit amet mollis
neque molestie. Integer ut leo eget mi volutpat congue. Vivamus sodales, turpis id venenatis
placerat, tellus purus adipiscing magna, eu aliquam nibh dolor id nibh. Pellentesque habitant
morbi tristique senectus et netus et malesuada fames ac turpis egestas. Sed cursus
convallis quam nec vehicula. Sed vulputate neque eget odio fringilla ac sodales urna
feugiat.
<!--l. 9--><p class="noindent" >Phasellus nisi quam, volutpat non ullamcorper eget, congue fringilla leo. Cras et erat et nibh
placerat commodo id ornare est. Nulla facilisi. Aenean pulvinar scelerisque eros
eget interdum. Nunc pulvinar magna ut felis varius in hendrerit dolor accumsan.
Nunc pellentesque magna quis magna bibendum non laoreet erat tincidunt. Nulla
facilisi.
<!--l. 11--><p class="noindent" >Duis eget massa sem, gravida interdum ipsum. Nulla nunc nisl, hendrerit sit amet commodo
vel, varius id tellus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc ac dolor est.
Suspendisse ultrices tincidunt metus eget accumsan. Nullam facilisis, justo vitae convallis
                                                                                
                                                                                
sollicitudin, eros augue malesuada metus, nec sagittis diam nibh ut sapien. Duis
blandit lectus vitae lorem aliquam nec euismod nisi volutpat. Vestibulum ornare
dictum tortor, at faucibus justo tempor non. Nulla facilisi. Cras non massa nunc,
eget euismod purus. Nunc metus ipsum, euismod a consectetur vel, hendrerit nec
nunc.
                                                                                
                                                                                
<a 
 id="x1-125001r7"></a>
<a 
 id="Q1-1-341"></a>
                                                                                
                                                                                
<h2 class="likechapterHead"><a 
 id="x1-1260007"></a>Bibliography</h2>
<div class="thebibliography">
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMarkowitz:1952aa"></a>[1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Harry Markowitz. Portfolio selection. <span 
class="cmti-10x-x-109">American Finance Association</span>, 1952.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XPeter-J.-Brockwell:2016aa"></a>[2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Peter&#x00A0;J. Brockwell and Richard&#x00A0;A. Davis.  <span 
class="cmti-10x-x-109">Introduction to Time Series and</span>
<span 
class="cmti-10x-x-109">Forecasting</span>. Springer, 2016.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMoskowitz:2011aa"></a>[3]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tobias  Moskowitz,  Yao  Hua&#x00A0;Ooi,  and  Lasse&#x00A0;H.  Pedersen.     Time  series
momentum. <span 
class="cmti-10x-x-109">Chicago Booth Research</span>, September 2011.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XRadha:2015aa"></a>[4]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>S.&#x00A0;Radha  and  M.&#x00A0;Thenmozhi.   Forecasting  short  term  interest  rates  using
arma, arma-garch and arma-egarch models. <span 
class="cmti-10x-x-109">Indian Institute of Capital Markets 9th</span>
<span 
class="cmti-10x-x-109">Capital Markets Conference Paper</span>, January 2006.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XAbrosimova:2002aa"></a>[5]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Natalia Abrosimova, Gishan Dissanaike, and Dirk Linowski. Testing weak-form
efficiency of the russian stock market. In <span 
class="cmti-10x-x-109">EFA 2002 Berlin Meetings</span>, 2002.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XDarrat:2001aa"></a>[6]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ali&#x00A0;F. Darrat and Maosen Zhong.  On testing the random walk hypothesis: A
model-comparison approach, 2001.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XCormen:2009aa"></a>[7]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Thomas&#x00A0;H. Cormen and Chales&#x00A0;E. Leiserson. <span 
class="cmti-10x-x-109">Introduction to Algorithms</span>. The
MIT Press, 2009.
                                                                                
                                                                                
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMitchell:1997aa"></a>[8]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tom&#x00A0;M. Mitchell. <span 
class="cmti-10x-x-109">Machine Learning</span>. McGraw Hill, 1997.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMurugesan:2012aa"></a>[9]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Punniyamoorthy Murugesan and Jose&#x00A0;Joy Thoppan.  Detection of stock price
manipulation using discriminant analysis, June 2012.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XKumar:2016aa"></a>[10]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Manish  Kumar  and  M.&#x00A0;Thenmozhi.   Forecasting  stock  index  movement:  A
comparison of support vector machines and random forest, June 2016.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XKakushadze:2015aa"></a>[11]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zura Kakushadze.     Mean-reversion  and  optimization  mean-reversion  and
optimization. <span 
class="cmti-10x-x-109">Journal of Asset Management</span>, 16(1):14&#8211;40, 2015.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XCreamer:2010aa"></a>[12]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Germn&#x00A0;G. Creamer and Yoav Freund. Automated trading with boosting and
expert weighting. <span 
class="cmti-10x-x-109">Quantitative Finance</span>, Vol. 4(No. 10):pp. 401&#8211;420, April 2010.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XGelman:2014aa"></a>[13]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Andrew Gelman, John&#x00A0;B. Carlin, Hal&#x00A0;S. Stern, David&#x00A0;B. Dunson, Aki Vehtari,
and Donald&#x00A0;B. Rubin. <span 
class="cmti-10x-x-109">Bayesian Data Analysis</span>. CRC Press, 2014.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XSavvides:1994aa"></a>[14]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Savvakis&#x00A0;C.  Savvides.    Risk  analysis  in  investment  appraisal  risk  analysis
in investment appraisal risk analysis in investment appraisal.  <span 
class="cmti-10x-x-109">Project Appraisal</span>
<span 
class="cmti-10x-x-109">Journal</span>, Vol. 9(No. 1), March 1994.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XAndersen:2007aa"></a>[15]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Leif B.&#x00A0;G. Andersen.  Efficient simulation of the heston stochastic volatility
model, 2007.
</p>
                                                                                
                                                                                
<p class="bibitem" ><span class="biblabel">
<a 
 id="XBlanchett:2013aa"></a>[16]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>David Blanchett, Michael&#x00A0;S. Finke, and Wade&#x00A0;D. Pfau.  Asset valuations and
safe portfolio withdrawal rates, 2013.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XGamba:2003aa"></a>[17]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Andrea Gamba. Real options valuation: A monte carlo approach, 2003.
</p>
<p class="bibitem" ><span class="biblabel">
<a 
 id="XMatthew-D.-Hoffman:2014aa"></a>[18]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Matthew&#x00A0;D. Hoffman and Andrew Gelman. The no-u-turn sampler: Adaptively
setting path lengths in hamiltonian monte carlo.   <span 
class="cmti-10x-x-109">Journal of Machine Learning</span>
<span 
class="cmti-10x-x-109">Research</span>, 15, 2014.
</p>
</div>
 
</body></html> 

                                                                                


